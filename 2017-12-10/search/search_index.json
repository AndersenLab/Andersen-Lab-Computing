{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome_to_mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project_layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"bash/","text":"Bash \u00b6 Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with the fo Introduction to bash . Basic Commands \u00b6 You should familiarize yourself with the following commands. alias - create a shortcut for a command awk - file manipulation; Filtering; Rearranging columns cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sed - quick find/replace sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted. More Advanced \u00b6 You should learn these once you have the basics down. git Good Guides \u00b6 Below I link to some good guides for various bash utilities. awk \u00b6 awk guide awk by example - hundreds of examples Rearranging columns \u00b6 cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column. Filtering based on criteria \u00b6 Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }' bcftools \u00b6 bcftools manual Screen \u00b6 Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Bash"},{"location":"bash/#bash","text":"Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with the fo Introduction to bash .","title":"Bash"},{"location":"bash/#basic_commands","text":"You should familiarize yourself with the following commands. alias - create a shortcut for a command awk - file manipulation; Filtering; Rearranging columns cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sed - quick find/replace sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted.","title":"Basic Commands"},{"location":"bash/#more_advanced","text":"You should learn these once you have the basics down. git","title":"More Advanced"},{"location":"bash/#good_guides","text":"Below I link to some good guides for various bash utilities.","title":"Good Guides"},{"location":"bash/#awk","text":"awk guide awk by example - hundreds of examples","title":"awk"},{"location":"bash/#rearranging_columns","text":"cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column.","title":"Rearranging columns"},{"location":"bash/#filtering_based_on_criteria","text":"Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }'","title":"Filtering based on criteria"},{"location":"bash/#bcftools","text":"bcftools manual","title":"bcftools"},{"location":"bash/#screen","text":"Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Screen"},{"location":"cendr/","text":"Setting up Quest \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"CeNDR"},{"location":"cendr/#setting_up_quest","text":"For full documentation visit mkdocs.org .","title":"Setting up Quest"},{"location":"cendr/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"cendr/#project_layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"domains/","text":"Domains \u00b6 Andersen Lab Domains are managed by Google.","title":"Domains"},{"location":"domains/#domains","text":"Andersen Lab Domains are managed by Google.","title":"Domains"},{"location":"google-cloud/","text":"Setting up Quest \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Google Cloud"},{"location":"google-cloud/#setting_up_quest","text":"For full documentation visit mkdocs.org .","title":"Setting up Quest"},{"location":"google-cloud/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"google-cloud/#project_layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"labsite/","text":"Andersenlab.org \u00b6 Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software Getting Started \u00b6 The Andersen Lab website was built using jekyll and runs using the Github Pages service. Software-Dependencies \u00b6 Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml . Cloning the repo \u00b6 To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url. Updating the site \u00b6 In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git. andersenlab.github.io \u00b6 The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with Announcements \u00b6 Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication. General Announcements \u00b6 To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more. Publication Post \u00b6 New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3! Lab members \u00b6 Adding new lab members: \u00b6 (1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page. Set Status to Former \u00b6 Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a> Remove lab members \u00b6 Remove the persons information from _data/people.yaml ; Optionally delete their photo. Funding \u00b6 Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here. Protocols \u00b6 Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines. Research \u00b6 The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000). Publications \u00b6 Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page. Photo Albums \u00b6 Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push Software \u00b6 If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Andersen Labsite"},{"location":"labsite/#andersenlaborg","text":"Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software","title":"Andersenlab.org"},{"location":"labsite/#getting_started","text":"The Andersen Lab website was built using jekyll and runs using the Github Pages service.","title":"Getting Started"},{"location":"labsite/#software-dependencies","text":"Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml .","title":"Software-Dependencies"},{"location":"labsite/#cloning_the_repo","text":"To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url.","title":"Cloning the repo"},{"location":"labsite/#updating_the_site","text":"In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git.","title":"Updating the site"},{"location":"labsite/#andersenlabgithubio","text":"The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with","title":"andersenlab.github.io"},{"location":"labsite/#announcements","text":"Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication.","title":"Announcements"},{"location":"labsite/#general_announcements","text":"To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more.","title":"General Announcements"},{"location":"labsite/#publication_post","text":"New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3!","title":"Publication Post"},{"location":"labsite/#lab_members","text":"","title":"Lab members"},{"location":"labsite/#adding_new_lab_members","text":"(1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page.","title":"Adding new lab members:"},{"location":"labsite/#set_status_to_former","text":"Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a>","title":"Set Status to Former"},{"location":"labsite/#remove_lab_members","text":"Remove the persons information from _data/people.yaml ; Optionally delete their photo.","title":"Remove lab members"},{"location":"labsite/#funding","text":"Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here.","title":"Funding"},{"location":"labsite/#protocols","text":"Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines.","title":"Protocols"},{"location":"labsite/#research","text":"The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000).","title":"Research"},{"location":"labsite/#publications","text":"Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page.","title":"Publications"},{"location":"labsite/#photo_albums","text":"Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push","title":"Photo Albums"},{"location":"labsite/#software","text":"If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Software"},{"location":"pipeline-cegwas/","text":"NIL-NF \u00b6 The nil-nf pipeline will align, call variants, and generate datasets for NIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. NIL-NF Usage Parameters --debug --cores --A, --B --cA, --cB --out --fqs (FASTQs) --vcf (Parental VCF) --reference --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ vcf/ Usage \u00b6 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --reference Reference Genome /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default). Parameters \u00b6 --debug \u00b6 The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. You can use the following command to debug: nextflow run main.nf --debug --reference=<path to reference> --cores \u00b6 The number of cores to use during alignments and variant calling. --A, --B \u00b6 Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details. --cA, --cB \u00b6 The color to use for parental strain A and B on plots. --out \u00b6 A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains. --fqs (FASTQs) \u00b6 In order to process NIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq files are relative to that file. The fastq sheet details the FASTQ files and their associated strains. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. --vcf (Parental VCF) \u00b6 Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use: /projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz --reference \u00b6 A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz --tmpdir \u00b6 A directory for storing temporary data. Output \u00b6 The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u2514\u2500\u2500 gt_hmm.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u251c\u2500\u2500 sitelist \u2502 \u251c\u2500\u2500 N2.CB4856.sitelist.tsv.gz \u2502 \u2514\u2500\u2500 N2.CB4856.sitelist.tsv.gz.tbi \u2514\u2500\u2500 vcf \u251c\u2500\u2500 NIL.filtered.stats.txt \u251c\u2500\u2500 NIL.filtered.vcf.gz \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u251c\u2500\u2500 NIL.hmm.vcf.gz \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u251c\u2500\u2500 gt_hmm.tsv \u251c\u2500\u2500 gt_hmm_fill.tsv \u2514\u2500\u2500 union_vcfs.txt log.txt \u00b6 A summary of the nextflow run. duplicates/ \u00b6 bam_duplicates.tsv - A summary of duplicate reads from aligned bams. fq/ \u00b6 fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq SM/ \u00b6 If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. hmm/ \u00b6 gt_hmm.(png/svg) - Haplotype plot for NILs. gt_hmm.tsv - Long form genotypes file. plots/ \u00b6 coverage_comparison.(png/svg/pdf) - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/svg/pdf) - Coverage vs. percent duplicated. unmapped_reads.(png/svg/pdf) - Coverage vs. unmapped read percent. sitelist/ \u00b6 <A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. vcf/ \u00b6 gt_hmm.tsv - Haplotypes defined by region with associated information. gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation NIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL.hmm.vcf.gz - The RIL VCF as output by VCF-Kit; HMM applied to determine genotypes. union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz","title":"Cegwas"},{"location":"pipeline-cegwas/#nil-nf","text":"The nil-nf pipeline will align, call variants, and generate datasets for NIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. NIL-NF Usage Parameters --debug --cores --A, --B --cA, --cB --out --fqs (FASTQs) --vcf (Parental VCF) --reference --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ vcf/","title":"NIL-NF"},{"location":"pipeline-cegwas/#usage","text":"\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --reference Reference Genome /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default).","title":"Usage"},{"location":"pipeline-cegwas/#parameters","text":"","title":"Parameters"},{"location":"pipeline-cegwas/#--debug","text":"The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. You can use the following command to debug: nextflow run main.nf --debug --reference=<path to reference>","title":"--debug"},{"location":"pipeline-cegwas/#--cores","text":"The number of cores to use during alignments and variant calling.","title":"--cores"},{"location":"pipeline-cegwas/#--a_--b","text":"Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details.","title":"--A, --B"},{"location":"pipeline-cegwas/#--ca_--cb","text":"The color to use for parental strain A and B on plots.","title":"--cA, --cB"},{"location":"pipeline-cegwas/#--out","text":"A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains.","title":"--out"},{"location":"pipeline-cegwas/#--fqs_fastqs","text":"In order to process NIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq files are relative to that file. The fastq sheet details the FASTQ files and their associated strains. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible.","title":"--fqs (FASTQs)"},{"location":"pipeline-cegwas/#--vcf_parental_vcf","text":"Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use: /projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz","title":"--vcf (Parental VCF)"},{"location":"pipeline-cegwas/#--reference","text":"A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz","title":"--reference"},{"location":"pipeline-cegwas/#--tmpdir","text":"A directory for storing temporary data.","title":"--tmpdir"},{"location":"pipeline-cegwas/#output","text":"The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u2514\u2500\u2500 gt_hmm.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u251c\u2500\u2500 sitelist \u2502 \u251c\u2500\u2500 N2.CB4856.sitelist.tsv.gz \u2502 \u2514\u2500\u2500 N2.CB4856.sitelist.tsv.gz.tbi \u2514\u2500\u2500 vcf \u251c\u2500\u2500 NIL.filtered.stats.txt \u251c\u2500\u2500 NIL.filtered.vcf.gz \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u251c\u2500\u2500 NIL.hmm.vcf.gz \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u251c\u2500\u2500 gt_hmm.tsv \u251c\u2500\u2500 gt_hmm_fill.tsv \u2514\u2500\u2500 union_vcfs.txt","title":"Output"},{"location":"pipeline-cegwas/#logtxt","text":"A summary of the nextflow run.","title":"log.txt"},{"location":"pipeline-cegwas/#duplicates","text":"bam_duplicates.tsv - A summary of duplicate reads from aligned bams.","title":"duplicates/"},{"location":"pipeline-cegwas/#fq","text":"fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq","title":"fq/"},{"location":"pipeline-cegwas/#sm","text":"If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level.","title":"SM/"},{"location":"pipeline-cegwas/#hmm","text":"gt_hmm.(png/svg) - Haplotype plot for NILs. gt_hmm.tsv - Long form genotypes file.","title":"hmm/"},{"location":"pipeline-cegwas/#plots","text":"coverage_comparison.(png/svg/pdf) - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/svg/pdf) - Coverage vs. percent duplicated. unmapped_reads.(png/svg/pdf) - Coverage vs. unmapped read percent.","title":"plots/"},{"location":"pipeline-cegwas/#sitelist","text":"<A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains.","title":"sitelist/"},{"location":"pipeline-cegwas/#vcf","text":"gt_hmm.tsv - Haplotypes defined by region with associated information. gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation NIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL.hmm.vcf.gz - The RIL VCF as output by VCF-Kit; HMM applied to determine genotypes. union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz","title":"vcf/"},{"location":"pipeline-concordance/","text":"wi-nf \u00b6 The wi-nf pipeline aligns, calls variants, and performs analysis from wild isolate sequence data. wi-nf Usage Parameters --debug --cores --out --fqs (FASTQs) --vcf (Parental VCF) --reference --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ vcf/ Loading BigQuery Usage \u00b6 \u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2584\u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2588\u2591\u2588\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2588\u2591\u2588\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u258c \u2580\u2580 \u2580\u2580 \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2580 \u2580\u2580 \u2580 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test ${params.debug} --cores Regular job cores ${params.cores} --out Directory to output results ${params.out} --fqs fastq file (see help) ${params.fqs} --fq_file_prefix fastq prefix ${params.fq_file_prefix} --reference Reference Genome ${params.reference} --annotation_reference SnpEff annotation ${params.annotation_reference} --bamdir Location for bams ${params.bamdir} --tmpdir A temporary directory ${params.tmpdir} HELP: http://andersenlab.org/dry-guide/pipeline-wi/ Parameters \u00b6 --debug \u00b6 The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. You can use the following command to debug: nextflow run main.nf --debug --reference=<path to reference> --cores \u00b6 The number of cores to use during alignments and variant calling. --out \u00b6 A directory in which to output results. By default it will be WI-YYYY-MM-DD where YYYY-MM-DD is todays date. --fqs (FASTQs) \u00b6 In order to process NIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq files are relative to that file. The fastq sheet details the FASTQ files and their associated strains. It should be tab-delimited and look like this: ECA551 ECA551_S2_L004 S2 ECA551_S2_L004_1P.fq.gz ECA551_S2_L004_2P.fq.gz seq_folder_1 ECA552 ECA552_S16_L004 S16 ECA552_S16_L004_1P.fq.gz ECA552_S16_L004_2P.fq.gz seq_folder_1 ECA571 ECA571_S5_L004 S5 ECA571_S5_L004_1P.fq.gz ECA571_S5_L004_2P.fq.gz seq_folder_1 ECA571 ECA572_S6_L004 S6 ECA572_S6_L004_1P.fq.gz ECA572_S6_L004_2P.fq.gz seq_folder_1 Notice that the file does not include a header. The table with corresponding columns looks like this. isotype fastq_pair_id library fastq-1-path fastq-2-path sequencing_folder ECA551 ECA551_S2_L004 S2 ECA551_S2_L004_1P.fq.gz ECA551_S2_L004_2P.fq.gz seq_folder_1 ECA552 ECA552_S16_L004 S16 ECA552_S16_L004_1P.fq.gz ECA552_S16_L004_2P.fq.gz seq_folder_1 The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. --vcf (Parental VCF) \u00b6 Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use: /projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz --reference \u00b6 A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz --tmpdir \u00b6 A directory for storing temporary data. Output \u00b6 The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u2514\u2500\u2500 gt_hmm.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u251c\u2500\u2500 sitelist \u2502 \u251c\u2500\u2500 N2.CB4856.sitelist.tsv.gz \u2502 \u2514\u2500\u2500 N2.CB4856.sitelist.tsv.gz.tbi \u2514\u2500\u2500 vcf \u251c\u2500\u2500 NIL.filtered.stats.txt \u251c\u2500\u2500 NIL.filtered.vcf.gz \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u251c\u2500\u2500 NIL.hmm.vcf.gz \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u251c\u2500\u2500 gt_hmm.tsv \u251c\u2500\u2500 gt_hmm_fill.tsv \u2514\u2500\u2500 union_vcfs.txt log.txt \u00b6 A summary of the nextflow run. duplicates/ \u00b6 bam_duplicates.tsv - A summary of duplicate reads from aligned bams. fq/ \u00b6 fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq SM/ \u00b6 If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. hmm/ \u00b6 gt_hmm.(png/svg) - Haplotype plot for NILs. gt_hmm.tsv - Long form genotypes file. plots/ \u00b6 coverage_comparison.(png/svg/pdf) - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/svg/pdf) - Coverage vs. percent duplicated. unmapped_reads.(png/svg/pdf) - Coverage vs. unmapped read percent. sitelist/ \u00b6 <A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. vcf/ \u00b6 gt_hmm.tsv - Haplotypes defined by region with associated information. gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation NIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL.hmm.vcf.gz - The RIL VCF as output by VCF-Kit; HMM applied to determine genotypes. union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz Loading BigQuery \u00b6 release_date=20170312 bq load --field_delimiter \"\\t\" \\ --skip_leading_rows 1 \\ --ignore_unknown_values \\ andersen-lab:WI.${release_date} \\ gs://elegansvariation.org/releases/${release_date}/WI.${release_date}.tsv.gz \\ CHROM:STRING,POS:INTEGER,SAMPLE:STRING,REF:STRING,ALT:STRING,FILTER:STRING,FT:STRING,GT:STRING","title":"Concordance"},{"location":"pipeline-concordance/#wi-nf","text":"The wi-nf pipeline aligns, calls variants, and performs analysis from wild isolate sequence data. wi-nf Usage Parameters --debug --cores --out --fqs (FASTQs) --vcf (Parental VCF) --reference --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ vcf/ Loading BigQuery","title":"wi-nf"},{"location":"pipeline-concordance/#usage","text":"\u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2584\u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2588\u2591\u2588\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2588\u2591\u2588\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u258c \u2580\u2580 \u2580\u2580 \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2580 \u2580\u2580 \u2580 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test ${params.debug} --cores Regular job cores ${params.cores} --out Directory to output results ${params.out} --fqs fastq file (see help) ${params.fqs} --fq_file_prefix fastq prefix ${params.fq_file_prefix} --reference Reference Genome ${params.reference} --annotation_reference SnpEff annotation ${params.annotation_reference} --bamdir Location for bams ${params.bamdir} --tmpdir A temporary directory ${params.tmpdir} HELP: http://andersenlab.org/dry-guide/pipeline-wi/","title":"Usage"},{"location":"pipeline-concordance/#parameters","text":"","title":"Parameters"},{"location":"pipeline-concordance/#--debug","text":"The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. You can use the following command to debug: nextflow run main.nf --debug --reference=<path to reference>","title":"--debug"},{"location":"pipeline-concordance/#--cores","text":"The number of cores to use during alignments and variant calling.","title":"--cores"},{"location":"pipeline-concordance/#--out","text":"A directory in which to output results. By default it will be WI-YYYY-MM-DD where YYYY-MM-DD is todays date.","title":"--out"},{"location":"pipeline-concordance/#--fqs_fastqs","text":"In order to process NIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq files are relative to that file. The fastq sheet details the FASTQ files and their associated strains. It should be tab-delimited and look like this: ECA551 ECA551_S2_L004 S2 ECA551_S2_L004_1P.fq.gz ECA551_S2_L004_2P.fq.gz seq_folder_1 ECA552 ECA552_S16_L004 S16 ECA552_S16_L004_1P.fq.gz ECA552_S16_L004_2P.fq.gz seq_folder_1 ECA571 ECA571_S5_L004 S5 ECA571_S5_L004_1P.fq.gz ECA571_S5_L004_2P.fq.gz seq_folder_1 ECA571 ECA572_S6_L004 S6 ECA572_S6_L004_1P.fq.gz ECA572_S6_L004_2P.fq.gz seq_folder_1 Notice that the file does not include a header. The table with corresponding columns looks like this. isotype fastq_pair_id library fastq-1-path fastq-2-path sequencing_folder ECA551 ECA551_S2_L004 S2 ECA551_S2_L004_1P.fq.gz ECA551_S2_L004_2P.fq.gz seq_folder_1 ECA552 ECA552_S16_L004 S16 ECA552_S16_L004_1P.fq.gz ECA552_S16_L004_2P.fq.gz seq_folder_1 The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible.","title":"--fqs (FASTQs)"},{"location":"pipeline-concordance/#--vcf_parental_vcf","text":"Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use: /projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz","title":"--vcf (Parental VCF)"},{"location":"pipeline-concordance/#--reference","text":"A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz","title":"--reference"},{"location":"pipeline-concordance/#--tmpdir","text":"A directory for storing temporary data.","title":"--tmpdir"},{"location":"pipeline-concordance/#output","text":"The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u2514\u2500\u2500 gt_hmm.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u251c\u2500\u2500 sitelist \u2502 \u251c\u2500\u2500 N2.CB4856.sitelist.tsv.gz \u2502 \u2514\u2500\u2500 N2.CB4856.sitelist.tsv.gz.tbi \u2514\u2500\u2500 vcf \u251c\u2500\u2500 NIL.filtered.stats.txt \u251c\u2500\u2500 NIL.filtered.vcf.gz \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u251c\u2500\u2500 NIL.hmm.vcf.gz \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u251c\u2500\u2500 gt_hmm.tsv \u251c\u2500\u2500 gt_hmm_fill.tsv \u2514\u2500\u2500 union_vcfs.txt","title":"Output"},{"location":"pipeline-concordance/#logtxt","text":"A summary of the nextflow run.","title":"log.txt"},{"location":"pipeline-concordance/#duplicates","text":"bam_duplicates.tsv - A summary of duplicate reads from aligned bams.","title":"duplicates/"},{"location":"pipeline-concordance/#fq","text":"fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq","title":"fq/"},{"location":"pipeline-concordance/#sm","text":"If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level.","title":"SM/"},{"location":"pipeline-concordance/#hmm","text":"gt_hmm.(png/svg) - Haplotype plot for NILs. gt_hmm.tsv - Long form genotypes file.","title":"hmm/"},{"location":"pipeline-concordance/#plots","text":"coverage_comparison.(png/svg/pdf) - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/svg/pdf) - Coverage vs. percent duplicated. unmapped_reads.(png/svg/pdf) - Coverage vs. unmapped read percent.","title":"plots/"},{"location":"pipeline-concordance/#sitelist","text":"<A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains.","title":"sitelist/"},{"location":"pipeline-concordance/#vcf","text":"gt_hmm.tsv - Haplotypes defined by region with associated information. gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation NIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL.hmm.vcf.gz - The RIL VCF as output by VCF-Kit; HMM applied to determine genotypes. union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz","title":"vcf/"},{"location":"pipeline-concordance/#loading_bigquery","text":"release_date=20170312 bq load --field_delimiter \"\\t\" \\ --skip_leading_rows 1 \\ --ignore_unknown_values \\ andersen-lab:WI.${release_date} \\ gs://elegansvariation.org/releases/${release_date}/WI.${release_date}.tsv.gz \\ CHROM:STRING,POS:INTEGER,SAMPLE:STRING,REF:STRING,ALT:STRING,FILTER:STRING,FT:STRING,GT:STRING","title":"Loading BigQuery"},{"location":"pipeline-nil-ril/","text":"nil-ril-nf \u00b6 The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. nil-ril-nf Docker image andersenlab/nil-ril-nf Usage Overview Docker image andersenlab/nil-ril-nf Testing Parameters --debug --cores --A, --B --cA, --cB --out --fqs (FASTQs) --relative --vcf (Parental VCF) --reference --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ Docker image \u00b6 The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf \u00b6 The Dockerfile is stored in the root of the nil-nf github repo and is automatically built on Dockerhub whenever the repo is pushed. Usage \u00b6 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix ${params.relative} --reference Reference Genome /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default). Overview \u00b6 The nil-ril-nf pipeline: Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data. Docker image \u00b6 The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf \u00b6 The Dockerfile is stored in the root of the nil-nf github repo and is automatically built on Dockerhub whenever the repo is pushed. Testing \u00b6 If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl https://storage.googleapis.com/andersen/genome/c_elegans/WS245/WS245.tar.gz > WS245.tar.gz tar -xvzf WS245.tar.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS245.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-nf pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD Parameters \u00b6 --debug \u00b6 The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information. --cores \u00b6 The number of cores to use during alignments and variant calling. --A, --B \u00b6 Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details. --cA, --cB \u00b6 The color to use for parental strain A and B on plots. --out \u00b6 A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains. --fqs (FASTQs) \u00b6 In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. If you want to specify fastqs using an absolute path use --relative=false --relative \u00b6 Set to true by default. If you set --relative=false , fq's in the fq_sheet are expected to use an absolute path. --vcf (Parental VCF) \u00b6 Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use: /projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz --reference \u00b6 A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz --tmpdir \u00b6 A directory for storing temporary data. Output \u00b6 The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi] log.txt \u00b6 A summary of the nextflow run. duplicates/ \u00b6 bam_duplicates.tsv - A summary of duplicate reads from aligned bams. fq/ \u00b6 fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq SM/ \u00b6 If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz hmm/ \u00b6 Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes. plots/ \u00b6 coverage_comparison.(png/svg/pdf) - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/svg/pdf) - Coverage vs. percent duplicated. unmapped_reads.(png/svg/pdf) - Coverage vs. unmapped read percent. sitelist/ \u00b6 <A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains.","title":"NIL / RIL"},{"location":"pipeline-nil-ril/#nil-ril-nf","text":"The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. nil-ril-nf Docker image andersenlab/nil-ril-nf Usage Overview Docker image andersenlab/nil-ril-nf Testing Parameters --debug --cores --A, --B --cA, --cB --out --fqs (FASTQs) --relative --vcf (Parental VCF) --reference --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/","title":"nil-ril-nf"},{"location":"pipeline-nil-ril/#docker_image","text":"The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image:","title":"Docker image"},{"location":"pipeline-nil-ril/#andersenlabnil-ril-nf","text":"The Dockerfile is stored in the root of the nil-nf github repo and is automatically built on Dockerhub whenever the repo is pushed.","title":"andersenlab/nil-ril-nf"},{"location":"pipeline-nil-ril/#usage","text":"\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix ${params.relative} --reference Reference Genome /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default).","title":"Usage"},{"location":"pipeline-nil-ril/#overview","text":"The nil-ril-nf pipeline: Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data.","title":"Overview"},{"location":"pipeline-nil-ril/#docker_image_1","text":"The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image:","title":"Docker image"},{"location":"pipeline-nil-ril/#andersenlabnil-ril-nf_1","text":"The Dockerfile is stored in the root of the nil-nf github repo and is automatically built on Dockerhub whenever the repo is pushed.","title":"andersenlab/nil-ril-nf"},{"location":"pipeline-nil-ril/#testing","text":"If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl https://storage.googleapis.com/andersen/genome/c_elegans/WS245/WS245.tar.gz > WS245.tar.gz tar -xvzf WS245.tar.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS245.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-nf pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD","title":"Testing"},{"location":"pipeline-nil-ril/#parameters","text":"","title":"Parameters"},{"location":"pipeline-nil-ril/#--debug","text":"The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information.","title":"--debug"},{"location":"pipeline-nil-ril/#--cores","text":"The number of cores to use during alignments and variant calling.","title":"--cores"},{"location":"pipeline-nil-ril/#--a_--b","text":"Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details.","title":"--A, --B"},{"location":"pipeline-nil-ril/#--ca_--cb","text":"The color to use for parental strain A and B on plots.","title":"--cA, --cB"},{"location":"pipeline-nil-ril/#--out","text":"A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains.","title":"--out"},{"location":"pipeline-nil-ril/#--fqs_fastqs","text":"In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. If you want to specify fastqs using an absolute path use --relative=false","title":"--fqs (FASTQs)"},{"location":"pipeline-nil-ril/#--relative","text":"Set to true by default. If you set --relative=false , fq's in the fq_sheet are expected to use an absolute path.","title":"--relative"},{"location":"pipeline-nil-ril/#--vcf_parental_vcf","text":"Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use: /projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz","title":"--vcf (Parental VCF)"},{"location":"pipeline-nil-ril/#--reference","text":"A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz","title":"--reference"},{"location":"pipeline-nil-ril/#--tmpdir","text":"A directory for storing temporary data.","title":"--tmpdir"},{"location":"pipeline-nil-ril/#output","text":"The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi]","title":"Output"},{"location":"pipeline-nil-ril/#logtxt","text":"A summary of the nextflow run.","title":"log.txt"},{"location":"pipeline-nil-ril/#duplicates","text":"bam_duplicates.tsv - A summary of duplicate reads from aligned bams.","title":"duplicates/"},{"location":"pipeline-nil-ril/#fq","text":"fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq","title":"fq/"},{"location":"pipeline-nil-ril/#sm","text":"If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz","title":"SM/"},{"location":"pipeline-nil-ril/#hmm","text":"Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes.","title":"hmm/"},{"location":"pipeline-nil-ril/#plots","text":"coverage_comparison.(png/svg/pdf) - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/svg/pdf) - Coverage vs. percent duplicated. unmapped_reads.(png/svg/pdf) - Coverage vs. unmapped read percent.","title":"plots/"},{"location":"pipeline-nil-ril/#sitelist","text":"<A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains.","title":"sitelist/"},{"location":"pipeline-trimming/","text":"trimmomatic-nf \u00b6 The trimmomatic workflow should be used to initially process sequence data. You should not use the trimmomatic workflow on low-coverage NIL or RIL data. Usage \u00b6 New sequence data should be stored in the appropriate path on the cluster in the raw directory. For example, when adding new sequence data for wild isolates, data should be added to a new folder here: /projects/b1059/data/fastq/WI/dna/raw/<folder_name> Important All FASTQs should end with a _1.fq.gz or a _2.fq.gz . To rename FASTQs you can use: rename --dry-run --subst .fastq.gz .fq.gz --subst _R1_001 _1 --subst _R2_001 *.fastq.gz Outside of these simple changes to the filenames, no further changes should be made. The original filenames are potentially useful when tracing issues. Downstream steps use a file to connect the filenames with the appropriate strain or isotype. To begin running the pipeline you will cd to the directory containing the raw FASTQs and run the pipeline: # cd to directory of fastqs nextflow run Andersenlab/trimmomatic-nf The resulting trimmed FASTQs will be output in the processed directory located up one level from the current directory. For example: FASTQs are deposited in this directory /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq You run the pipeline while sitting in the same directory: /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq And results are output in the following directory: /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq","title":"Trimming"},{"location":"pipeline-trimming/#trimmomatic-nf","text":"The trimmomatic workflow should be used to initially process sequence data. You should not use the trimmomatic workflow on low-coverage NIL or RIL data.","title":"trimmomatic-nf"},{"location":"pipeline-trimming/#usage","text":"New sequence data should be stored in the appropriate path on the cluster in the raw directory. For example, when adding new sequence data for wild isolates, data should be added to a new folder here: /projects/b1059/data/fastq/WI/dna/raw/<folder_name> Important All FASTQs should end with a _1.fq.gz or a _2.fq.gz . To rename FASTQs you can use: rename --dry-run --subst .fastq.gz .fq.gz --subst _R1_001 _1 --subst _R2_001 *.fastq.gz Outside of these simple changes to the filenames, no further changes should be made. The original filenames are potentially useful when tracing issues. Downstream steps use a file to connect the filenames with the appropriate strain or isotype. To begin running the pipeline you will cd to the directory containing the raw FASTQs and run the pipeline: # cd to directory of fastqs nextflow run Andersenlab/trimmomatic-nf The resulting trimmed FASTQs will be output in the processed directory located up one level from the current directory. For example: FASTQs are deposited in this directory /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq You run the pipeline while sitting in the same directory: /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq And results are output in the following directory: /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq","title":"Usage"},{"location":"pipeline-wi/","text":"wi-nf \u00b6 The wi-nf pipeline aligns, calls variants, and performs analysis from wild isolate sequence data. wi-nf Usage Usage Docker File Running the pipeline on Quest --debug --cores --out --fqs (FASTQs) --fqs_file_prefix --reference --tmpdir Adding new sequence data Output log.txt alignment/ cegwas/ isotype isotype/tsv/ isotype/vcf/ phenotype/ popgen/ report/ track/ Variation/ Usage \u00b6 \u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2584\u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2588\u2591\u2588\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2588\u2591\u2588\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u258c \u2580\u2580 \u2580\u2580 \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2580 \u2580\u2580 \u2580 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test ${params.debug} --cores Regular job cores ${params.cores} --out Directory to output results ${params.out} --fqs fastq file (see help) ${params.fqs} --fq_file_prefix fastq prefix ${params.fq_file_prefix} --reference Reference Genome ${params.reference} --annotation_reference SnpEff annotation ${params.annotation_reference} --bamdir Location for bams ${params.bamdir} --tmpdir A temporary directory ${params.tmpdir} HELP: http://andersenlab.org/dry-guide/pipeline-wi/ Usage \u00b6 Docker File \u00b6 andersenlab/wi-nf is the docker file for the wi-nf pipeline. It can be converted to a singularity image for use later. Running the pipeline on Quest \u00b6 Typical usage: nextflow run main.nf -profile quest -resume -with-report report.html --debug \u00b6 The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. You can use the following command to debug: nextflow run main.nf --debug --reference=<path to reference> -with-docker andersenlab/wi-nf Note the use of the -with-docker flag. --cores \u00b6 The number of cores to use during alignments and variant calling. --out \u00b6 A directory in which to output results. By default it will be WI-YYYY-MM-DD where YYYY-MM-DD is todays date. --fqs (FASTQs) \u00b6 When running the wi-nf pipeline you must provide a sample sheet that tells it where fastqs are and which samples group into isotypes. By default, this is the sample sheet in the base of the wi-nf repo ( SM_sample_sheet.tsv ), but can be specified using --fqs if an alternative is required. The sample sheet provides information on the isotype, fastq_pairs, library, location of fastqs, and sequencing folder. More information on the sample sheet and adding new sequence data in the adding new sequence data section. --fqs_file_prefix \u00b6 The sample sheet is constructed using the scripts/construct_SM_sheet.sh script. When new sequence data is added, this needs to be modified to add information about the new FASTQs. Changes should be committed to the repo. More information on this in the adding new sequence data section. Unfortunately, no sequencing center can agree on how to name FASTQs, so you'll have to come up with a way to pull out the appropriate Important You have to call new isotypes using the concordance script before they can be processed using wi-nf. When you run the script, it will output the SM_Sample_sheet.tsv using the location of FASTQs for Quest. You The script will output a sample sheet that defines information about FASTQs. By default absolute or relative mannerThis file defines the fastqs that should be processed. The fastq files are relative to that file. The fastq sheet details the FASTQ files and their associated strains. It should be tab-delimited and look like this: Sample sheet structure ECA551 ECA551_S2_L004 S2 ECA551_S2_L004_1P.fq.gz ECA551_S2_L004_2P.fq.gz seq_folder_1 ECA552 ECA552_S16_L004 S16 ECA552_S16_L004_1P.fq.gz ECA552_S16_L004_2P.fq.gz seq_folder_1 ECA571 ECA571_S5_L004 S5 ECA571_S5_L004_1P.fq.gz ECA571_S5_L004_2P.fq.gz seq_folder_1 ECA571 ECA572_S6_L004 S6 ECA572_S6_L004_1P.fq.gz ECA572_S6_L004_2P.fq.gz seq_folder_1 Notice that the file does not include a header. The table with corresponding columns looks like this. isotype fastq_pair_id library fastq-1-path fastq-2-path sequencing_folder ECA551 ECA551_S2_L004 S2 ECA551_S2_L004_1P.fq.gz ECA551_S2_L004_2P.fq.gz seq_folder_1 ECA552 ECA552_S16_L004 S16 ECA552_S16_L004_1P.fq.gz ECA552_S16_L004_2P.fq.gz seq_folder_1 The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. --reference \u00b6 A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz --tmpdir \u00b6 A directory for storing temporary data. Adding new sequence data \u00b6 Output \u00b6 The output from the pipeline is structured to enable it to easily be integrated with CeNDR. The final output directory looks like this: \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 alignment \u2502 \u251c\u2500\u2500 isotype_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 isotype_bam_stats.tsv \u2502 \u251c\u2500\u2500 isotype_coverage.full.tsv \u2502 \u2514\u2500\u2500 isotype_coverage.tsv \u251c\u2500\u2500 cegwas \u2502 \u251c\u2500\u2500 kinship.Rda \u2502 \u2514\u2500\u2500 snps.Rda \u251c\u2500\u2500 isotype \u2502 \u251c\u2500\u2500 tsv \u2502 \u2502 \u251c\u2500\u2500 <isotype>.(date).tsv.gz \u2502 \u2502 \u2514\u2500\u2500 <isotype>.(date).tsv.gz \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 <isotype>.(date).vcf.gz \u2502 \u2514\u2500\u2500 <isotype>.(date).vcf.gz.tbi \u251c\u2500\u2500 phenotype \u2502 \u251c\u2500\u2500 MT_content.tsv \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 telseq.tsv \u251c\u2500\u2500 popgen \u2502 \u251c\u2500\u2500 WI.(date).tajima.bed.gz \u2502 \u251c\u2500\u2500 WI.(date).tajima.bed.gz.tbi \u2502 \u2514\u2500\u2500 trees \u2502 \u251c\u2500\u2500 I.pdf \u2502 \u251c\u2500\u2500 I.png \u2502 \u251c\u2500\u2500 I.tree \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 genome.pdf \u2502 \u251c\u2500\u2500 genome.png \u2502 \u2514\u2500\u2500 genome.tree \u251c\u2500\u2500 report \u2502 \u251c\u2500\u2500 multiqc.html \u2502 \u2514\u2500\u2500 multiqc_data \u2502 \u251c\u2500\u2500 multiqc_bcftools_stats.json \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_fastqc.json \u2502 \u251c\u2500\u2500 multiqc_general_stats.json \u2502 \u251c\u2500\u2500 multiqc_picard_AlignmentSummaryMetrics.json \u2502 \u251c\u2500\u2500 multiqc_picard_dups.json \u2502 \u251c\u2500\u2500 multiqc_picard_insertSize.json \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.json \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.json \u2502 \u251c\u2500\u2500 multiqc_snpeff.json \u2502 \u2514\u2500\u2500 multiqc_sources.json \u251c\u2500\u2500 tracks \u2502 \u251c\u2500\u2500 (date).HIGH.bed.gz \u2502 \u251c\u2500\u2500 (date).HIGH.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).LOW.bed.gz \u2502 \u251c\u2500\u2500 (date).LOW.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).MODERATE.bed.gz \u2502 \u251c\u2500\u2500 (date).MODERATE.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).MODIFIER.bed.gz \u2502 \u251c\u2500\u2500 (date).MODIFIER.bed.gz.tbi \u2502 \u251c\u2500\u2500 phastcons.bed.gz \u2502 \u251c\u2500\u2500 phastcons.bed.gz.tbi \u2502 \u251c\u2500\u2500 phylop.bed.gz \u2502 \u2514\u2500\u2500 phylop.bed.gz.tbi \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.csi \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).soft-filter.stats.txt \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.csi \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).hard-filter.stats.txt \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.tsv \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.frequency.tsv \u251c\u2500\u2500 WI.(date).impute.vcf.gz \u251c\u2500\u2500 WI.(date).impute.vcf.gz.csi \u251c\u2500\u2500 WI.(date).impute.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).impute.stats.txt \u251c\u2500\u2500 sitelist.tsv.gz \u2514\u2500\u2500 sitelist.tsv.gz.tbi log.txt \u00b6 A summary of the nextflow run. alignment/ \u00b6 Alignment statistics by isotype isotype_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. isotype_bam_stats.tsv - BAM summary at the sample level isotype_coverage.full.tsv - Coverage at the sample level isotype_coverage.tsv - Simple coverage at the sample level. cegwas/ \u00b6 kinship.Rda - A kinship matrix constructed from WI.(date).impute.vcf.gz snps.Rda - A mapping snp set generated from WI.(date).hard-filter.vcf.gz isotype \u00b6 This directory contains files that integrate with the genome browser on CeDNR. isotype/tsv/ \u00b6 This directory contains tsv's that are used to show where variants are in CeNDR. isotype/vcf/ \u00b6 This direcoty contains the VCFs of isotypes. phenotype/ \u00b6 MT_content.tsv - Mitochondrial content (MtDNA cov / Nuclear cov). kmers.tsv - 6-mers for each isotype. telseq.tsv - Telomere length for each isotype ~ split out by read group. popgen/ \u00b6 WI.(date).tajima.bed.gz - Tajima's D bedfile for use on the report viewer of CeNDR. WI.(date).tajima.bed.gz.tbi - Tajima's D index. trees/ - Phylogenetic trees for each chromosome and the entire genome. I.(pdf|png|tree) ... genome.(pdf|png|tree) report/ \u00b6 multiqc.html - A comprehensive report of the sequencing run. multiqc_data/ multiqc_bcftools_stats.json multiqc_data.json multiqc_fastqc.json multiqc_general_stats.json multiqc_picard_AlignmentSummaryMetrics.json multiqc_picard_dups.json multiqc_picard_insertSize.json multiqc_samtools_idxstats.json multiqc_samtools_stats.json multiqc_snpeff.json multiqc_sources.json track/ \u00b6 (date).LOW.bed.gz(+.tbi) - LOW effect mutations bed track and index. (date).MODERATE.bed.gz(+.tbi) - MODERATE effect mutations bed track and index. (date).HIGH.bed.gz(+.tbi) - HIGH effect mutations bed track and index. (date).MODIFIER.bed.gz(+.tbi) - MODERATE effect mutations bed track and index. phastcons.bed.gz(+.tbi) - Phastcons bed track and index. phylop.bed.gz(+.tbi) - PhyloP bed track and index Variation/ \u00b6 WI.(date).soft-filter.vcf.gz(+.csi|+.tbi) - WI.(date).soft-filter.stats.txt - WI.(date).hard-filter.vcf.gz(+.csi|+.tbi) - WI.(date).hard-filter.stats.txt - WI.(date).hard-filter.genotypes.tsv - WI.(date).hard-filter.genotypes.frequency.tsv - WI.(date).impute.vcf.gz(+.csi|+.tbi) - WI.(date).impute.stats.txt - sitelist.tsv.gz(+.tbi) - Union of all sites identified in original SNV variant calling round.","title":"WI"},{"location":"pipeline-wi/#wi-nf","text":"The wi-nf pipeline aligns, calls variants, and performs analysis from wild isolate sequence data. wi-nf Usage Usage Docker File Running the pipeline on Quest --debug --cores --out --fqs (FASTQs) --fqs_file_prefix --reference --tmpdir Adding new sequence data Output log.txt alignment/ cegwas/ isotype isotype/tsv/ isotype/vcf/ phenotype/ popgen/ report/ track/ Variation/","title":"wi-nf"},{"location":"pipeline-wi/#usage","text":"\u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2584\u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2588\u2591\u2588\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2588\u2591\u2588\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u258c \u2580\u2580 \u2580\u2580 \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2580 \u2580\u2580 \u2580 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test ${params.debug} --cores Regular job cores ${params.cores} --out Directory to output results ${params.out} --fqs fastq file (see help) ${params.fqs} --fq_file_prefix fastq prefix ${params.fq_file_prefix} --reference Reference Genome ${params.reference} --annotation_reference SnpEff annotation ${params.annotation_reference} --bamdir Location for bams ${params.bamdir} --tmpdir A temporary directory ${params.tmpdir} HELP: http://andersenlab.org/dry-guide/pipeline-wi/","title":"Usage"},{"location":"pipeline-wi/#usage_1","text":"","title":"Usage"},{"location":"pipeline-wi/#docker_file","text":"andersenlab/wi-nf is the docker file for the wi-nf pipeline. It can be converted to a singularity image for use later.","title":"Docker File"},{"location":"pipeline-wi/#running_the_pipeline_on_quest","text":"Typical usage: nextflow run main.nf -profile quest -resume -with-report report.html","title":"Running the pipeline on Quest"},{"location":"pipeline-wi/#--debug","text":"The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. You can use the following command to debug: nextflow run main.nf --debug --reference=<path to reference> -with-docker andersenlab/wi-nf Note the use of the -with-docker flag.","title":"--debug"},{"location":"pipeline-wi/#--cores","text":"The number of cores to use during alignments and variant calling.","title":"--cores"},{"location":"pipeline-wi/#--out","text":"A directory in which to output results. By default it will be WI-YYYY-MM-DD where YYYY-MM-DD is todays date.","title":"--out"},{"location":"pipeline-wi/#--fqs_fastqs","text":"When running the wi-nf pipeline you must provide a sample sheet that tells it where fastqs are and which samples group into isotypes. By default, this is the sample sheet in the base of the wi-nf repo ( SM_sample_sheet.tsv ), but can be specified using --fqs if an alternative is required. The sample sheet provides information on the isotype, fastq_pairs, library, location of fastqs, and sequencing folder. More information on the sample sheet and adding new sequence data in the adding new sequence data section.","title":"--fqs (FASTQs)"},{"location":"pipeline-wi/#--fqs_file_prefix","text":"The sample sheet is constructed using the scripts/construct_SM_sheet.sh script. When new sequence data is added, this needs to be modified to add information about the new FASTQs. Changes should be committed to the repo. More information on this in the adding new sequence data section. Unfortunately, no sequencing center can agree on how to name FASTQs, so you'll have to come up with a way to pull out the appropriate Important You have to call new isotypes using the concordance script before they can be processed using wi-nf. When you run the script, it will output the SM_Sample_sheet.tsv using the location of FASTQs for Quest. You The script will output a sample sheet that defines information about FASTQs. By default absolute or relative mannerThis file defines the fastqs that should be processed. The fastq files are relative to that file. The fastq sheet details the FASTQ files and their associated strains. It should be tab-delimited and look like this: Sample sheet structure ECA551 ECA551_S2_L004 S2 ECA551_S2_L004_1P.fq.gz ECA551_S2_L004_2P.fq.gz seq_folder_1 ECA552 ECA552_S16_L004 S16 ECA552_S16_L004_1P.fq.gz ECA552_S16_L004_2P.fq.gz seq_folder_1 ECA571 ECA571_S5_L004 S5 ECA571_S5_L004_1P.fq.gz ECA571_S5_L004_2P.fq.gz seq_folder_1 ECA571 ECA572_S6_L004 S6 ECA572_S6_L004_1P.fq.gz ECA572_S6_L004_2P.fq.gz seq_folder_1 Notice that the file does not include a header. The table with corresponding columns looks like this. isotype fastq_pair_id library fastq-1-path fastq-2-path sequencing_folder ECA551 ECA551_S2_L004 S2 ECA551_S2_L004_1P.fq.gz ECA551_S2_L004_2P.fq.gz seq_folder_1 ECA552 ECA552_S16_L004 S16 ECA552_S16_L004_1P.fq.gz ECA552_S16_L004_2P.fq.gz seq_folder_1 The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible.","title":"--fqs_file_prefix"},{"location":"pipeline-wi/#--reference","text":"A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz","title":"--reference"},{"location":"pipeline-wi/#--tmpdir","text":"A directory for storing temporary data.","title":"--tmpdir"},{"location":"pipeline-wi/#adding_new_sequence_data","text":"","title":"Adding new sequence data"},{"location":"pipeline-wi/#output","text":"The output from the pipeline is structured to enable it to easily be integrated with CeNDR. The final output directory looks like this: \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 alignment \u2502 \u251c\u2500\u2500 isotype_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 isotype_bam_stats.tsv \u2502 \u251c\u2500\u2500 isotype_coverage.full.tsv \u2502 \u2514\u2500\u2500 isotype_coverage.tsv \u251c\u2500\u2500 cegwas \u2502 \u251c\u2500\u2500 kinship.Rda \u2502 \u2514\u2500\u2500 snps.Rda \u251c\u2500\u2500 isotype \u2502 \u251c\u2500\u2500 tsv \u2502 \u2502 \u251c\u2500\u2500 <isotype>.(date).tsv.gz \u2502 \u2502 \u2514\u2500\u2500 <isotype>.(date).tsv.gz \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 <isotype>.(date).vcf.gz \u2502 \u2514\u2500\u2500 <isotype>.(date).vcf.gz.tbi \u251c\u2500\u2500 phenotype \u2502 \u251c\u2500\u2500 MT_content.tsv \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 telseq.tsv \u251c\u2500\u2500 popgen \u2502 \u251c\u2500\u2500 WI.(date).tajima.bed.gz \u2502 \u251c\u2500\u2500 WI.(date).tajima.bed.gz.tbi \u2502 \u2514\u2500\u2500 trees \u2502 \u251c\u2500\u2500 I.pdf \u2502 \u251c\u2500\u2500 I.png \u2502 \u251c\u2500\u2500 I.tree \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 genome.pdf \u2502 \u251c\u2500\u2500 genome.png \u2502 \u2514\u2500\u2500 genome.tree \u251c\u2500\u2500 report \u2502 \u251c\u2500\u2500 multiqc.html \u2502 \u2514\u2500\u2500 multiqc_data \u2502 \u251c\u2500\u2500 multiqc_bcftools_stats.json \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_fastqc.json \u2502 \u251c\u2500\u2500 multiqc_general_stats.json \u2502 \u251c\u2500\u2500 multiqc_picard_AlignmentSummaryMetrics.json \u2502 \u251c\u2500\u2500 multiqc_picard_dups.json \u2502 \u251c\u2500\u2500 multiqc_picard_insertSize.json \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.json \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.json \u2502 \u251c\u2500\u2500 multiqc_snpeff.json \u2502 \u2514\u2500\u2500 multiqc_sources.json \u251c\u2500\u2500 tracks \u2502 \u251c\u2500\u2500 (date).HIGH.bed.gz \u2502 \u251c\u2500\u2500 (date).HIGH.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).LOW.bed.gz \u2502 \u251c\u2500\u2500 (date).LOW.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).MODERATE.bed.gz \u2502 \u251c\u2500\u2500 (date).MODERATE.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).MODIFIER.bed.gz \u2502 \u251c\u2500\u2500 (date).MODIFIER.bed.gz.tbi \u2502 \u251c\u2500\u2500 phastcons.bed.gz \u2502 \u251c\u2500\u2500 phastcons.bed.gz.tbi \u2502 \u251c\u2500\u2500 phylop.bed.gz \u2502 \u2514\u2500\u2500 phylop.bed.gz.tbi \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.csi \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).soft-filter.stats.txt \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.csi \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).hard-filter.stats.txt \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.tsv \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.frequency.tsv \u251c\u2500\u2500 WI.(date).impute.vcf.gz \u251c\u2500\u2500 WI.(date).impute.vcf.gz.csi \u251c\u2500\u2500 WI.(date).impute.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).impute.stats.txt \u251c\u2500\u2500 sitelist.tsv.gz \u2514\u2500\u2500 sitelist.tsv.gz.tbi","title":"Output"},{"location":"pipeline-wi/#logtxt","text":"A summary of the nextflow run.","title":"log.txt"},{"location":"pipeline-wi/#alignment","text":"Alignment statistics by isotype isotype_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. isotype_bam_stats.tsv - BAM summary at the sample level isotype_coverage.full.tsv - Coverage at the sample level isotype_coverage.tsv - Simple coverage at the sample level.","title":"alignment/"},{"location":"pipeline-wi/#cegwas","text":"kinship.Rda - A kinship matrix constructed from WI.(date).impute.vcf.gz snps.Rda - A mapping snp set generated from WI.(date).hard-filter.vcf.gz","title":"cegwas/"},{"location":"pipeline-wi/#isotype","text":"This directory contains files that integrate with the genome browser on CeDNR.","title":"isotype"},{"location":"pipeline-wi/#isotypetsv","text":"This directory contains tsv's that are used to show where variants are in CeNDR.","title":"isotype/tsv/"},{"location":"pipeline-wi/#isotypevcf","text":"This direcoty contains the VCFs of isotypes.","title":"isotype/vcf/"},{"location":"pipeline-wi/#phenotype","text":"MT_content.tsv - Mitochondrial content (MtDNA cov / Nuclear cov). kmers.tsv - 6-mers for each isotype. telseq.tsv - Telomere length for each isotype ~ split out by read group.","title":"phenotype/"},{"location":"pipeline-wi/#popgen","text":"WI.(date).tajima.bed.gz - Tajima's D bedfile for use on the report viewer of CeNDR. WI.(date).tajima.bed.gz.tbi - Tajima's D index. trees/ - Phylogenetic trees for each chromosome and the entire genome. I.(pdf|png|tree) ... genome.(pdf|png|tree)","title":"popgen/"},{"location":"pipeline-wi/#report","text":"multiqc.html - A comprehensive report of the sequencing run. multiqc_data/ multiqc_bcftools_stats.json multiqc_data.json multiqc_fastqc.json multiqc_general_stats.json multiqc_picard_AlignmentSummaryMetrics.json multiqc_picard_dups.json multiqc_picard_insertSize.json multiqc_samtools_idxstats.json multiqc_samtools_stats.json multiqc_snpeff.json multiqc_sources.json","title":"report/"},{"location":"pipeline-wi/#track","text":"(date).LOW.bed.gz(+.tbi) - LOW effect mutations bed track and index. (date).MODERATE.bed.gz(+.tbi) - MODERATE effect mutations bed track and index. (date).HIGH.bed.gz(+.tbi) - HIGH effect mutations bed track and index. (date).MODIFIER.bed.gz(+.tbi) - MODERATE effect mutations bed track and index. phastcons.bed.gz(+.tbi) - Phastcons bed track and index. phylop.bed.gz(+.tbi) - PhyloP bed track and index","title":"track/"},{"location":"pipeline-wi/#variation","text":"WI.(date).soft-filter.vcf.gz(+.csi|+.tbi) - WI.(date).soft-filter.stats.txt - WI.(date).hard-filter.vcf.gz(+.csi|+.tbi) - WI.(date).hard-filter.stats.txt - WI.(date).hard-filter.genotypes.tsv - WI.(date).hard-filter.genotypes.frequency.tsv - WI.(date).impute.vcf.gz(+.csi|+.tbi) - WI.(date).impute.stats.txt - sitelist.tsv.gz(+.tbi) - Union of all sites identified in original SNV variant calling round.","title":"Variation/"},{"location":"quest-intro/","text":"Introduction \u00b6 Introduction Signing into Quest Home Directory Projects b1059 Structure Installing and using software Using module Using linuxbrew Starting interactive jobs The Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation Signing into Quest \u00b6 After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu I recommend setting an alias in your .bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below). Home Directory \u00b6 Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc. . More information is provided below to help install and use software. Projects \u00b6 Quest is broadly organized into projects. Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 155 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. b1059 - The Andersen Lab Project. b1059 does not have any nodes associated with it, but it does have 10 Tb of storage. b1059 storage is located at: /projects/b1059/ . b1059 Structure \u00b6 The overall directory structure of /projects/b1059/ is as follows: data - fastqs, bams, and other data. software - Centrally used software can be stored here. projects - Individual projects that have a definitive endpoint. workflows - Workflows used in ongoing projects. For example, Wild Isolate, RIL, and NIL sequencing. analysis - Output from workflows run on ongoing projects . Installing and using software \u00b6 Using module \u00b6 Quest comes with a command called module that allows you to load software for use. The following commands can be used to see/load software. module avail - List available software. module load software/version - Load a software package (e.g. module load R/3.3.1 ). Using linuxbrew \u00b6 Linuxbrew is a fork of Homebrew - which is a package manager for MacOSX. Linuxbrew should be installed in your home directory using the following command: A lot of the software you use can be installed using a bottle format. The Andersen Lab has access to the 'Genomics' project Starting interactive jobs \u00b6 The command below will start up an interactive job. msub -I -A b1042","title":"Introduction"},{"location":"quest-intro/#introduction","text":"Introduction Signing into Quest Home Directory Projects b1059 Structure Installing and using software Using module Using linuxbrew Starting interactive jobs The Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation","title":"Introduction"},{"location":"quest-intro/#signing_into_quest","text":"After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu I recommend setting an alias in your .bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below).","title":"Signing into Quest"},{"location":"quest-intro/#home_directory","text":"Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc. . More information is provided below to help install and use software.","title":"Home Directory"},{"location":"quest-intro/#projects","text":"Quest is broadly organized into projects. Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 155 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. b1059 - The Andersen Lab Project. b1059 does not have any nodes associated with it, but it does have 10 Tb of storage. b1059 storage is located at: /projects/b1059/ .","title":"Projects"},{"location":"quest-intro/#b1059_structure","text":"The overall directory structure of /projects/b1059/ is as follows: data - fastqs, bams, and other data. software - Centrally used software can be stored here. projects - Individual projects that have a definitive endpoint. workflows - Workflows used in ongoing projects. For example, Wild Isolate, RIL, and NIL sequencing. analysis - Output from workflows run on ongoing projects .","title":"b1059 Structure"},{"location":"quest-intro/#installing_and_using_software","text":"","title":"Installing and using software"},{"location":"quest-intro/#using_module","text":"Quest comes with a command called module that allows you to load software for use. The following commands can be used to see/load software. module avail - List available software. module load software/version - Load a software package (e.g. module load R/3.3.1 ).","title":"Using module"},{"location":"quest-intro/#using_linuxbrew","text":"Linuxbrew is a fork of Homebrew - which is a package manager for MacOSX. Linuxbrew should be installed in your home directory using the following command: A lot of the software you use can be installed using a bottle format. The Andersen Lab has access to the 'Genomics' project","title":"Using linuxbrew"},{"location":"quest-intro/#starting_interactive_jobs","text":"The command below will start up an interactive job. msub -I -A b1042","title":"Starting interactive jobs"},{"location":"quest-mount/","text":"1. Download and Install Fuse for Mac OS https://osxfuse.github.io/ 2. Install sshfs You can use the link on https://osxfuse.github.io/ or use: brew install sshfs 3. Create a folder in your documents called b1059 mkdir ~/b1059 4. Mount our labs quest project folder ( b1059 ) to the b1059 folder you created locally sshfs <NETID>@quest.it.northwestern.edu:/projects/b1059/ ~/Documents/b1059 -ovolname=b1059 To mount alignments of isotypes at this location: sshfs <NETID>@quest.it.northwestern.edu:/projects/b1059/data/alignments/WI/isotype ~/Documents/b1059 -ovolname=b1059","title":"Mounting Quest"},{"location":"quest-nextflow/","text":"Installation Quest cluster configuration Global Configuration: ~/.nextflow/config Pipeline Configuration Resources Installation \u00b6 Important If you haven't already, take a look at Quest Setup for more information on how to setup your environment on Quest. That page has a script which can automate much of what you see on this page, although I am leaving this page here as it has a lot of detail on configuring nextflow. Nextflow can be installed with linuxbrew . Use: brew tap homebrew/science brew install nextflow Quest cluster configuration \u00b6 Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config . Global Configuration: ~/.nextflow/config \u00b6 In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { module='R/3.3.1' executor = 'pbs' queue = 'genomicsguest' clusterOptions = '-A b1042 -l walltime=24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: module='R/3.3.1' - automatically loads R for all processes. Sets the executor to pbs (which is what Quest uses) Sets the queue to genomicsguest which submits jobs to genomics nodes. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. tmpDir - Creates a temporary working directory. This can be used within workflows when necessary. Pipeline Configuration \u00b6 At the pipeline level you will want to define things like the number of processors for a given process, the analysis output directory, or the reference genome used. The following is an example from the wi-nf pipeline: genome = \"WS245\" reference = \"/projects/b1059/data/genomes/c_elegans/${genome}/${genome}.fa.gz\" alignment_cores = 16 variant_cores = 6 compression_threads = 4 //date = new Date().format( 'yyyyMMdd' ) date = 20170531 analysis_dir = \"/projects/b1059/analysis/WI-${date}\" SM_alignments_dir = \"/projects/b1059/data/alignments\" beagle_location = \"/projects/b1059/software/beagle/beagle.jar\" process { module='gcc/5.1.0:R/3.3.1' $perform_alignment { cpus = 4 } $call_variants_individual { cpus = 6 memory = '8G' } $call_variants_union { cpus = 6 memory = '8G' } $merge_union_vcf { cpus = 20 memory = '50G' } $filter_union_vcf { cpus = 20 memory = '50G' } $annotate_vcf_snpeff { module='gcc/4.6.3' } $generate_isotype_vcf { errorStrategy='retry' maxRetries=20 maxForks=10 } } Resources \u00b6 Nextflow documentation - Awesome Nextflow pipeline examples - Repository of great nextflow pipelines.","title":"Nextflow"},{"location":"quest-nextflow/#installation","text":"Important If you haven't already, take a look at Quest Setup for more information on how to setup your environment on Quest. That page has a script which can automate much of what you see on this page, although I am leaving this page here as it has a lot of detail on configuring nextflow. Nextflow can be installed with linuxbrew . Use: brew tap homebrew/science brew install nextflow","title":"Installation"},{"location":"quest-nextflow/#quest_cluster_configuration","text":"Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config .","title":"Quest cluster configuration"},{"location":"quest-nextflow/#global_configuration_nextflowconfig","text":"In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { module='R/3.3.1' executor = 'pbs' queue = 'genomicsguest' clusterOptions = '-A b1042 -l walltime=24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: module='R/3.3.1' - automatically loads R for all processes. Sets the executor to pbs (which is what Quest uses) Sets the queue to genomicsguest which submits jobs to genomics nodes. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. tmpDir - Creates a temporary working directory. This can be used within workflows when necessary.","title":"Global Configuration: ~/.nextflow/config"},{"location":"quest-nextflow/#pipeline_configuration","text":"At the pipeline level you will want to define things like the number of processors for a given process, the analysis output directory, or the reference genome used. The following is an example from the wi-nf pipeline: genome = \"WS245\" reference = \"/projects/b1059/data/genomes/c_elegans/${genome}/${genome}.fa.gz\" alignment_cores = 16 variant_cores = 6 compression_threads = 4 //date = new Date().format( 'yyyyMMdd' ) date = 20170531 analysis_dir = \"/projects/b1059/analysis/WI-${date}\" SM_alignments_dir = \"/projects/b1059/data/alignments\" beagle_location = \"/projects/b1059/software/beagle/beagle.jar\" process { module='gcc/5.1.0:R/3.3.1' $perform_alignment { cpus = 4 } $call_variants_individual { cpus = 6 memory = '8G' } $call_variants_union { cpus = 6 memory = '8G' } $merge_union_vcf { cpus = 20 memory = '50G' } $filter_union_vcf { cpus = 20 memory = '50G' } $annotate_vcf_snpeff { module='gcc/4.6.3' } $generate_isotype_vcf { errorStrategy='retry' maxRetries=20 maxForks=10 } }","title":"Pipeline Configuration"},{"location":"quest-nextflow/#resources","text":"Nextflow documentation - Awesome Nextflow pipeline examples - Repository of great nextflow pipelines.","title":"Resources"},{"location":"quest-setup/","text":"Setting Up Quest \u00b6 Setting up your environment \u00b6 In order to do any sort of analysis you will need to set up your environment first. Daniel Cook has written a script that will set up the environment on quest. The script works as of 2017-12-05. However, because software gets updated frequently it may break and may need to be updated. However - it will get you 90% of the way there. You need to read the script before running it and to understand what it is doing. And you may need to make adjustments if it is out of date. Quest Setup Scripts What do these scripts do? \u00b6 Cleans up the existing environment - Removes ~/.linuxbrew and ~/.cache \u00b6 These older files are removed. Installs linuxbrew \u00b6 Linuxbrew is the tool used to install additional software. Installs the following software \u00b6 curl git nextflow pigz igvtools vcfanno snpeff fastq-tools muscle sambamba google-cloud-sdk # utilities pyenv pyenv-virtualenv autojump Replaces your .bash_profile \u00b6 The .bash_profile will be replaced with the one found on the Quest Setup Scripts page. Installs Python 2.7.11 and Python 3.6.0 \u00b6 These are installed using pyenv . python 2.7.11 is set as the default global version. Installs python modules for python 2.7.11 \u00b6 pip numpy setuptools cython scipy gcloud vcf-kit bam-toolbox (for coverage calculations) Configures nextflow \u00b6 See Quest-Nextflow for further details on what exactly is being done here. ... And More \u00b6 Please read the script prior to running it. Running the script \u00b6 curl https://gist.github.com/danielecook/aed4a6fd53195fca7a3297f054d613c7#file-quest_setup-sh | bash","title":"Setup"},{"location":"quest-setup/#setting_up_quest","text":"","title":"Setting Up Quest"},{"location":"quest-setup/#setting_up_your_environment","text":"In order to do any sort of analysis you will need to set up your environment first. Daniel Cook has written a script that will set up the environment on quest. The script works as of 2017-12-05. However, because software gets updated frequently it may break and may need to be updated. However - it will get you 90% of the way there. You need to read the script before running it and to understand what it is doing. And you may need to make adjustments if it is out of date. Quest Setup Scripts","title":"Setting up your environment"},{"location":"quest-setup/#what_do_these_scripts_do","text":"","title":"What do these scripts do?"},{"location":"quest-setup/#cleans_up_the_existing_environment_-_removes_linuxbrew_and_cache","text":"These older files are removed.","title":"Cleans up the existing environment - Removes ~/.linuxbrew and ~/.cache"},{"location":"quest-setup/#installs_linuxbrew","text":"Linuxbrew is the tool used to install additional software.","title":"Installs linuxbrew"},{"location":"quest-setup/#installs_the_following_software","text":"curl git nextflow pigz igvtools vcfanno snpeff fastq-tools muscle sambamba google-cloud-sdk # utilities pyenv pyenv-virtualenv autojump","title":"Installs the following software"},{"location":"quest-setup/#replaces_your_bash_profile","text":"The .bash_profile will be replaced with the one found on the Quest Setup Scripts page.","title":"Replaces your .bash_profile"},{"location":"quest-setup/#installs_python_2711_and_python_360","text":"These are installed using pyenv . python 2.7.11 is set as the default global version.","title":"Installs Python 2.7.11 and Python 3.6.0"},{"location":"quest-setup/#installs_python_modules_for_python_2711","text":"pip numpy setuptools cython scipy gcloud vcf-kit bam-toolbox (for coverage calculations)","title":"Installs python modules for python 2.7.11"},{"location":"quest-setup/#configures_nextflow","text":"See Quest-Nextflow for further details on what exactly is being done here.","title":"Configures nextflow"},{"location":"quest-setup/#and_more","text":"Please read the script prior to running it.","title":"... And More"},{"location":"quest-setup/#running_the_script","text":"curl https://gist.github.com/danielecook/aed4a6fd53195fca7a3297f054d613c7#file-quest_setup-sh | bash","title":"Running the script"},{"location":"r/","text":"R \u00b6 R Packages \u00b6 The Andersen lab maintains several R packages. cegwas \u00b6 linkagemapping \u00b6 easysorter \u00b6","title":"R"},{"location":"r/#r","text":"","title":"R"},{"location":"r/#r_packages","text":"The Andersen lab maintains several R packages.","title":"R Packages"},{"location":"r/#cegwas","text":"","title":"cegwas"},{"location":"r/#linkagemapping","text":"","title":"linkagemapping"},{"location":"r/#easysorter","text":"","title":"easysorter"},{"location":"travis-ci/","text":"Setting up Quest \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Travis ci"},{"location":"travis-ci/#setting_up_quest","text":"For full documentation visit mkdocs.org .","title":"Setting up Quest"},{"location":"travis-ci/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"travis-ci/#project_layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"}]}