{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Andersen Lab Dry Guide \u00b6 The Dry Guide details the computational infrastructure and tasks used in the Andersen Lab.","title":"Home"},{"location":"#welcome_to_the_andersen_lab_dry_guide","text":"The Dry Guide details the computational infrastructure and tasks used in the Andersen Lab.","title":"Welcome to the Andersen Lab Dry Guide"},{"location":"adding-seq-data/","text":"Introduction \u00b6 This section is for adding genomic sequencing data (.tsv files) onto the existing dataset provided and displayed on the NILs browser page on Andersenlab.org . Basic Commands To Know Prior \u00b6 You should freshen up on the following terminal commands. cd - change directories rm - delete files cp - make a copy git Your Sequencing Data \u00b6 In order for your sequencing data to be properly added, it is important to make sure that there are no empty/additional lines located at the bottom of your .tsv file. What you do not want What you do want Once your file has no empty lines at the bottom, save the file and move onto the next instructions below. Step By Step Instructions \u00b6 Add your .tsv file into the pages folder of your Andersenlab github directory. Open terminal and use the cd command to change directories into the pages folder in your Andersenlab github directory. If you did everything correctly, when you type ls into your terminal, it should look something like this. Then run the following commands in your terminal (while still in your pages directory): $ cp yourFileName.tsv copy.tsv $ python addDataTogt_hmm.tsv.py After running the above commands, your sequencing data has now been added to the existing NILs dataset on Andersenlab.org. You can now remove your .tsv from the pages directory by using the rm command in your terminal. Finally, commit your changes and push your code to update the Andersenlab github.","title":"Adding NILs Sequencing Data"},{"location":"adding-seq-data/#introduction","text":"This section is for adding genomic sequencing data (.tsv files) onto the existing dataset provided and displayed on the NILs browser page on Andersenlab.org .","title":"Introduction"},{"location":"adding-seq-data/#basic_commands_to_know_prior","text":"You should freshen up on the following terminal commands. cd - change directories rm - delete files cp - make a copy git","title":"Basic Commands To Know Prior"},{"location":"adding-seq-data/#your_sequencing_data","text":"In order for your sequencing data to be properly added, it is important to make sure that there are no empty/additional lines located at the bottom of your .tsv file. What you do not want What you do want Once your file has no empty lines at the bottom, save the file and move onto the next instructions below.","title":"Your Sequencing Data"},{"location":"adding-seq-data/#step_by_step_instructions","text":"Add your .tsv file into the pages folder of your Andersenlab github directory. Open terminal and use the cd command to change directories into the pages folder in your Andersenlab github directory. If you did everything correctly, when you type ls into your terminal, it should look something like this. Then run the following commands in your terminal (while still in your pages directory): $ cp yourFileName.tsv copy.tsv $ python addDataTogt_hmm.tsv.py After running the above commands, your sequencing data has now been added to the existing NILs dataset on Andersenlab.org. You can now remove your .tsv from the pages directory by using the rm command in your terminal. Finally, commit your changes and push your code to update the Andersenlab github.","title":"Step By Step Instructions"},{"location":"backup/","text":"Configuring AWS Client \u00b6 The aws commandline utility is installed as a part of the andersen-lab-env You must provide the appropriate s3 credentials in order to backup BAMs. Erik knows where these are. aws configure Backing up BAMS to Amazon S3 \u00b6 BAMS are uploaded to S3. This allows us to back them up there and access them through CeNDR. Currently, the only BAMs that we upload to s3 are wild isolate BAMs. Once you've added new sequence data you can sync the new BAMs by running the following: # First, cd to the location of isotype bams cd /projects/b1059/data/alignments/WI/isotype # Then run the following command aws s3 sync . s3://elegansvariation.org/bam Local Backup \u00b6 In addition to backing up BAMs on S3, we also backup FASTQs locally. There are four hard drives: PortusTotus - FASTQ Backup 1 Hawkeye - FASTQ Backup 2 Armadillo - open Rhino - open Each one is labeled with what is backed up on it. After you have added new sequenced data you should sync FROM the data/fastq folder on b1059 TO the the PortusTotus (FASTQ Backup 1) and Hawkeye (FASTQ Backup 2) hard drives.","title":"Backup"},{"location":"backup/#configuring_aws_client","text":"The aws commandline utility is installed as a part of the andersen-lab-env You must provide the appropriate s3 credentials in order to backup BAMs. Erik knows where these are. aws configure","title":"Configuring AWS Client"},{"location":"backup/#backing_up_bams_to_amazon_s3","text":"BAMS are uploaded to S3. This allows us to back them up there and access them through CeNDR. Currently, the only BAMs that we upload to s3 are wild isolate BAMs. Once you've added new sequence data you can sync the new BAMs by running the following: # First, cd to the location of isotype bams cd /projects/b1059/data/alignments/WI/isotype # Then run the following command aws s3 sync . s3://elegansvariation.org/bam","title":"Backing up BAMS to Amazon S3"},{"location":"backup/#local_backup","text":"In addition to backing up BAMs on S3, we also backup FASTQs locally. There are four hard drives: PortusTotus - FASTQ Backup 1 Hawkeye - FASTQ Backup 2 Armadillo - open Rhino - open Each one is labeled with what is backed up on it. After you have added new sequenced data you should sync FROM the data/fastq folder on b1059 TO the the PortusTotus (FASTQ Backup 1) and Hawkeye (FASTQ Backup 2) hard drives.","title":"Local Backup"},{"location":"bash/","text":"Bash \u00b6 Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with the fo Introduction to bash . Basic Commands \u00b6 You should familiarize yourself with the following commands. alias - create a shortcut for a command awk - file manipulation; Filtering; Rearranging columns cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sed - quick find/replace sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted. More Advanced \u00b6 You should learn these once you have the basics down. git Good Guides \u00b6 Below I link to some good guides for various bash utilities. awk \u00b6 awk guide awk by example - hundreds of examples Rearranging columns \u00b6 cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column. Filtering based on criteria \u00b6 Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }' bcftools \u00b6 bcftools manual Screen \u00b6 Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Bash"},{"location":"bash/#bash","text":"Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with the fo Introduction to bash .","title":"Bash"},{"location":"bash/#basic_commands","text":"You should familiarize yourself with the following commands. alias - create a shortcut for a command awk - file manipulation; Filtering; Rearranging columns cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sed - quick find/replace sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted.","title":"Basic Commands"},{"location":"bash/#more_advanced","text":"You should learn these once you have the basics down. git","title":"More Advanced"},{"location":"bash/#good_guides","text":"Below I link to some good guides for various bash utilities.","title":"Good Guides"},{"location":"bash/#awk","text":"awk guide awk by example - hundreds of examples","title":"awk"},{"location":"bash/#rearranging_columns","text":"cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column.","title":"Rearranging columns"},{"location":"bash/#filtering_based_on_criteria","text":"Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }'","title":"Filtering based on criteria"},{"location":"bash/#bcftools","text":"bcftools manual","title":"bcftools"},{"location":"bash/#screen","text":"Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Screen"},{"location":"cendr/","text":"CeNDR \u00b6 CeNDR Setting up CeNDR Clone the repo Setup a python environment Download and install the gcloud-sdk Create a cendr gcloud configuration Install direnv Test flask Setup Credentials Load the database Test the site Creating a new release Uploading BAMs Uploading Release Data Bump the CeNDR version number and change-log Adding the release to the CeNDR website Adding isotype images Update the current variant datasets. Updating Publications Setting up CeNDR \u00b6 Clone the repo \u00b6 Clone the repo first. git clone http://www.github.com/andersenlab/cendr Switch to the development branch git checkout --track origin/development Setup a python environment \u00b6 Use miniconda as it will make your life much easier. The conda environment has been specified in the env.yaml file, and can be installed using: conda env create -f env.yaml Download and install the gcloud-sdk \u00b6 Install the gcloud-sdk Create a cendr gcloud configuration \u00b6 gcloud config configurations create cendr Install direnv \u00b6 direnv allows you to load a configuration file when you enter the development directory. Please read about how it works. CeNDR uses a .envrc file within the repo to set up the appropriate environmental variables. Once direnv is installed you can run direnv allow within the CeNDR repo: direnv allow Test flask \u00b6 With direnv enabled, you are nearly able to run the site locally. Run flask , and you should see the following: > flask Usage: flask [OPTIONS] COMMAND [ARGS]... A general utility script for Flask applications. Provides commands from Flask, extensions, and the application. Loads the application defined in the FLASK_APP environment variable, or from a wsgi.py file. Setting the FLASK_ENV environment variable to 'development' will enable debug mode. $ export FLASK_APP=hello.py $ export FLASK_ENV=development $ flask run Options: --version Show the flask version --help Show this message and exit. Commands: decrypt_credentials Decrypt credentials download_db Download the database (used in docker... initdb Initialize the database routes Show the routes for the app. run Run a development server. shell Runs a shell in the app context. update_credentials Update credentials update_strains Updates the strain table of the database If you do not see the full set of commands there - something is broken. Setup Credentials \u00b6 Authenticate with gcloud. Run the following command: mkdir -p env_config flask decrypt_credentials This will create a directory with the site credentials ( env_config ). Keep these secret. Important DO NOT COMMIT THESE CREDENTIALS TO GITHUB !!! Load the database \u00b6 The site uses an SQLite database that can be setup by running: flask download_db This will update the SQLite database used by CeNDR ( base/cendr.db ). The tables are: homologs - A table of homologs+orthologs. strain - Strain info pulled from the google spreadsheet C. elegans WI Strain Info . wormbase_gene - Summarizes gene information; Broken into component parts (e.g. exons, introns etc.). wormbase_gene_summary - Summarizes gene information. One line per gene. metadata - tracks how data was obtained. When. Where. etc. Test the site \u00b6 You can at this point test the site locally by running: flask run Be sure you have direnv. Otherwise you should source the .envrc file prior to running: source .envrc flask run Creating a new release \u00b6 Before a new release is possible, you must have first completed the following tasks: See Add new sequence data for further details . Add new wild isolate sequence data, and process with the trimmomatic-nf pipeline. Identified new isotypes using the concordance-nf Updated the C. elegans WI Strain Info spreadsheet, adding in new isotypes. Update the release column to reflect the release data in the C. elegans WI Strain Info spreadsheet Run and process sequence data with the wi-nf pipeline. Pushing a new release requires a series of steps described below. Uploading BAMs \u00b6 You will need AWS credentials to upload BAMs to Amazon S3. These are available in the secret credentials location. pip install aws-shell aws configure # Use s3 user credentials Once configured, navigate to the BAM location on b1059. cd /projects/b1059/data/alignments/WI/isotype # CD to bams folder... aws s3 sync . s3://elegansvariation.org/bam Run this command in screen to ensure that it completes (it's going to take a while) Uploading Release Data \u00b6 When you run the wi-nf pipeline it will create a folder with the format WI-YYYYMMDD . These data are output in a format that CeNDR can read as a release. You must upload the WI-YYYYMMDD folder to google storage with a command that looks like this: # First cd to the path of the results folder (WI-YYYYMMDD) from the `wi-nf` pipeline. gsutil rsync . gs://elegansvariation.org/releases/YYYYMMDD/ Important Use rsync to copy the files up to google storage. Note that the WI- prefix has been dropped from the YYYYMMDD declaration. Bump the CeNDR version number and change-log \u00b6 Because we are creating a new data release, we need to \"bump\" or move up the CeNDR version. The CeNDR version number is specified in a file at the base of the repo: travis.yml . Modify this line: - export VERSION_NUM=1-2-8 And increase the version number by 1 (e.g. 1-2-9). You should also update the change log and/or add a news item. The change-log is a markdown file located at base/static/content/help/Change-Log.md ; News items are located at base/static/news/ . Look at existing content to get an idea of how to add new items. It is fairly straightforward. You should be able to see changes on the test site. Adding the release to the CeNDR website \u00b6 After the site is loaded, the BAMs and release data are up, and the database is updated, you need to modify the file base/constants.py to add the new release. The date must match the date of the release that was uploaded. Add your release with the appropriate date and the annotation database used (e.g. (\"YYYYMMDD\", \"Annotation Database\") ). RELEASES = [(\"20180413\", \"WS263\"), (\"20170531\", \"WS258\"), (\"20160408\", \"WS245\")] Commit your changes to the development branch of CeNDR and push them to github. Once pushed, travis-ci will build the app and deploy it to a test branch. Use the google app engine interface to identify and test the app. If everything looks good open a pull request bringing the changes on the development branch to the master branch. Again, travis-ci will launch the new site. Important You need to shut down development instances and older versions of the site on the google-app engine interface once you are done testing/deploying new instances to prevent us from incurring charges for those running instances. Adding isotype images \u00b6 Isolation photos are initially prepared on dropbox and are located in the folder here: ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans Each file should be named using the isotype name and the strain name strain name in the following format: <isotype>_<strain>.jpg Then you will use imagemagick (a commandline-based utility) to scale the images down to 1000 pixels (width) and generate a 150px thumbnail. for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; Once you have generated the images you can upload them to google storage. They should be uploaded to the following location: gs://elegansvariation.org/photos/isolation You can drag/drop the photos using the web-based browser or use gsutil : # First cd to the appropriate directory # cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation The script for processing files is located in the dropbox folder and is called 'process_images.sh'. It's also here: for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; # Copy using rsync; Skip .DS_Store files. gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation Update the current variant datasets. \u00b6 The current folder located in gs://elegansvariation.org/releases contains the latest variant datasets and is used by WormBase to display natural variation data. Once you've completed a new release, update the files in this folder gs://elegansvariation.org/releases/current folder. Updating Publications \u00b6 The publications page ( /about/publications ) is generated using a google spreadsheet. The spreadsheet can be accessed here . You can request access to edit the spreadsheet by visiting that link. The last row of the spreadsheet contains a function that can fetch publication data from Pubmed using its API. Simply fill in column A with the PMID (Pubmed Identifier), and the publication data will be fetched. Once you have retrieved the latest pubmed data, create a new row and copy/paste the values for any new publications so they are not fetched from the Pubmed API. Alternatively, you can fill in the details for a publication manually. In either case, any details added should be double checked. Changes should be instant, but there may be some dely on the CeNDR website.","title":"CeNDR"},{"location":"cendr/#cendr","text":"CeNDR Setting up CeNDR Clone the repo Setup a python environment Download and install the gcloud-sdk Create a cendr gcloud configuration Install direnv Test flask Setup Credentials Load the database Test the site Creating a new release Uploading BAMs Uploading Release Data Bump the CeNDR version number and change-log Adding the release to the CeNDR website Adding isotype images Update the current variant datasets. Updating Publications","title":"CeNDR"},{"location":"cendr/#setting_up_cendr","text":"","title":"Setting up CeNDR"},{"location":"cendr/#clone_the_repo","text":"Clone the repo first. git clone http://www.github.com/andersenlab/cendr Switch to the development branch git checkout --track origin/development","title":"Clone the repo"},{"location":"cendr/#setup_a_python_environment","text":"Use miniconda as it will make your life much easier. The conda environment has been specified in the env.yaml file, and can be installed using: conda env create -f env.yaml","title":"Setup a python environment"},{"location":"cendr/#download_and_install_the_gcloud-sdk","text":"Install the gcloud-sdk","title":"Download and install the gcloud-sdk"},{"location":"cendr/#create_a_cendr_gcloud_configuration","text":"gcloud config configurations create cendr","title":"Create a cendr gcloud configuration"},{"location":"cendr/#install_direnv","text":"direnv allows you to load a configuration file when you enter the development directory. Please read about how it works. CeNDR uses a .envrc file within the repo to set up the appropriate environmental variables. Once direnv is installed you can run direnv allow within the CeNDR repo: direnv allow","title":"Install direnv"},{"location":"cendr/#test_flask","text":"With direnv enabled, you are nearly able to run the site locally. Run flask , and you should see the following: > flask Usage: flask [OPTIONS] COMMAND [ARGS]... A general utility script for Flask applications. Provides commands from Flask, extensions, and the application. Loads the application defined in the FLASK_APP environment variable, or from a wsgi.py file. Setting the FLASK_ENV environment variable to 'development' will enable debug mode. $ export FLASK_APP=hello.py $ export FLASK_ENV=development $ flask run Options: --version Show the flask version --help Show this message and exit. Commands: decrypt_credentials Decrypt credentials download_db Download the database (used in docker... initdb Initialize the database routes Show the routes for the app. run Run a development server. shell Runs a shell in the app context. update_credentials Update credentials update_strains Updates the strain table of the database If you do not see the full set of commands there - something is broken.","title":"Test flask"},{"location":"cendr/#setup_credentials","text":"Authenticate with gcloud. Run the following command: mkdir -p env_config flask decrypt_credentials This will create a directory with the site credentials ( env_config ). Keep these secret. Important DO NOT COMMIT THESE CREDENTIALS TO GITHUB !!!","title":"Setup Credentials"},{"location":"cendr/#load_the_database","text":"The site uses an SQLite database that can be setup by running: flask download_db This will update the SQLite database used by CeNDR ( base/cendr.db ). The tables are: homologs - A table of homologs+orthologs. strain - Strain info pulled from the google spreadsheet C. elegans WI Strain Info . wormbase_gene - Summarizes gene information; Broken into component parts (e.g. exons, introns etc.). wormbase_gene_summary - Summarizes gene information. One line per gene. metadata - tracks how data was obtained. When. Where. etc.","title":"Load the database"},{"location":"cendr/#test_the_site","text":"You can at this point test the site locally by running: flask run Be sure you have direnv. Otherwise you should source the .envrc file prior to running: source .envrc flask run","title":"Test the site"},{"location":"cendr/#creating_a_new_release","text":"Before a new release is possible, you must have first completed the following tasks: See Add new sequence data for further details . Add new wild isolate sequence data, and process with the trimmomatic-nf pipeline. Identified new isotypes using the concordance-nf Updated the C. elegans WI Strain Info spreadsheet, adding in new isotypes. Update the release column to reflect the release data in the C. elegans WI Strain Info spreadsheet Run and process sequence data with the wi-nf pipeline. Pushing a new release requires a series of steps described below.","title":"Creating a new release"},{"location":"cendr/#uploading_bams","text":"You will need AWS credentials to upload BAMs to Amazon S3. These are available in the secret credentials location. pip install aws-shell aws configure # Use s3 user credentials Once configured, navigate to the BAM location on b1059. cd /projects/b1059/data/alignments/WI/isotype # CD to bams folder... aws s3 sync . s3://elegansvariation.org/bam Run this command in screen to ensure that it completes (it's going to take a while)","title":"Uploading BAMs"},{"location":"cendr/#uploading_release_data","text":"When you run the wi-nf pipeline it will create a folder with the format WI-YYYYMMDD . These data are output in a format that CeNDR can read as a release. You must upload the WI-YYYYMMDD folder to google storage with a command that looks like this: # First cd to the path of the results folder (WI-YYYYMMDD) from the `wi-nf` pipeline. gsutil rsync . gs://elegansvariation.org/releases/YYYYMMDD/ Important Use rsync to copy the files up to google storage. Note that the WI- prefix has been dropped from the YYYYMMDD declaration.","title":"Uploading Release Data"},{"location":"cendr/#bump_the_cendr_version_number_and_change-log","text":"Because we are creating a new data release, we need to \"bump\" or move up the CeNDR version. The CeNDR version number is specified in a file at the base of the repo: travis.yml . Modify this line: - export VERSION_NUM=1-2-8 And increase the version number by 1 (e.g. 1-2-9). You should also update the change log and/or add a news item. The change-log is a markdown file located at base/static/content/help/Change-Log.md ; News items are located at base/static/news/ . Look at existing content to get an idea of how to add new items. It is fairly straightforward. You should be able to see changes on the test site.","title":"Bump the CeNDR version number and change-log"},{"location":"cendr/#adding_the_release_to_the_cendr_website","text":"After the site is loaded, the BAMs and release data are up, and the database is updated, you need to modify the file base/constants.py to add the new release. The date must match the date of the release that was uploaded. Add your release with the appropriate date and the annotation database used (e.g. (\"YYYYMMDD\", \"Annotation Database\") ). RELEASES = [(\"20180413\", \"WS263\"), (\"20170531\", \"WS258\"), (\"20160408\", \"WS245\")] Commit your changes to the development branch of CeNDR and push them to github. Once pushed, travis-ci will build the app and deploy it to a test branch. Use the google app engine interface to identify and test the app. If everything looks good open a pull request bringing the changes on the development branch to the master branch. Again, travis-ci will launch the new site. Important You need to shut down development instances and older versions of the site on the google-app engine interface once you are done testing/deploying new instances to prevent us from incurring charges for those running instances.","title":"Adding the release to the CeNDR website"},{"location":"cendr/#adding_isotype_images","text":"Isolation photos are initially prepared on dropbox and are located in the folder here: ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans Each file should be named using the isotype name and the strain name strain name in the following format: <isotype>_<strain>.jpg Then you will use imagemagick (a commandline-based utility) to scale the images down to 1000 pixels (width) and generate a 150px thumbnail. for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; Once you have generated the images you can upload them to google storage. They should be uploaded to the following location: gs://elegansvariation.org/photos/isolation You can drag/drop the photos using the web-based browser or use gsutil : # First cd to the appropriate directory # cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation The script for processing files is located in the dropbox folder and is called 'process_images.sh'. It's also here: for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; # Copy using rsync; Skip .DS_Store files. gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation","title":"Adding isotype images"},{"location":"cendr/#update_the_current_variant_datasets","text":"The current folder located in gs://elegansvariation.org/releases contains the latest variant datasets and is used by WormBase to display natural variation data. Once you've completed a new release, update the files in this folder gs://elegansvariation.org/releases/current folder.","title":"Update the current variant datasets."},{"location":"cendr/#updating_publications","text":"The publications page ( /about/publications ) is generated using a google spreadsheet. The spreadsheet can be accessed here . You can request access to edit the spreadsheet by visiting that link. The last row of the spreadsheet contains a function that can fetch publication data from Pubmed using its API. Simply fill in column A with the PMID (Pubmed Identifier), and the publication data will be fetched. Once you have retrieved the latest pubmed data, create a new row and copy/paste the values for any new publications so they are not fetched from the Pubmed API. Alternatively, you can fill in the details for a publication manually. In either case, any details added should be double checked. Changes should be instant, but there may be some dely on the CeNDR website.","title":"Updating Publications"},{"location":"cloud/","text":"AndersenLab cloud resources \u00b6 For full documentation visit mkdocs.org . AndersenLab cloud resources Google Domains Google Cloud Google Cloud Storage Buckets elegansvariation.org andersenlab.org Other buckets Secret Bucket cegwas (deprecated) Google datastore App engine Error Reporting BigQuery AWS S3 Fargate Google Domains \u00b6 Any domain names the lab uses should be registered with Google Domains. The two ones currently are: andersenlab.org elegansvariation.org Google domains can be used to forward domain-specific email addresses if necessary. For example, example@andersenlab.org could be created and forwarded to an email address. Google Cloud \u00b6 Google cloud is used for a variety of services that the lab uses. Google Cloud Storage \u00b6 Google cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website. Buckets \u00b6 Files are grouped into 'buckets' on google storage. We use the following buckets: elegansvariation.org \u00b6 This bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories. browser_tracks - for genome-browser tracks that rarely if ever change. db - Storage/access to the SQLite database. photos - sample collection photos. releases - dataset releases. For more detail, see wi-nf . reports - images and data files within reports. static - static assets used by the site. andersenlab.org \u00b6 In some cases the data associated with a publication is too large to put on github. We store those data here, along with a couple other odds and ends. Other buckets \u00b6 Google Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the Secret Bucket \u00b6 There is one other secret bucket. Ask Erik about it. cegwas (deprecated) \u00b6 Currently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database used by CeNDR in the db/ folder. Once the new version of cegwas is developed - this section should be removed, and that bucket should be deleted. Google datastore \u00b6 Google datastore used as the database for CeNDR. It stores information on mappings, traits, and more. App engine \u00b6 App engine is the platform CeNDR runs on. Error Reporting \u00b6 Google cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on CeNDR. A github issue can be created for these errors and they can be addressed. BigQuery \u00b6 We have used bigquery in the past for large query jobs. We are not actively using it as of late. AWS \u00b6 S3 \u00b6 S3 = simple storage service. We use it to store BAMs. The bucket name on AWS is elegasnvariation.org . S3 is used to store bam files. Originally, we stored BAM files of isotypes which represented groups of near-genetically identical strains. BAMs on S3 are stored as follows. /bam/ - Stores isotype-level bams. /bam/reference_strain/ - Stores reference-strain bams corresponding to each isotype. Fargate \u00b6 Amazon Fargate is used to run the mapping pipeline on CeNDR","title":"Cloud"},{"location":"cloud/#andersenlab_cloud_resources","text":"For full documentation visit mkdocs.org . AndersenLab cloud resources Google Domains Google Cloud Google Cloud Storage Buckets elegansvariation.org andersenlab.org Other buckets Secret Bucket cegwas (deprecated) Google datastore App engine Error Reporting BigQuery AWS S3 Fargate","title":"AndersenLab cloud resources"},{"location":"cloud/#google_domains","text":"Any domain names the lab uses should be registered with Google Domains. The two ones currently are: andersenlab.org elegansvariation.org Google domains can be used to forward domain-specific email addresses if necessary. For example, example@andersenlab.org could be created and forwarded to an email address.","title":"Google Domains"},{"location":"cloud/#google_cloud","text":"Google cloud is used for a variety of services that the lab uses.","title":"Google Cloud"},{"location":"cloud/#google_cloud_storage","text":"Google cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website.","title":"Google Cloud Storage"},{"location":"cloud/#buckets","text":"Files are grouped into 'buckets' on google storage. We use the following buckets:","title":"Buckets"},{"location":"cloud/#elegansvariationorg","text":"This bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories. browser_tracks - for genome-browser tracks that rarely if ever change. db - Storage/access to the SQLite database. photos - sample collection photos. releases - dataset releases. For more detail, see wi-nf . reports - images and data files within reports. static - static assets used by the site.","title":"elegansvariation.org"},{"location":"cloud/#andersenlaborg","text":"In some cases the data associated with a publication is too large to put on github. We store those data here, along with a couple other odds and ends.","title":"andersenlab.org"},{"location":"cloud/#other_buckets","text":"Google Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the","title":"Other buckets"},{"location":"cloud/#secret_bucket","text":"There is one other secret bucket. Ask Erik about it.","title":"Secret Bucket"},{"location":"cloud/#cegwas_deprecated","text":"Currently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database used by CeNDR in the db/ folder. Once the new version of cegwas is developed - this section should be removed, and that bucket should be deleted.","title":"cegwas (deprecated)"},{"location":"cloud/#google_datastore","text":"Google datastore used as the database for CeNDR. It stores information on mappings, traits, and more.","title":"Google datastore"},{"location":"cloud/#app_engine","text":"App engine is the platform CeNDR runs on.","title":"App engine"},{"location":"cloud/#error_reporting","text":"Google cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on CeNDR. A github issue can be created for these errors and they can be addressed.","title":"Error Reporting"},{"location":"cloud/#bigquery","text":"We have used bigquery in the past for large query jobs. We are not actively using it as of late.","title":"BigQuery"},{"location":"cloud/#aws","text":"","title":"AWS"},{"location":"cloud/#s3","text":"S3 = simple storage service. We use it to store BAMs. The bucket name on AWS is elegasnvariation.org . S3 is used to store bam files. Originally, we stored BAM files of isotypes which represented groups of near-genetically identical strains. BAMs on S3 are stored as follows. /bam/ - Stores isotype-level bams. /bam/reference_strain/ - Stores reference-strain bams corresponding to each isotype.","title":"S3"},{"location":"cloud/#fargate","text":"Amazon Fargate is used to run the mapping pipeline on CeNDR","title":"Fargate"},{"location":"labsite/","text":"Andersenlab.org \u00b6 Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software Getting Started \u00b6 The Andersen Lab website was built using jekyll and runs using the Github Pages service. Software-Dependencies \u00b6 Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml . Cloning the repo \u00b6 To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url. Updating the site \u00b6 In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git. andersenlab.github.io \u00b6 The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with Announcements \u00b6 Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication. General Announcements \u00b6 To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more. Publication Post \u00b6 New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3! Lab members \u00b6 Adding new lab members: \u00b6 (1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page. Set Status to Former \u00b6 Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a> Remove lab members \u00b6 Remove the persons information from _data/people.yaml ; Optionally delete their photo. Funding \u00b6 Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here. Protocols \u00b6 Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines. Research \u00b6 The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000). Publications \u00b6 Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page. Photo Albums \u00b6 Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push Software \u00b6 If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Andersen Labsite"},{"location":"labsite/#andersenlaborg","text":"Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software","title":"Andersenlab.org"},{"location":"labsite/#getting_started","text":"The Andersen Lab website was built using jekyll and runs using the Github Pages service.","title":"Getting Started"},{"location":"labsite/#software-dependencies","text":"Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml .","title":"Software-Dependencies"},{"location":"labsite/#cloning_the_repo","text":"To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url.","title":"Cloning the repo"},{"location":"labsite/#updating_the_site","text":"In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git.","title":"Updating the site"},{"location":"labsite/#andersenlabgithubio","text":"The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with","title":"andersenlab.github.io"},{"location":"labsite/#announcements","text":"Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication.","title":"Announcements"},{"location":"labsite/#general_announcements","text":"To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more.","title":"General Announcements"},{"location":"labsite/#publication_post","text":"New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3!","title":"Publication Post"},{"location":"labsite/#lab_members","text":"","title":"Lab members"},{"location":"labsite/#adding_new_lab_members","text":"(1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page.","title":"Adding new lab members:"},{"location":"labsite/#set_status_to_former","text":"Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a>","title":"Set Status to Former"},{"location":"labsite/#remove_lab_members","text":"Remove the persons information from _data/people.yaml ; Optionally delete their photo.","title":"Remove lab members"},{"location":"labsite/#funding","text":"Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here.","title":"Funding"},{"location":"labsite/#protocols","text":"Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines.","title":"Protocols"},{"location":"labsite/#research","text":"The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000).","title":"Research"},{"location":"labsite/#publications","text":"Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page.","title":"Publications"},{"location":"labsite/#photo_albums","text":"Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push","title":"Photo Albums"},{"location":"labsite/#software","text":"If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Software"},{"location":"pipeline-GCPconfig/","text":"Running Nextflow pipeline on GCP Enable API Create service account Generate credential for the service account Nextflow version and mode Configure Nextflow for GCP Running Nextflow pipeline on GCP \u00b6 Google genomic API allows auto scale for computational resources by creating and closing VMs automatically. We have a dedicated google project caendr which using Google genomic APT for all the nextflow pipelines in our lab. To access it, you should provide your gmail accout to Erik and ask Erik give you a project owner role for caendr . I already preset the project to enable running Nextflow pipelines using Google genomic API . See below for more details. Enable API \u00b6 Go the the main page of google cloud platform . In the Produck & Services menu, click APIs & Services , and then click Enable APIs and Services . The following APIs should be enabled to run nextflow pipeline on GCP. Genomics API Cloud Life Sciences API Compute Engine API Google Container Registry API Create service account \u00b6 Go to the IAM & admin , find the Service accounts . Click CREATE SERVICE ACCOUNT to create a new service accounts. Note this service accounts must have a project owner role to run Nextflow pipelines. The service account I created here is called nextflowRUN . You don't need to do the above processes when you use GCP. But you have to do all the following processes to make sure you have the right permissions to caendr . Generate credential for the service account \u00b6 After you get the access to caendr , go to the API & Services . Click the Create credentials button, select Service account key . And choose nextflowRUN to generate a JSON file, which is a privite key file for using nextflowRUN . Download the file and save it in a safe place. Finally, define the GOOGLE_APPLICATION_CREDENTIALS variable in .bash_profile with the directory of the JSON file. Which should looks like the example. export GOOGLE_APPLICATION_CREDENTIALS=$HOME/google_creds/caendr-2cae6210c8d1.json Nextflow version and mode \u00b6 Only the version of 19.07.0 or higher of Nextflow are compatible with GCP. And also, the Nextflow should have a google mode. You can define the version and mode in .bash_profile . export NXF_VER=19.07.0 export NXF_MODE=google Then, run the following code to update or install Nextflow. curl https://get.nextflow.io | bash Configure Nextflow for GCP \u00b6 To run Nextflow pipelines on GCP, you need to build docker images for them. Check the docker file repo of our lab for more information. The google genomic API has its own executor called google-pipelines , you need to define the executor variable with google-pipelines in the nextflow.config file. Here is the example for concordance-nf . docker { enabled = true } process { executor = 'google-pipelines' withLabel: bam_coverage { container = 'faithman/bam_toolbox:latest' } container = 'faithman/concordance:latest' machineType = 'n1-standard-4' } google { project = 'caendr' zone = 'us-central1-a' } cloud { preemptible = true } executor { queueSize = 500 } Important The file system of google buckets is not like S3 that can read/write directly by most softwares. You have to use gsutil tool to interact with google buckets to read/write files in most situations. Nextflow has built-in functions to interact with google buckets, but you still can not read/write files directly in your script. All the files have to be read and write via channels in Nextflow!","title":"GCP configuration"},{"location":"pipeline-GCPconfig/#running_nextflow_pipeline_on_gcp","text":"Google genomic API allows auto scale for computational resources by creating and closing VMs automatically. We have a dedicated google project caendr which using Google genomic APT for all the nextflow pipelines in our lab. To access it, you should provide your gmail accout to Erik and ask Erik give you a project owner role for caendr . I already preset the project to enable running Nextflow pipelines using Google genomic API . See below for more details.","title":"Running Nextflow pipeline on GCP"},{"location":"pipeline-GCPconfig/#enable_api","text":"Go the the main page of google cloud platform . In the Produck & Services menu, click APIs & Services , and then click Enable APIs and Services . The following APIs should be enabled to run nextflow pipeline on GCP. Genomics API Cloud Life Sciences API Compute Engine API Google Container Registry API","title":"Enable API"},{"location":"pipeline-GCPconfig/#create_service_account","text":"Go to the IAM & admin , find the Service accounts . Click CREATE SERVICE ACCOUNT to create a new service accounts. Note this service accounts must have a project owner role to run Nextflow pipelines. The service account I created here is called nextflowRUN . You don't need to do the above processes when you use GCP. But you have to do all the following processes to make sure you have the right permissions to caendr .","title":"Create service account"},{"location":"pipeline-GCPconfig/#generate_credential_for_the_service_account","text":"After you get the access to caendr , go to the API & Services . Click the Create credentials button, select Service account key . And choose nextflowRUN to generate a JSON file, which is a privite key file for using nextflowRUN . Download the file and save it in a safe place. Finally, define the GOOGLE_APPLICATION_CREDENTIALS variable in .bash_profile with the directory of the JSON file. Which should looks like the example. export GOOGLE_APPLICATION_CREDENTIALS=$HOME/google_creds/caendr-2cae6210c8d1.json","title":"Generate credential for the service account"},{"location":"pipeline-GCPconfig/#nextflow_version_and_mode","text":"Only the version of 19.07.0 or higher of Nextflow are compatible with GCP. And also, the Nextflow should have a google mode. You can define the version and mode in .bash_profile . export NXF_VER=19.07.0 export NXF_MODE=google Then, run the following code to update or install Nextflow. curl https://get.nextflow.io | bash","title":"Nextflow version and mode"},{"location":"pipeline-GCPconfig/#configure_nextflow_for_gcp","text":"To run Nextflow pipelines on GCP, you need to build docker images for them. Check the docker file repo of our lab for more information. The google genomic API has its own executor called google-pipelines , you need to define the executor variable with google-pipelines in the nextflow.config file. Here is the example for concordance-nf . docker { enabled = true } process { executor = 'google-pipelines' withLabel: bam_coverage { container = 'faithman/bam_toolbox:latest' } container = 'faithman/concordance:latest' machineType = 'n1-standard-4' } google { project = 'caendr' zone = 'us-central1-a' } cloud { preemptible = true } executor { queueSize = 500 } Important The file system of google buckets is not like S3 that can read/write directly by most softwares. You have to use gsutil tool to interact with google buckets to read/write files in most situations. Nextflow has built-in functions to interact with google buckets, but you still can not read/write files directly in your script. All the files have to be read and write via channels in Nextflow!","title":"Configure Nextflow for GCP"},{"location":"pipeline-alignment/","text":"alignment-nf \u00b6 The alignment-nf pipeline performs alignment for wild isolate sequence data at the strain level , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling, concordance analysis , wi-nf (variant calling) and other analyses. This page details how to run the pipeline and how to add new wild isolate sequencing data. Note Historically, sequence processing was performed at the isotype level. We are still interested in filtering strains used in analysis at the isotype level, but alignment and variant calling are now performed at the strain level rather than at the isotype level. alignment-nf Usage Pipeline Overview Usage Quick Start Sample Sheet construct_sample_sheet.sh Adding new sequencing datasets Profiles and Running the Pipeline Software Parameters --debug --sample_sheet --fq_prefix --kmers --reference --output --email Output Strain Usage \u00b6 \u2597\u2596 \u259d\u259c \u259d \u2597 \u2597\u2596 \u2596\u2597\u2584\u2584\u2596 \u2590\u258c \u2590 \u2597\u2584 \u2584\u2584 \u2597\u2597\u2596 \u2597\u2584\u2584 \u2584\u2596 \u2597\u2597\u2596 \u2597\u259f\u2584 \u2590\u259a \u258c\u2590 \u258c\u2590 \u2590 \u2590 \u2590\u2598\u259c \u2590\u2598\u2590 \u2590\u2590\u2590 \u2590\u2598\u2590 \u2590\u2598\u2590 \u2590 \u2590\u2590\u2596\u258c\u2590\u2584\u2584\u2596 \u2599\u259f \u2590 \u2590 \u2590 \u2590 \u2590 \u2590 \u2590\u2590\u2590 \u2590\u2580\u2580 \u2590 \u2590 \u2590 \u2580\u2598 \u2590 \u258c\u258c\u2590 \u2590 \u258c \u259d\u2584 \u2597\u259f\u2584 \u259d\u2599\u259c \u2590 \u2590 \u2590\u2590\u2590 \u259d\u2599\u259e \u2590 \u2590 \u259d\u2584 \u2590 \u2590\u258c\u2590 \u2596\u2590 \u259d\u2598 parameters description Set/Default ========== =========== ======================== --debug Set to 'true' to test ${params.debug} --sample_sheet sample_sheet ${params.sample_sheet} --fq_prefix fastq prefix ${params.fq_prefix} --kmers count kmers ${params.kmers} --reference Reference Genome (w/ .gz) ${params.reference} --output Location for output ${params.output} --email Email to be sent results ${params.email} HELP: http://andersenlab.org/dry-guide/pipeline-alignment/ The logo above looks better in your terminal! Pipeline Overview \u00b6 Usage \u00b6 Quick Start \u00b6 Testing locally This command uses a test dataset NXF_VER=19.12.0-edge nextflow run main.nf --debug -profile local Testing on Quest This command uses a test dataset NXF_VER=19.12.0-edge nextflow run main.nf --debug -profile quest Running on Quest You should run this in a screen session. NXF_VER=19.12.0-edge nextflow run main.nf -profile quest -resume Sample Sheet \u00b6 The sample sheet for wild isolate data is located in the base of the alignment-nf repo and is called WI_sample_sheet.tsv . This file is generated by the scripts/construct_sample_sheet.sh script. Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . The sample sheet has the following columns: strain - the name of the strain. Multiple sequencing runs of the same strain are merged together. reference_strain - reference strain isotype - strain isotype id - A unique ID for each sequencing run. This must be unique for every single pair of FASTQs. library - A library ID. This should uniquely identify a DNA sequencing library. fq1 - The path to FASTQ1 fq2 - The path to FASTQ2 strain reference_strain isotype id library fq1 fq2 seq_folder AB1 FALSE ISO1 BGI1-RET2-AB1 RET2 BGI1-RET2-AB1-trim-1P.fq.gz BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set ECA243 TRUE ISO1 BGI3-RET3b-ECA243 RET3b BGI3-RET3b-ECA243-trim-1P.fq.gz BGI3-RET3b-ECA243-trim-2P.fq.gz original_wi_set ECA718 TRUE ISO2 ECA718_RET-S16_S26_L001 RET-S16 ECA718_RET-S16_S26_L001_1P.fq.gz ECA718_RET-S16_S26_L001_2P.fq.gz 20180306_Duke_NovaSeq_6000 The library column is a useful tool for identifying errors by variant callers. For example, if the same library is sequenced twice, and a variant is only observed in one sequencing run then that variant may be excluded as a technical / PCR artifact depending on the variant caller being used. Important The alignment pipeline will merge multiple sequencing runs of the same strain into a single bam. However, summary output is provided at both the strain and id level. In this way, if there is a poor sequencing run it can be identified and removed from a collection of sequencing runs belonging to a strain. Note The sample sheet is a critical tool. It allows us to associated metadata with each sequencing run (e.g. isotype, reference strain, id, library). It also allows us to quickly verify that all results have been output. It is much easier than working with a list of files! construct_sample_sheet.sh \u00b6 The scripts/construct_sample_sheet.sh script generates the WI_sample_sheet.tsv file. Warning The WI_sample_sheet.tsv file should never be generated and/or edited by hand. It should only be generated using the scripts/construct_sample_sheet.tsv script. The construct_sample_sheet.sh script does a few things. (1) Parses FASTQ Filenames Unfortunately, no two sequencing centers are alike and they use different formats for naming sequencing files. For example: ECA768_RET-S11_S79_L001_2P.fq.gz [strain]_[lib_lib#]_[sample_#]_[lane]_[read].fq.gz XZ1734_S573_L007_2P.fq.gz [strain]_[sample_#]_[lane]_[read].fq.gz In some cases they even changed formats over time! The script parses the FASTQ filenames from different sequencing centers, extracting the strain name, and a unique ID. Note that the library and unique sequencing run ID ( id ) are named somewhat arbitrarily. The most imporant aspect of these columns is that any DNA library that has been sequenced multiple times possess the same library , and that every pair of FASTQs possess a unique sequencing ID. Consider the following (fake) example: strain isotype reference_strain id library AB1 AB1 TRUE BGI2-RET2-AB1 RET2 AB1 AB1 TRUE BGI2-RET3-AB1 RET3 AB4 CB4858 FALSE BGI1-RET2-AB4 RET2 AB4 CB4858 FALSE BGI2-RET2-AB4 RET2 AB1 was sequenced twice, however two different DNA libraries were produced for each sequencing run ( RET2 and RET3 ). AB4 was also sequenced twice, but both sequencing runs were of the same DNA library (called RET2 ). Note that the id column is always unique across all sequencing runs. If you look at the WI_sample_sheet.tsv in more detail you will observe that the id and library columns are not consistantly named. This is not ideal, but it works. The inconsistancy does not affect analysis, and exists because the filenames are not consistant, but unique library and sequencing run IDs must be derived from them. (2) Clean up strain names The second thing the construct_sample_sheet.sh script does is that it replaces shorthand strain names or innapropriately named strains with the 3-letter system. For example, N2Baer is renamed to ECA254 . (3) Integrate metadata The C. elegans WI Strain Info google spreadsheet is a master spreadlist containing every strain, reference_strain, and isotype for C. elegans wild isolates. The script downloads this dataset and uses it to integrate the isotype and reference strain into the sample sheet. Adding new sequencing datasets \u00b6 Sequencing data should be added to QUEST and processed through the trimming pipeline before being added to WI_sample_sheet.tsv . Before proceeding, be sure to read pipeline-trimming To add new sequencing datasets you will need to devise a strategy for extracting the strain name, a unique ID, and sequencing library from the FASTQ filenames. This may be the same as a past dataset, in which case you can append the sequencing run folder name to the list with that format. Alternatively, you may need to create a custom set of bash commands for generating the rows corresponding to each FASTQ pair. Here is an example from the construct_sample_sheet.sh script. #===================================# # BGI-20161012-ECA23 # #===================================# out=`mktemp` seq_folder=BGI-20161012-ECA23 >&2 echo ${seq_folder} prefix=${fastq_dir}/WI/dna/processed/$seq_folder for i in `ls -1 $prefix/*1P.fq.gz`; do bname=`basename ${i}`; barcode=`zcat ${i} | grep '@' | cut -f 10 -d ':' | sed 's/_//g' | head -n 100 | uniq -c | sort -k 1,1n | cut -c 9-100 | tail -n 1` echo -e \"${bname}\\t${i}\\t${barcode}\" >> ${out} done; cat ${out} |\\ awk -v prefix=${prefix} -v seq_folder=${seq_folder} '{ fq1 = $1; fq2 = $1; LB = $3; gsub(\"N\", \"\", LB); gsub(\"1P.fq.gz\", \"2P.fq.gz\", fq2); ID = $1; gsub(\"_1P.fq.gz\", \"\", ID); split(ID, a, \"[-_]\") SM=a[2]; print SM \"\\t\" ID \"\\t\" LB \"\\t\" prefix \"/\" fq1 \"\\t\" prefix \"/\" fq2 \"\\t\" seq_folder; }' >> ${fq_sheet} Notes on this snippet: SM = strain , LB = library , and ID = id in the final output file. The sequencing run is listed in the comment box at the top. Barcodes are extracted from each FASTQ in the first forloop. These are used to define the library . The id is defined using the basename of the file. A final column corresponding to the seq_folder is always added. Profiles and Running the Pipeline \u00b6 There are three configuration profiles for this pipeline. local - Used for local development. quest - Used for running on Quest. gcp - For running on Google Cloud. Software \u00b6 Almost all processes within the pipeline are now managed using conda . To use the pipeline, you must have conda installed and available. Nextflow will take care of installing conda environments and managing software. Note mosdepth is used to calculate coverage. mosdepth is available on Linux machines, but not on Mac OSX. That is why the conda environment for the coverage process is specified as conda { System.properties['os.name'] != \"Mac OS X\" ? 'bioconda::mosdepth=0.2.6' : \"\" } . This snippet allows mosdepth to run off the executable present in the bin folder locally on Mac OSX, or use the conda-based installation when on Linux. Parameters \u00b6 --debug \u00b6 You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder) using your specified configuration profile (e.g. local / quest / gcp). For example: nextflow run main.nf -profile local --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv --sample_sheet \u00b6 A custom sample sheet can be specified using --sample_sheet . The sample sheet format is described above in sample sheet When using --debug true , the test_data/sample_sheet.tsv file is used. Note Previously this option was specified using --fqs . --fq_prefix \u00b6 Within a sample sheet you may specify the locations of FASTQs using an absolute directory or a relative directory. If you want to use a relative directory, you should use the --fq_prefix to set the path that should be prefixed to each FASTQ. Note Previously, this option was --fqs_file_prefix --kmers \u00b6 default = true Toggles kmer-analysis --reference \u00b6 A fasta reference indexed with BWA. WS245 is packaged with the pipeline for convenience when testing or running locally. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz --output \u00b6 Default - Alignment-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be alignment-YYYYMMDD-debug . --email \u00b6 Setting --email will trigger an email report following pipeline execution. Output \u00b6 Strain \u00b6 \u251c\u2500\u2500 _aggregate \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 multiqc \u2502 \u251c\u2500\u2500 id_data/ \u2502 \u2502 \u251c\u2500\u2500 ... (same as strain_data/) \u2502 \u251c\u2500\u2500 id_multiqc_report.html \u2502 \u251c\u2500\u2500 strain_data/ \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-dist-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-per-contig_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-plot-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_picard_deduplication_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Normalised_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools_alignment_plot_1.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc.log \u2502 \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u2502 \u251c\u2500\u2500 multiqc_general_stats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_picard_dups.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_qualimap_bamqc_genome_results.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_flagstat.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_sources.txt \u2502 \u2514\u2500\u2500 strain_multiqc_report.html \u251c\u2500\u2500 bam \u2502 \u251c\u2500\u2500 [strain].bam \u2502 \u2514\u2500\u2500 [strain].bam.bai \u251c\u2500\u2500 coverage \u2502 \u251c\u2500\u2500 id \u2502 \u2502 \u251c\u2500\u2500 [id].mosdepth.global.dist.txt \u2502 \u2502 \u251c\u2500\u2500 [id].mosdepth.summary.txt \u2502 \u2502 \u251c\u2500\u2500 [id].per-base.bed.gz \u2502 \u2502 \u2514\u2500\u2500 [id].per-base.bed.gz.csi \u2502 \u2514\u2500\u2500 strain \u2502 \u251c\u2500\u2500 [strain].mosdepth.global.dist.txt \u2502 \u251c\u2500\u2500 [strain].mosdepth.summary.txt \u2502 \u251c\u2500\u2500 [strain].per-base.bed.gz \u2502 \u2514\u2500\u2500 [strain].per-base.bed.gz.csi \u251c\u2500\u2500 software_versions.txt \u2514\u2500\u2500 summary.txt Most files should be obvious. A few are detailed below. software_versions.txt - Outputs the software versions used for every process (step) of the pipeline. summary.txt - Outputs a summary of the parameters used. sample_sheet.tsv - The sample sheet that was used to produce the alignment directory. strain_sheet.tsv - A summary of all strains and bams in the alignment directory. aggregate - Stores data that has been aggregated across all strains or sequencing IDs. coverage - Contains coverage data at the strain or id level, presented in a variety of ways.","title":"WI-Alignment"},{"location":"pipeline-alignment/#alignment-nf","text":"The alignment-nf pipeline performs alignment for wild isolate sequence data at the strain level , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling, concordance analysis , wi-nf (variant calling) and other analyses. This page details how to run the pipeline and how to add new wild isolate sequencing data. Note Historically, sequence processing was performed at the isotype level. We are still interested in filtering strains used in analysis at the isotype level, but alignment and variant calling are now performed at the strain level rather than at the isotype level. alignment-nf Usage Pipeline Overview Usage Quick Start Sample Sheet construct_sample_sheet.sh Adding new sequencing datasets Profiles and Running the Pipeline Software Parameters --debug --sample_sheet --fq_prefix --kmers --reference --output --email Output Strain","title":"alignment-nf"},{"location":"pipeline-alignment/#usage","text":"\u2597\u2596 \u259d\u259c \u259d \u2597 \u2597\u2596 \u2596\u2597\u2584\u2584\u2596 \u2590\u258c \u2590 \u2597\u2584 \u2584\u2584 \u2597\u2597\u2596 \u2597\u2584\u2584 \u2584\u2596 \u2597\u2597\u2596 \u2597\u259f\u2584 \u2590\u259a \u258c\u2590 \u258c\u2590 \u2590 \u2590 \u2590\u2598\u259c \u2590\u2598\u2590 \u2590\u2590\u2590 \u2590\u2598\u2590 \u2590\u2598\u2590 \u2590 \u2590\u2590\u2596\u258c\u2590\u2584\u2584\u2596 \u2599\u259f \u2590 \u2590 \u2590 \u2590 \u2590 \u2590 \u2590\u2590\u2590 \u2590\u2580\u2580 \u2590 \u2590 \u2590 \u2580\u2598 \u2590 \u258c\u258c\u2590 \u2590 \u258c \u259d\u2584 \u2597\u259f\u2584 \u259d\u2599\u259c \u2590 \u2590 \u2590\u2590\u2590 \u259d\u2599\u259e \u2590 \u2590 \u259d\u2584 \u2590 \u2590\u258c\u2590 \u2596\u2590 \u259d\u2598 parameters description Set/Default ========== =========== ======================== --debug Set to 'true' to test ${params.debug} --sample_sheet sample_sheet ${params.sample_sheet} --fq_prefix fastq prefix ${params.fq_prefix} --kmers count kmers ${params.kmers} --reference Reference Genome (w/ .gz) ${params.reference} --output Location for output ${params.output} --email Email to be sent results ${params.email} HELP: http://andersenlab.org/dry-guide/pipeline-alignment/ The logo above looks better in your terminal!","title":"Usage"},{"location":"pipeline-alignment/#pipeline_overview","text":"","title":"Pipeline Overview"},{"location":"pipeline-alignment/#usage_1","text":"","title":"Usage"},{"location":"pipeline-alignment/#quick_start","text":"Testing locally This command uses a test dataset NXF_VER=19.12.0-edge nextflow run main.nf --debug -profile local Testing on Quest This command uses a test dataset NXF_VER=19.12.0-edge nextflow run main.nf --debug -profile quest Running on Quest You should run this in a screen session. NXF_VER=19.12.0-edge nextflow run main.nf -profile quest -resume","title":"Quick Start"},{"location":"pipeline-alignment/#sample_sheet","text":"The sample sheet for wild isolate data is located in the base of the alignment-nf repo and is called WI_sample_sheet.tsv . This file is generated by the scripts/construct_sample_sheet.sh script. Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . The sample sheet has the following columns: strain - the name of the strain. Multiple sequencing runs of the same strain are merged together. reference_strain - reference strain isotype - strain isotype id - A unique ID for each sequencing run. This must be unique for every single pair of FASTQs. library - A library ID. This should uniquely identify a DNA sequencing library. fq1 - The path to FASTQ1 fq2 - The path to FASTQ2 strain reference_strain isotype id library fq1 fq2 seq_folder AB1 FALSE ISO1 BGI1-RET2-AB1 RET2 BGI1-RET2-AB1-trim-1P.fq.gz BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set ECA243 TRUE ISO1 BGI3-RET3b-ECA243 RET3b BGI3-RET3b-ECA243-trim-1P.fq.gz BGI3-RET3b-ECA243-trim-2P.fq.gz original_wi_set ECA718 TRUE ISO2 ECA718_RET-S16_S26_L001 RET-S16 ECA718_RET-S16_S26_L001_1P.fq.gz ECA718_RET-S16_S26_L001_2P.fq.gz 20180306_Duke_NovaSeq_6000 The library column is a useful tool for identifying errors by variant callers. For example, if the same library is sequenced twice, and a variant is only observed in one sequencing run then that variant may be excluded as a technical / PCR artifact depending on the variant caller being used. Important The alignment pipeline will merge multiple sequencing runs of the same strain into a single bam. However, summary output is provided at both the strain and id level. In this way, if there is a poor sequencing run it can be identified and removed from a collection of sequencing runs belonging to a strain. Note The sample sheet is a critical tool. It allows us to associated metadata with each sequencing run (e.g. isotype, reference strain, id, library). It also allows us to quickly verify that all results have been output. It is much easier than working with a list of files!","title":"Sample Sheet"},{"location":"pipeline-alignment/#construct_sample_sheetsh","text":"The scripts/construct_sample_sheet.sh script generates the WI_sample_sheet.tsv file. Warning The WI_sample_sheet.tsv file should never be generated and/or edited by hand. It should only be generated using the scripts/construct_sample_sheet.tsv script. The construct_sample_sheet.sh script does a few things. (1) Parses FASTQ Filenames Unfortunately, no two sequencing centers are alike and they use different formats for naming sequencing files. For example: ECA768_RET-S11_S79_L001_2P.fq.gz [strain]_[lib_lib#]_[sample_#]_[lane]_[read].fq.gz XZ1734_S573_L007_2P.fq.gz [strain]_[sample_#]_[lane]_[read].fq.gz In some cases they even changed formats over time! The script parses the FASTQ filenames from different sequencing centers, extracting the strain name, and a unique ID. Note that the library and unique sequencing run ID ( id ) are named somewhat arbitrarily. The most imporant aspect of these columns is that any DNA library that has been sequenced multiple times possess the same library , and that every pair of FASTQs possess a unique sequencing ID. Consider the following (fake) example: strain isotype reference_strain id library AB1 AB1 TRUE BGI2-RET2-AB1 RET2 AB1 AB1 TRUE BGI2-RET3-AB1 RET3 AB4 CB4858 FALSE BGI1-RET2-AB4 RET2 AB4 CB4858 FALSE BGI2-RET2-AB4 RET2 AB1 was sequenced twice, however two different DNA libraries were produced for each sequencing run ( RET2 and RET3 ). AB4 was also sequenced twice, but both sequencing runs were of the same DNA library (called RET2 ). Note that the id column is always unique across all sequencing runs. If you look at the WI_sample_sheet.tsv in more detail you will observe that the id and library columns are not consistantly named. This is not ideal, but it works. The inconsistancy does not affect analysis, and exists because the filenames are not consistant, but unique library and sequencing run IDs must be derived from them. (2) Clean up strain names The second thing the construct_sample_sheet.sh script does is that it replaces shorthand strain names or innapropriately named strains with the 3-letter system. For example, N2Baer is renamed to ECA254 . (3) Integrate metadata The C. elegans WI Strain Info google spreadsheet is a master spreadlist containing every strain, reference_strain, and isotype for C. elegans wild isolates. The script downloads this dataset and uses it to integrate the isotype and reference strain into the sample sheet.","title":"construct_sample_sheet.sh"},{"location":"pipeline-alignment/#adding_new_sequencing_datasets","text":"Sequencing data should be added to QUEST and processed through the trimming pipeline before being added to WI_sample_sheet.tsv . Before proceeding, be sure to read pipeline-trimming To add new sequencing datasets you will need to devise a strategy for extracting the strain name, a unique ID, and sequencing library from the FASTQ filenames. This may be the same as a past dataset, in which case you can append the sequencing run folder name to the list with that format. Alternatively, you may need to create a custom set of bash commands for generating the rows corresponding to each FASTQ pair. Here is an example from the construct_sample_sheet.sh script. #===================================# # BGI-20161012-ECA23 # #===================================# out=`mktemp` seq_folder=BGI-20161012-ECA23 >&2 echo ${seq_folder} prefix=${fastq_dir}/WI/dna/processed/$seq_folder for i in `ls -1 $prefix/*1P.fq.gz`; do bname=`basename ${i}`; barcode=`zcat ${i} | grep '@' | cut -f 10 -d ':' | sed 's/_//g' | head -n 100 | uniq -c | sort -k 1,1n | cut -c 9-100 | tail -n 1` echo -e \"${bname}\\t${i}\\t${barcode}\" >> ${out} done; cat ${out} |\\ awk -v prefix=${prefix} -v seq_folder=${seq_folder} '{ fq1 = $1; fq2 = $1; LB = $3; gsub(\"N\", \"\", LB); gsub(\"1P.fq.gz\", \"2P.fq.gz\", fq2); ID = $1; gsub(\"_1P.fq.gz\", \"\", ID); split(ID, a, \"[-_]\") SM=a[2]; print SM \"\\t\" ID \"\\t\" LB \"\\t\" prefix \"/\" fq1 \"\\t\" prefix \"/\" fq2 \"\\t\" seq_folder; }' >> ${fq_sheet} Notes on this snippet: SM = strain , LB = library , and ID = id in the final output file. The sequencing run is listed in the comment box at the top. Barcodes are extracted from each FASTQ in the first forloop. These are used to define the library . The id is defined using the basename of the file. A final column corresponding to the seq_folder is always added.","title":"Adding new sequencing datasets"},{"location":"pipeline-alignment/#profiles_and_running_the_pipeline","text":"There are three configuration profiles for this pipeline. local - Used for local development. quest - Used for running on Quest. gcp - For running on Google Cloud.","title":"Profiles and Running the Pipeline"},{"location":"pipeline-alignment/#software","text":"Almost all processes within the pipeline are now managed using conda . To use the pipeline, you must have conda installed and available. Nextflow will take care of installing conda environments and managing software. Note mosdepth is used to calculate coverage. mosdepth is available on Linux machines, but not on Mac OSX. That is why the conda environment for the coverage process is specified as conda { System.properties['os.name'] != \"Mac OS X\" ? 'bioconda::mosdepth=0.2.6' : \"\" } . This snippet allows mosdepth to run off the executable present in the bin folder locally on Mac OSX, or use the conda-based installation when on Linux.","title":"Software"},{"location":"pipeline-alignment/#parameters","text":"","title":"Parameters"},{"location":"pipeline-alignment/#--debug","text":"You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder) using your specified configuration profile (e.g. local / quest / gcp). For example: nextflow run main.nf -profile local --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv","title":"--debug"},{"location":"pipeline-alignment/#--sample_sheet","text":"A custom sample sheet can be specified using --sample_sheet . The sample sheet format is described above in sample sheet When using --debug true , the test_data/sample_sheet.tsv file is used. Note Previously this option was specified using --fqs .","title":"--sample_sheet"},{"location":"pipeline-alignment/#--fq_prefix","text":"Within a sample sheet you may specify the locations of FASTQs using an absolute directory or a relative directory. If you want to use a relative directory, you should use the --fq_prefix to set the path that should be prefixed to each FASTQ. Note Previously, this option was --fqs_file_prefix","title":"--fq_prefix"},{"location":"pipeline-alignment/#--kmers","text":"default = true Toggles kmer-analysis","title":"--kmers"},{"location":"pipeline-alignment/#--reference","text":"A fasta reference indexed with BWA. WS245 is packaged with the pipeline for convenience when testing or running locally. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz","title":"--reference"},{"location":"pipeline-alignment/#--output","text":"Default - Alignment-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be alignment-YYYYMMDD-debug .","title":"--output"},{"location":"pipeline-alignment/#--email","text":"Setting --email will trigger an email report following pipeline execution.","title":"--email"},{"location":"pipeline-alignment/#output","text":"","title":"Output"},{"location":"pipeline-alignment/#strain","text":"\u251c\u2500\u2500 _aggregate \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 multiqc \u2502 \u251c\u2500\u2500 id_data/ \u2502 \u2502 \u251c\u2500\u2500 ... (same as strain_data/) \u2502 \u251c\u2500\u2500 id_multiqc_report.html \u2502 \u251c\u2500\u2500 strain_data/ \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-dist-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-per-contig_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-plot-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_picard_deduplication_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Normalised_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools_alignment_plot_1.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc.log \u2502 \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u2502 \u251c\u2500\u2500 multiqc_general_stats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_picard_dups.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_qualimap_bamqc_genome_results.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_flagstat.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_sources.txt \u2502 \u2514\u2500\u2500 strain_multiqc_report.html \u251c\u2500\u2500 bam \u2502 \u251c\u2500\u2500 [strain].bam \u2502 \u2514\u2500\u2500 [strain].bam.bai \u251c\u2500\u2500 coverage \u2502 \u251c\u2500\u2500 id \u2502 \u2502 \u251c\u2500\u2500 [id].mosdepth.global.dist.txt \u2502 \u2502 \u251c\u2500\u2500 [id].mosdepth.summary.txt \u2502 \u2502 \u251c\u2500\u2500 [id].per-base.bed.gz \u2502 \u2502 \u2514\u2500\u2500 [id].per-base.bed.gz.csi \u2502 \u2514\u2500\u2500 strain \u2502 \u251c\u2500\u2500 [strain].mosdepth.global.dist.txt \u2502 \u251c\u2500\u2500 [strain].mosdepth.summary.txt \u2502 \u251c\u2500\u2500 [strain].per-base.bed.gz \u2502 \u2514\u2500\u2500 [strain].per-base.bed.gz.csi \u251c\u2500\u2500 software_versions.txt \u2514\u2500\u2500 summary.txt Most files should be obvious. A few are detailed below. software_versions.txt - Outputs the software versions used for every process (step) of the pipeline. summary.txt - Outputs a summary of the parameters used. sample_sheet.tsv - The sample sheet that was used to produce the alignment directory. strain_sheet.tsv - A summary of all strains and bams in the alignment directory. aggregate - Stores data that has been aggregated across all strains or sequencing IDs. coverage - Contains coverage data at the strain or id level, presented in a variety of ways.","title":"Strain"},{"location":"pipeline-cegwas/","text":"cegwas-nf \u00b6 Docker image \u00b6 The wild-isolate docker file can be used. The cegwas-nf pipeline requires R/cegwas to be installed and a python module, awesome-slugify. andersenlab/wi-nf Requirements \u00b6 The cegwas-nf pipeline requires awesome-slugify ; Install using: pip install awesome-slugify It also requires that cegwas be installed. See the cegwas repo for more information. Alternatively, you can use the wi-nf docker image. Usage \u00b6 # cd to directory containing a trait file. nextflow run Andersenlab/cegwas-nf --in=<input file> Input Format \u00b6 Save trait data as a .tsv ; Strains in column 1. Traits are in columns 2 and above. strain trait1 trait2 trait3 trait4 AB1 0 0 19.6825 16.5026 AB4 14.5294 13.9775 18.9721 20.6803 BRC20067 18.0132 17.1509 18.4466 21.0243 CB4855_UK 0 13.2552 19.5265 21.7389 CB4856 14.4711 12.2563 19.3584 21.2358 CB4932 0 0 19.8662 20.326 CX11254 17.5516 16.9135 19.5696 21.7276 CX11264 15.3574 13.8575 18.9888 21.3832","title":"Cegwas"},{"location":"pipeline-cegwas/#cegwas-nf","text":"","title":"cegwas-nf"},{"location":"pipeline-cegwas/#docker_image","text":"The wild-isolate docker file can be used. The cegwas-nf pipeline requires R/cegwas to be installed and a python module, awesome-slugify. andersenlab/wi-nf","title":"Docker image"},{"location":"pipeline-cegwas/#requirements","text":"The cegwas-nf pipeline requires awesome-slugify ; Install using: pip install awesome-slugify It also requires that cegwas be installed. See the cegwas repo for more information. Alternatively, you can use the wi-nf docker image.","title":"Requirements"},{"location":"pipeline-cegwas/#usage","text":"# cd to directory containing a trait file. nextflow run Andersenlab/cegwas-nf --in=<input file>","title":"Usage"},{"location":"pipeline-cegwas/#input_format","text":"Save trait data as a .tsv ; Strains in column 1. Traits are in columns 2 and above. strain trait1 trait2 trait3 trait4 AB1 0 0 19.6825 16.5026 AB4 14.5294 13.9775 18.9721 20.6803 BRC20067 18.0132 17.1509 18.4466 21.0243 CB4855_UK 0 13.2552 19.5265 21.7389 CB4856 14.4711 12.2563 19.3584 21.2358 CB4932 0 0 19.8662 20.326 CX11254 17.5516 16.9135 19.5696 21.7276 CX11264 15.3574 13.8575 18.9888 21.3832","title":"Input Format"},{"location":"pipeline-concordance/","text":"concordance-nf \u00b6 The concordance-nf pipeline... concordance-nf Usage Overview Usage Debugging the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Parameters --cores --out --sample_sheet --fq_prefix --reference --tmpdir --bamdir --email Output Concordance/ concordance.png concordance_above_99.png isotype_groups.tsv gtcheck isotype_count.txt Heterozygosity fq_concordance.tsv concordance/pairwise/ (directory) Duplicates/ bam_duplicates.tsv fq/ strain/ variation/ sitelist.tsv.gz(+tbi) union_vcf.txt merged.raw.vcf.gz(+csi) concordance.vcf.gz(+csi) concordance.stats Usage \u00b6 \u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510 \u250c\u2510\u250c\u250c\u2500\u2510 \u2502 \u2502 \u2502\u2502\u2502\u2502\u2502 \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518 \u2518\u2514\u2518\u2514 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Regular job cores 4 --out Directory to output results concordance-<todays date> --sample_sheet fastq file (see help) sample_sheet.tsv --fq_prefix fastq file (see help) null --reference Reference Genome WS245/WS245.fa.gz --bamdir Location for bams bam --tmpdir A temporary directory tmp/ --email Email to be sent results HELP: http://andersenlab.org/dry-guide/pipeline-concordance/ Overview \u00b6 The concordance pipeline is used to detect sample swaps, identify samples with quality issues, and determine which wild isolate strains should be grouped together as an isotype. When performing sequencing, we often sequence the same DNA library or strain multiple times in order to attain adequate coverage. To ensure that samples are labeled properly we examine whether they contain discordant variant calls based on what strain they are labeled as. The concordance-nf pipeline will proceed to group FASTQs labeled as a strain regardless of the quality of the data. Therefore, if issues are suspected the problemetic data needs to be removed and the pipeline rerun. More details are available below. To determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.9%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings. The second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes. The process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely. Note There is at least one exception to the 99.9% cutoff rule we use to determine isotypes. We classify LSJ1 and N2 as separate isotypes despite the fact that they are greater than 99.9% identical. The strains are known to have diverged from one another, but work has demonstrated significant genetic and phenotypic differences. Usage \u00b6 The nextflow.config file sets most of the defaults you need to get the pipeline running locally for debugging purposes or on Quest. Debugging the pipeline locally \u00b6 The pipeline comes with a test dataset that you can use to make changes or fix problems. When running locally, you should install the andersen-lab-env which will install all the required dependencies. Fetching the reference You will need a reference genome to align to. You can fetch one by running the following command: curl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz > WS245.tar.gz tar -xvzf WS245.tar.gz Run the pipeline locally by setting -profile debug : nextflow run main.nf -profile debug -resume Debugging the pipeline on Quest \u00b6 When running on Quest, you should first run the quest debug profile. The Quest debug profile will use the test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume --email <your email> Running the pipeline on Quest \u00b6 The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest -resume --email <your email> Parameters \u00b6 The nextflow profiles configured in nextflow.config are designed to make it so that you don't need to change the parameters. However, the pipeline offers this flexibility if it is ever called for. --cores \u00b6 The number of cores to use during alignments and variant calling. --out \u00b6 A directory in which to output results. By default it will be concordance-YYYY-MM-DD where YYYY-MM-DD is todays date. --sample_sheet \u00b6 The sample sheet to use. Normally you can use the sample_sheet.tsv located in the base of the repo. This sample sheet is constructed usign scripts/construct_sample_sheet.sh . When running with -debug the sample sheet located in test_data/sample_sheet.tsv will be used. More information on the sample sheet and adding new sequence data on the Sample sheet page section. --fq_prefix \u00b6 A prefix path for FASTQs defined in the sample sheet. The sample sheet designed for usage on Quest ( sample_sheet.tsv ) uses absolute paths so no FASTQ prefix is necessary. It is set to null by default. --reference \u00b6 A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz --tmpdir \u00b6 A directory for storing temporary data. --bamdir \u00b6 A directory to output strain-level BAM files to. On Quest this is set by default to /projects/b1059/data/alignments/WI/strain/ --email \u00b6 Set an email to get notified when the pipeline succeeds or fails. Output \u00b6 Concordance/ \u00b6 concordance.png \u00b6 An image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise. concordance_above_99.png \u00b6 A close up view of the concordances showing more detail. isotype_groups.tsv \u00b6 This is the most important output file . It illustrates the isotypes identified for each strain and identifies potential issues. A file with the following structure: group strain isotype latitude longitude coverage unique_isotypes_per_group unique_groups_per_isotype strain_in_multiple_isotypes location_issue strain_conflict 1 AB1 AB1 -34.93 138.59 69.4687 1 1 FALSE FALSE FALSE 112 AB4 CB4858 -34.93 138.59 158.358 1 1 FALSE TRUE TRUE 112 ECA251 CB4858 34.1 -118.1 73.5843 1 1 FALSE TRUE TRUE 112 JU1960 CB4858 34.1897 -118.131 55.0373 1 1 FALSE TRUE TRUE 175 BRC20067 BRC20067 24.073 121.17 33.5934 1 1 FALSE FALSE FALSE 175 BRC20113 BRC20067 24.1242 121.283 38.9916 1 1 FALSE FALSE FALSE 186 BRC20231 MY23 23.5415 120.908 44.1452 1 1 FALSE TRUE TRUE 186 MY23 MY23 51.96 7.53 132.185 1 1 FALSE TRUE TRUE group - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--> AB1, 112 --> CB4858, BRC20067 --> 175). The number can change between analyses. strain - the strain isotype - the currently assigned isotype for a strain taken from the WI Strain Info spreadsheet. When new strains are added this is blank. latitude longitude coverage - Depth of coverage for strain. unique_isotypes_per_group - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question. unique_groups_per_isotype Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain. strain_in_multiple_isotypes - Indicates that a strain is falling into multiple isotypes (a problem!). location_issue - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored. strain_conflict - TRUE if any issue is present that should be investigated. gtcheck \u00b6 File produced using bcftools gtcheck ; Raw genotype differences between strains. isotype_count.txt \u00b6 Gives a count of the number of isotypes identified. Heterozygosity \u00b6 Number of heterozygous sites/strain. Can be an indicator of mixed samples or other issues. fq_concordance.tsv \u00b6 Intra-strain FASTQ-pair concordances. The format is: concordance/pairwise/ (directory) \u00b6 Contains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (> 2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (> 99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes. Duplicates/ \u00b6 bam_duplicates.tsv \u00b6 Summary of duplicate reads (determined by picard). fq/ \u00b6 The following files are output in the fastq directory fq_bam_idxstats.tsv # Stats generated with `samtools idxstats` fq_bam_stats.tsv # Stats generated with `samtools stats` fq_coverage.full.tsv # Detailed coverage numbers fq_coverage.tsv # Summary coverage of individual fastqs. strain/ \u00b6 The following files are output in the strain directory strain_bam_idxstats.tsv # Stats generated with `samtools idxstats` strain_bam_stats.tsv # Stats generated with `samtools stats` strain_coverage.full.tsv # Detailed coverage numbers strain_coverage.tsv # Summary coverage for strains. variation/ \u00b6 sitelist.tsv.gz(+tbi) \u00b6 The union-variant sitelist from all strains. union_vcf.txt \u00b6 Locations of the union VCFs. merged.raw.vcf.gz(+csi) \u00b6 The raw VCF. The following filters are applied before calculating concordance: min_depth>3 # Minimum Depth qual>30 # Quality (VCF=QUAL) mq>40 # Mapping Quality dv_dp=0.5 # DV/DP > 0.5 (high-quality allelic ALT bases over total depth) max_missing<0.05 # Max number of missing sites cannot exceed 5% concordance.vcf.gz(+csi) \u00b6 Filtered VCF, filtered for true SNPs (no monomorphic sites) concordance.stats \u00b6 Stats from concordance vcf. Contains the number of SNPs.","title":"WI-Concordance"},{"location":"pipeline-concordance/#concordance-nf","text":"The concordance-nf pipeline... concordance-nf Usage Overview Usage Debugging the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Parameters --cores --out --sample_sheet --fq_prefix --reference --tmpdir --bamdir --email Output Concordance/ concordance.png concordance_above_99.png isotype_groups.tsv gtcheck isotype_count.txt Heterozygosity fq_concordance.tsv concordance/pairwise/ (directory) Duplicates/ bam_duplicates.tsv fq/ strain/ variation/ sitelist.tsv.gz(+tbi) union_vcf.txt merged.raw.vcf.gz(+csi) concordance.vcf.gz(+csi) concordance.stats","title":"concordance-nf"},{"location":"pipeline-concordance/#usage","text":"\u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510 \u250c\u2510\u250c\u250c\u2500\u2510 \u2502 \u2502 \u2502\u2502\u2502\u2502\u2502 \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518 \u2518\u2514\u2518\u2514 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Regular job cores 4 --out Directory to output results concordance-<todays date> --sample_sheet fastq file (see help) sample_sheet.tsv --fq_prefix fastq file (see help) null --reference Reference Genome WS245/WS245.fa.gz --bamdir Location for bams bam --tmpdir A temporary directory tmp/ --email Email to be sent results HELP: http://andersenlab.org/dry-guide/pipeline-concordance/","title":"Usage"},{"location":"pipeline-concordance/#overview","text":"The concordance pipeline is used to detect sample swaps, identify samples with quality issues, and determine which wild isolate strains should be grouped together as an isotype. When performing sequencing, we often sequence the same DNA library or strain multiple times in order to attain adequate coverage. To ensure that samples are labeled properly we examine whether they contain discordant variant calls based on what strain they are labeled as. The concordance-nf pipeline will proceed to group FASTQs labeled as a strain regardless of the quality of the data. Therefore, if issues are suspected the problemetic data needs to be removed and the pipeline rerun. More details are available below. To determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.9%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings. The second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes. The process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely. Note There is at least one exception to the 99.9% cutoff rule we use to determine isotypes. We classify LSJ1 and N2 as separate isotypes despite the fact that they are greater than 99.9% identical. The strains are known to have diverged from one another, but work has demonstrated significant genetic and phenotypic differences.","title":"Overview"},{"location":"pipeline-concordance/#usage_1","text":"The nextflow.config file sets most of the defaults you need to get the pipeline running locally for debugging purposes or on Quest.","title":"Usage"},{"location":"pipeline-concordance/#debugging_the_pipeline_locally","text":"The pipeline comes with a test dataset that you can use to make changes or fix problems. When running locally, you should install the andersen-lab-env which will install all the required dependencies. Fetching the reference You will need a reference genome to align to. You can fetch one by running the following command: curl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz > WS245.tar.gz tar -xvzf WS245.tar.gz Run the pipeline locally by setting -profile debug : nextflow run main.nf -profile debug -resume","title":"Debugging the pipeline locally"},{"location":"pipeline-concordance/#debugging_the_pipeline_on_quest","text":"When running on Quest, you should first run the quest debug profile. The Quest debug profile will use the test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume --email <your email>","title":"Debugging the pipeline on Quest"},{"location":"pipeline-concordance/#running_the_pipeline_on_quest","text":"The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest -resume --email <your email>","title":"Running the pipeline on Quest"},{"location":"pipeline-concordance/#parameters","text":"The nextflow profiles configured in nextflow.config are designed to make it so that you don't need to change the parameters. However, the pipeline offers this flexibility if it is ever called for.","title":"Parameters"},{"location":"pipeline-concordance/#--cores","text":"The number of cores to use during alignments and variant calling.","title":"--cores"},{"location":"pipeline-concordance/#--out","text":"A directory in which to output results. By default it will be concordance-YYYY-MM-DD where YYYY-MM-DD is todays date.","title":"--out"},{"location":"pipeline-concordance/#--sample_sheet","text":"The sample sheet to use. Normally you can use the sample_sheet.tsv located in the base of the repo. This sample sheet is constructed usign scripts/construct_sample_sheet.sh . When running with -debug the sample sheet located in test_data/sample_sheet.tsv will be used. More information on the sample sheet and adding new sequence data on the Sample sheet page section.","title":"--sample_sheet"},{"location":"pipeline-concordance/#--fq_prefix","text":"A prefix path for FASTQs defined in the sample sheet. The sample sheet designed for usage on Quest ( sample_sheet.tsv ) uses absolute paths so no FASTQ prefix is necessary. It is set to null by default.","title":"--fq_prefix"},{"location":"pipeline-concordance/#--reference","text":"A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz","title":"--reference"},{"location":"pipeline-concordance/#--tmpdir","text":"A directory for storing temporary data.","title":"--tmpdir"},{"location":"pipeline-concordance/#--bamdir","text":"A directory to output strain-level BAM files to. On Quest this is set by default to /projects/b1059/data/alignments/WI/strain/","title":"--bamdir"},{"location":"pipeline-concordance/#--email","text":"Set an email to get notified when the pipeline succeeds or fails.","title":"--email"},{"location":"pipeline-concordance/#output","text":"","title":"Output"},{"location":"pipeline-concordance/#concordance","text":"","title":"Concordance/"},{"location":"pipeline-concordance/#concordancepng","text":"An image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise.","title":"concordance.png"},{"location":"pipeline-concordance/#concordance_above_99png","text":"A close up view of the concordances showing more detail.","title":"concordance_above_99.png"},{"location":"pipeline-concordance/#isotype_groupstsv","text":"This is the most important output file . It illustrates the isotypes identified for each strain and identifies potential issues. A file with the following structure: group strain isotype latitude longitude coverage unique_isotypes_per_group unique_groups_per_isotype strain_in_multiple_isotypes location_issue strain_conflict 1 AB1 AB1 -34.93 138.59 69.4687 1 1 FALSE FALSE FALSE 112 AB4 CB4858 -34.93 138.59 158.358 1 1 FALSE TRUE TRUE 112 ECA251 CB4858 34.1 -118.1 73.5843 1 1 FALSE TRUE TRUE 112 JU1960 CB4858 34.1897 -118.131 55.0373 1 1 FALSE TRUE TRUE 175 BRC20067 BRC20067 24.073 121.17 33.5934 1 1 FALSE FALSE FALSE 175 BRC20113 BRC20067 24.1242 121.283 38.9916 1 1 FALSE FALSE FALSE 186 BRC20231 MY23 23.5415 120.908 44.1452 1 1 FALSE TRUE TRUE 186 MY23 MY23 51.96 7.53 132.185 1 1 FALSE TRUE TRUE group - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--> AB1, 112 --> CB4858, BRC20067 --> 175). The number can change between analyses. strain - the strain isotype - the currently assigned isotype for a strain taken from the WI Strain Info spreadsheet. When new strains are added this is blank. latitude longitude coverage - Depth of coverage for strain. unique_isotypes_per_group - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question. unique_groups_per_isotype Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain. strain_in_multiple_isotypes - Indicates that a strain is falling into multiple isotypes (a problem!). location_issue - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored. strain_conflict - TRUE if any issue is present that should be investigated.","title":"isotype_groups.tsv"},{"location":"pipeline-concordance/#gtcheck","text":"File produced using bcftools gtcheck ; Raw genotype differences between strains.","title":"gtcheck"},{"location":"pipeline-concordance/#isotype_counttxt","text":"Gives a count of the number of isotypes identified.","title":"isotype_count.txt"},{"location":"pipeline-concordance/#heterozygosity","text":"Number of heterozygous sites/strain. Can be an indicator of mixed samples or other issues.","title":"Heterozygosity"},{"location":"pipeline-concordance/#fq_concordancetsv","text":"Intra-strain FASTQ-pair concordances. The format is:","title":"fq_concordance.tsv"},{"location":"pipeline-concordance/#concordancepairwise_directory","text":"Contains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (> 2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (> 99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes.","title":"concordance/pairwise/ (directory)"},{"location":"pipeline-concordance/#duplicates","text":"","title":"Duplicates/"},{"location":"pipeline-concordance/#bam_duplicatestsv","text":"Summary of duplicate reads (determined by picard).","title":"bam_duplicates.tsv"},{"location":"pipeline-concordance/#fq","text":"The following files are output in the fastq directory fq_bam_idxstats.tsv # Stats generated with `samtools idxstats` fq_bam_stats.tsv # Stats generated with `samtools stats` fq_coverage.full.tsv # Detailed coverage numbers fq_coverage.tsv # Summary coverage of individual fastqs.","title":"fq/"},{"location":"pipeline-concordance/#strain","text":"The following files are output in the strain directory strain_bam_idxstats.tsv # Stats generated with `samtools idxstats` strain_bam_stats.tsv # Stats generated with `samtools stats` strain_coverage.full.tsv # Detailed coverage numbers strain_coverage.tsv # Summary coverage for strains.","title":"strain/"},{"location":"pipeline-concordance/#variation","text":"","title":"variation/"},{"location":"pipeline-concordance/#sitelisttsvgztbi","text":"The union-variant sitelist from all strains.","title":"sitelist.tsv.gz(+tbi)"},{"location":"pipeline-concordance/#union_vcftxt","text":"Locations of the union VCFs.","title":"union_vcf.txt"},{"location":"pipeline-concordance/#mergedrawvcfgzcsi","text":"The raw VCF. The following filters are applied before calculating concordance: min_depth>3 # Minimum Depth qual>30 # Quality (VCF=QUAL) mq>40 # Mapping Quality dv_dp=0.5 # DV/DP > 0.5 (high-quality allelic ALT bases over total depth) max_missing<0.05 # Max number of missing sites cannot exceed 5%","title":"merged.raw.vcf.gz(+csi)"},{"location":"pipeline-concordance/#concordancevcfgzcsi","text":"Filtered VCF, filtered for true SNPs (no monomorphic sites)","title":"concordance.vcf.gz(+csi)"},{"location":"pipeline-concordance/#concordancestats","text":"Stats from concordance vcf. Contains the number of SNPs.","title":"concordance.stats"},{"location":"pipeline-docker/","text":"Create docker iamge Dockerfile Build docker image Push docker image to dockerhub Create docker iamge \u00b6 Docker can help us to maintain our computational environments. Each of our Nextflow pipeline has a dedicated docker iamge in our lab. And all the docker files should be avalible at dockerfile . Dockerfile \u00b6 To simplify the image building, we can use conda to install most of the tools. We can collect the tools avalible on conda cloud into a conda.yml file, which might looks like this. name: concordance-nf channels: - defaults - bioconda - r - biobuilds - conda-forge dependencies: - bwa=0.7.17 - sambamba=0.7.0 - samtools=1.9 - picard=2.20.6 - bcftools=1.9 # - vcfkit=0.1.6 - csvtk=0.18.2 - r=3.6.0 - r-ggplot2=3.1.1 - r-readr=1.3.1 - r-tidyverse=1.2.1 Then, build the Dockerfile as bellow. FROM continuumio/miniconda MAINTAINER XXX <email> COPY conda.yml . RUN \\ conda env update -n root -f conda.yml \\ && conda clean -a # install other tools not avalible on conda cloud RUN apt-get install -y procps RUN R -e \"install.packages('roperators',dependencies=TRUE, repos='http://cran.us.r-project.org')\" Note Put the conda.ymal and Dockerfile under the same folder. Build docker image \u00b6 To build the docker iamge, you need docker desktop installed in your local machine. Also you should sign up the dockerhub to enable pushing docker image to cloud. Go to the folder which have conda.ymal and Dockerfile , run docker build -t <dockerhub account>/<name of the iamge> . # don't ingore the dot here You can use docker image ls to check the image list you have in your local machine. Importantly, you have to check if the tools were installed sucessfully in your docker image. To test the docker iamge, run docker run -ti <dockerhub account>/<name of the iamge> sh The above command will let you into the docker image, where you can check the tools by their normal commands. Make sure all the tools you need have been installed appropriately. Push docker image to dockerhub \u00b6 After the image check, you are ready to push the image to dockerhub which allows you to download the image when ever you need to use. docker push <dockerhub account>/<name of the iamge>","title":"Build docker iamge"},{"location":"pipeline-docker/#create_docker_iamge","text":"Docker can help us to maintain our computational environments. Each of our Nextflow pipeline has a dedicated docker iamge in our lab. And all the docker files should be avalible at dockerfile .","title":"Create docker iamge"},{"location":"pipeline-docker/#dockerfile","text":"To simplify the image building, we can use conda to install most of the tools. We can collect the tools avalible on conda cloud into a conda.yml file, which might looks like this. name: concordance-nf channels: - defaults - bioconda - r - biobuilds - conda-forge dependencies: - bwa=0.7.17 - sambamba=0.7.0 - samtools=1.9 - picard=2.20.6 - bcftools=1.9 # - vcfkit=0.1.6 - csvtk=0.18.2 - r=3.6.0 - r-ggplot2=3.1.1 - r-readr=1.3.1 - r-tidyverse=1.2.1 Then, build the Dockerfile as bellow. FROM continuumio/miniconda MAINTAINER XXX <email> COPY conda.yml . RUN \\ conda env update -n root -f conda.yml \\ && conda clean -a # install other tools not avalible on conda cloud RUN apt-get install -y procps RUN R -e \"install.packages('roperators',dependencies=TRUE, repos='http://cran.us.r-project.org')\" Note Put the conda.ymal and Dockerfile under the same folder.","title":"Dockerfile"},{"location":"pipeline-docker/#build_docker_image","text":"To build the docker iamge, you need docker desktop installed in your local machine. Also you should sign up the dockerhub to enable pushing docker image to cloud. Go to the folder which have conda.ymal and Dockerfile , run docker build -t <dockerhub account>/<name of the iamge> . # don't ingore the dot here You can use docker image ls to check the image list you have in your local machine. Importantly, you have to check if the tools were installed sucessfully in your docker image. To test the docker iamge, run docker run -ti <dockerhub account>/<name of the iamge> sh The above command will let you into the docker image, where you can check the tools by their normal commands. Make sure all the tools you need have been installed appropriately.","title":"Build docker image"},{"location":"pipeline-docker/#push_docker_image_to_dockerhub","text":"After the image check, you are ready to push the image to dockerhub which allows you to download the image when ever you need to use. docker push <dockerhub account>/<name of the iamge>","title":"Push docker image to dockerhub"},{"location":"pipeline-nil-ril/","text":"nil-ril-nf \u00b6 The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. nil-ril-nf Docker image andersenlab/nil-ril-nf Usage Overview Docker image Usage Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Testing Parameters --debug --cores --A, --B --cA, --cB --out --fqs (FASTQs) --relative --vcf (Parental VCF) --reference --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ Docker image \u00b6 The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf \u00b6 The Dockerfile is stored in the root of the nil-nf github repo and is automatically built on Dockerhub whenever the repo is pushed. Usage \u00b6 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix ${params.relative} --reference Reference Genome /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default). Overview \u00b6 The nil-ril-nf pipeline: Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data. Docker image \u00b6 The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf The Dockerfile is stored in the root of the nil-ril-nf github repo and is automatically built on Dockerhub whenever the repo is pushed. Usage \u00b6 Profiles and Running the Pipeline \u00b6 The nextflow.config file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. travis - Used by travis-ci for testing purposes. Running the pipeline locally \u00b6 When running locally, the pipeline will run using the andersenlab/nil-ril-nf docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference: curl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz > WS245.tar.gz tar -xvzf WS245.tar.gz Run the pipeline locally with: nextflow run main.nf -profile local -resume Debugging the pipeline on Quest \u00b6 When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume Running the pipeline on Quest \u00b6 The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest -resume Testing \u00b6 If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz > WS245.tar.gz tar -xvzf WS245.tar.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS245.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-nf pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD Parameters \u00b6 --debug \u00b6 The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information. --cores \u00b6 The number of cores to use during alignments and variant calling. --A, --B \u00b6 Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details. --cA, --cB \u00b6 The color to use for parental strain A and B on plots. --out \u00b6 A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains. --fqs (FASTQs) \u00b6 In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. If you want to specify fastqs using an absolute path use --relative=false --relative \u00b6 Set to true by default. If you set --relative=false , fq's in the fq_sheet are expected to use an absolute path. --vcf (Parental VCF) \u00b6 Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use: /projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz --reference \u00b6 A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz --tmpdir \u00b6 A directory for storing temporary data. Output \u00b6 The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi] log.txt \u00b6 A summary of the nextflow run. duplicates/ \u00b6 bam_duplicates.tsv - A summary of duplicate reads from aligned bams. fq/ \u00b6 fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq SM/ \u00b6 If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz hmm/ \u00b6 Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes. plots/ \u00b6 coverage_comparison.(png/svg/pdf) - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/svg/pdf) - Coverage vs. percent duplicated. unmapped_reads.(png/svg/pdf) - Coverage vs. unmapped read percent. sitelist/ \u00b6 <A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains.","title":"NIL / RIL"},{"location":"pipeline-nil-ril/#nil-ril-nf","text":"The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. nil-ril-nf Docker image andersenlab/nil-ril-nf Usage Overview Docker image Usage Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Testing Parameters --debug --cores --A, --B --cA, --cB --out --fqs (FASTQs) --relative --vcf (Parental VCF) --reference --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/","title":"nil-ril-nf"},{"location":"pipeline-nil-ril/#docker_image","text":"The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image:","title":"Docker image"},{"location":"pipeline-nil-ril/#andersenlabnil-ril-nf","text":"The Dockerfile is stored in the root of the nil-nf github repo and is automatically built on Dockerhub whenever the repo is pushed.","title":"andersenlab/nil-ril-nf"},{"location":"pipeline-nil-ril/#usage","text":"\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix ${params.relative} --reference Reference Genome /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default).","title":"Usage"},{"location":"pipeline-nil-ril/#overview","text":"The nil-ril-nf pipeline: Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data.","title":"Overview"},{"location":"pipeline-nil-ril/#docker_image_1","text":"The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf The Dockerfile is stored in the root of the nil-ril-nf github repo and is automatically built on Dockerhub whenever the repo is pushed.","title":"Docker image"},{"location":"pipeline-nil-ril/#usage_1","text":"","title":"Usage"},{"location":"pipeline-nil-ril/#profiles_and_running_the_pipeline","text":"The nextflow.config file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. travis - Used by travis-ci for testing purposes.","title":"Profiles and Running the Pipeline"},{"location":"pipeline-nil-ril/#running_the_pipeline_locally","text":"When running locally, the pipeline will run using the andersenlab/nil-ril-nf docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference: curl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz > WS245.tar.gz tar -xvzf WS245.tar.gz Run the pipeline locally with: nextflow run main.nf -profile local -resume","title":"Running the pipeline locally"},{"location":"pipeline-nil-ril/#debugging_the_pipeline_on_quest","text":"When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume","title":"Debugging the pipeline on Quest"},{"location":"pipeline-nil-ril/#running_the_pipeline_on_quest","text":"The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest -resume","title":"Running the pipeline on Quest"},{"location":"pipeline-nil-ril/#testing","text":"If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz > WS245.tar.gz tar -xvzf WS245.tar.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS245.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-nf pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD","title":"Testing"},{"location":"pipeline-nil-ril/#parameters","text":"","title":"Parameters"},{"location":"pipeline-nil-ril/#--debug","text":"The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information.","title":"--debug"},{"location":"pipeline-nil-ril/#--cores","text":"The number of cores to use during alignments and variant calling.","title":"--cores"},{"location":"pipeline-nil-ril/#--a_--b","text":"Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details.","title":"--A, --B"},{"location":"pipeline-nil-ril/#--ca_--cb","text":"The color to use for parental strain A and B on plots.","title":"--cA, --cB"},{"location":"pipeline-nil-ril/#--out","text":"A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains.","title":"--out"},{"location":"pipeline-nil-ril/#--fqs_fastqs","text":"In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. If you want to specify fastqs using an absolute path use --relative=false","title":"--fqs (FASTQs)"},{"location":"pipeline-nil-ril/#--relative","text":"Set to true by default. If you set --relative=false , fq's in the fq_sheet are expected to use an absolute path.","title":"--relative"},{"location":"pipeline-nil-ril/#--vcf_parental_vcf","text":"Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use: /projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz","title":"--vcf (Parental VCF)"},{"location":"pipeline-nil-ril/#--reference","text":"A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz","title":"--reference"},{"location":"pipeline-nil-ril/#--tmpdir","text":"A directory for storing temporary data.","title":"--tmpdir"},{"location":"pipeline-nil-ril/#output","text":"The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi]","title":"Output"},{"location":"pipeline-nil-ril/#logtxt","text":"A summary of the nextflow run.","title":"log.txt"},{"location":"pipeline-nil-ril/#duplicates","text":"bam_duplicates.tsv - A summary of duplicate reads from aligned bams.","title":"duplicates/"},{"location":"pipeline-nil-ril/#fq","text":"fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq","title":"fq/"},{"location":"pipeline-nil-ril/#sm","text":"If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz","title":"SM/"},{"location":"pipeline-nil-ril/#hmm","text":"Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes.","title":"hmm/"},{"location":"pipeline-nil-ril/#plots","text":"coverage_comparison.(png/svg/pdf) - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/svg/pdf) - Coverage vs. percent duplicated. unmapped_reads.(png/svg/pdf) - Coverage vs. unmapped read percent.","title":"plots/"},{"location":"pipeline-nil-ril/#sitelist","text":"<A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains.","title":"sitelist/"},{"location":"pipeline-overview/","text":"Pipeline Overview \u00b6 An overview of the sequencing pipelines is shown above. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline.","title":"Overview"},{"location":"pipeline-overview/#pipeline_overview","text":"An overview of the sequencing pipelines is shown above. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline.","title":"Pipeline Overview"},{"location":"pipeline-trimming/","text":"trimmomatic-nf \u00b6 The trimmomatic workflow performs trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trimmomatic workflow on low-coverage NIL or RIL data. trimmomatic-nf Usage Prerequisites Running the pipeline Parameters Overview Output Cleanup Poor quality data Backup Usage \u00b6 Prerequisites \u00b6 You have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be /projects/b1059/data/fastq/WI/dna/raw/<folder_name> FASTQs must end in a .fq.gz extension for the pipeline to work.. You have modified FASTQ names if necessary to add strain names or other identifying information. You have installed software-requirements, preferably using the andersen-lab-env . You can learn how to install the environment here . Software requirements trimmomatic fastqc multiqc Note All FASTQs should end with a _1.fq.gz or a _2.fq.gz . You can rename FASTQs using the rename command: rename --dry-run --subst .fastq.gz .fq.gz --subst _R1_001 _1 --subst _R2_001 _2 *.fastq.gz The --dry-run flag will output how files will be renamed. Review the output and remove the flag when you are ready. Running the pipeline \u00b6 First you will need to cd to the directory containing the raw FASTQs. This directory will be downloaded into a raw parent directory. When you run the pipeline it will create a sequence directory of the same name in an existing or newly created processed directory and dump FASTQs there. Unlike all other pipelines, the trimmomatic-nf pipeline is run directly from the git repo nextflow run andersenlab/trimmomatic-nf -latest --email <your email address> Note The pipeline is designed to not be destructive. Trimming creates from the raw parent directory to the processed parent directory as shown below. /projects/b1059/data/fastq/WI/dna/raw/<seq_folder>/S_1.fq.gz -- trimming --> /projects/b1059/data/fastq/WI/dna/processed/<folder_name>/S_1P.fq.gz Parameters \u00b6 --email - Specify an email address to be notified when the pipeline succeeds or fails. Overview \u00b6 Output \u00b6 The resulting trimmed FASTQs will be output in the processed directory located up one level from the current directory. For example: FASTQs are originally deposited in this directory /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq You run the pipeline while sitting in the same directory: /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq And results are output in the following directory: /projects/b1059/data/fastq/WI/dna/processed/new_wi_seq Report output The trimmomatic-nf pipeline outputs four files, all of which are located in the processed directory. Continuing with the example above, report files will be located here: /projects/b1059/data/fastq/WI/dna/processed/new_wi_seq/report The report output files are: md5sum.txt - md5 hashes of all the untrimmed FASTQs. These can be used to verify the integrity of the files. trimming_log.txt - A summary of the pipeline-run and software environment. multiqc_report_pre.html - Aggregated FASTQC report before trimming. multiqc_report_post.html - Aggregated FASTQC report after trimming. Additionally, the raw/seq and processed/seq will have fastqc/ folders containing the original, unaggregatred FASTQC reports. Cleanup \u00b6 If you have triple-checked everything and are satisfied with the results, the original, raw sequence data can be deleted. Poor quality data \u00b6 If you observe poor quality sequence data you should remove it. Backup \u00b6 Once you have completed the trimmomatic-nf pipeline you should backup the FASTQs. More information on this is available in the backup","title":"Trimming"},{"location":"pipeline-trimming/#trimmomatic-nf","text":"The trimmomatic workflow performs trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trimmomatic workflow on low-coverage NIL or RIL data. trimmomatic-nf Usage Prerequisites Running the pipeline Parameters Overview Output Cleanup Poor quality data Backup","title":"trimmomatic-nf"},{"location":"pipeline-trimming/#usage","text":"","title":"Usage"},{"location":"pipeline-trimming/#prerequisites","text":"You have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be /projects/b1059/data/fastq/WI/dna/raw/<folder_name> FASTQs must end in a .fq.gz extension for the pipeline to work.. You have modified FASTQ names if necessary to add strain names or other identifying information. You have installed software-requirements, preferably using the andersen-lab-env . You can learn how to install the environment here . Software requirements trimmomatic fastqc multiqc Note All FASTQs should end with a _1.fq.gz or a _2.fq.gz . You can rename FASTQs using the rename command: rename --dry-run --subst .fastq.gz .fq.gz --subst _R1_001 _1 --subst _R2_001 _2 *.fastq.gz The --dry-run flag will output how files will be renamed. Review the output and remove the flag when you are ready.","title":"Prerequisites"},{"location":"pipeline-trimming/#running_the_pipeline","text":"First you will need to cd to the directory containing the raw FASTQs. This directory will be downloaded into a raw parent directory. When you run the pipeline it will create a sequence directory of the same name in an existing or newly created processed directory and dump FASTQs there. Unlike all other pipelines, the trimmomatic-nf pipeline is run directly from the git repo nextflow run andersenlab/trimmomatic-nf -latest --email <your email address> Note The pipeline is designed to not be destructive. Trimming creates from the raw parent directory to the processed parent directory as shown below. /projects/b1059/data/fastq/WI/dna/raw/<seq_folder>/S_1.fq.gz -- trimming --> /projects/b1059/data/fastq/WI/dna/processed/<folder_name>/S_1P.fq.gz","title":"Running the pipeline"},{"location":"pipeline-trimming/#parameters","text":"--email - Specify an email address to be notified when the pipeline succeeds or fails.","title":"Parameters"},{"location":"pipeline-trimming/#overview","text":"","title":"Overview"},{"location":"pipeline-trimming/#output","text":"The resulting trimmed FASTQs will be output in the processed directory located up one level from the current directory. For example: FASTQs are originally deposited in this directory /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq You run the pipeline while sitting in the same directory: /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq And results are output in the following directory: /projects/b1059/data/fastq/WI/dna/processed/new_wi_seq Report output The trimmomatic-nf pipeline outputs four files, all of which are located in the processed directory. Continuing with the example above, report files will be located here: /projects/b1059/data/fastq/WI/dna/processed/new_wi_seq/report The report output files are: md5sum.txt - md5 hashes of all the untrimmed FASTQs. These can be used to verify the integrity of the files. trimming_log.txt - A summary of the pipeline-run and software environment. multiqc_report_pre.html - Aggregated FASTQC report before trimming. multiqc_report_post.html - Aggregated FASTQC report after trimming. Additionally, the raw/seq and processed/seq will have fastqc/ folders containing the original, unaggregatred FASTQC reports.","title":"Output"},{"location":"pipeline-trimming/#cleanup","text":"If you have triple-checked everything and are satisfied with the results, the original, raw sequence data can be deleted.","title":"Cleanup"},{"location":"pipeline-trimming/#poor_quality_data","text":"If you observe poor quality sequence data you should remove it.","title":"Poor quality data"},{"location":"pipeline-trimming/#backup","text":"Once you have completed the trimmomatic-nf pipeline you should backup the FASTQs. More information on this is available in the backup","title":"Backup"},{"location":"pipeline-wi/","text":"wi-nf \u00b6 The wi-nf pipeline aligns, calls variants, and performs analysis from wild isolate sequence data. The output of the wi-nf pipeline can be uploaded to google storage as a new release for the CeNDR website. wi-nf Usage Pipeline Overview Usage Docker File pyenv environments Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Configuration --cores --out --fqs (FASTQs) --fqs_file_prefix --reference --tmpdir Sample Sheet Output log.txt alignment/ cegwas/ isotype isotype/tsv/ isotype/vcf/ phenotype/ popgen/ report/ track/ Variation/ Usage \u00b6 \u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2584\u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2588\u2591\u2588\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2588\u2591\u2588\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u258c \u2580\u2580 \u2580\u2580 \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2580 \u2580\u2580 \u2580 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test ${params.debug} --cores Regular job cores ${params.cores} --out Directory to output results ${params.out} --fqs fastq file (see help) ${params.fqs} --fq_file_prefix fastq prefix ${params.fq_file_prefix} --reference Reference Genome ${params.reference} --annotation_reference SnpEff annotation ${params.annotation_reference} --bamdir Location for bams ${params.bamdir} --tmpdir A temporary directory ${params.tmpdir} --email Email to be sent results ${params.email} HELP: http://andersenlab.org/dry-guide/pipeline-wi/ Pipeline Overview \u00b6 Usage \u00b6 Docker File \u00b6 andersenlab/wi-nf is the docker image used within the wi-nf pipeline. If Quest ever supports singularity, it can be converted to a singularity image and used with Nextflow. pyenv environments \u00b6 The pipeline uses two python environments due to clashing dependencies. The two environments are: vcf-kit - A python 2.7.14 environment with vcf-kit and cyvcf installed. multiqc - A python 3.6.0 environment with multiqc installed. If you are not using the docker container you must install these virtual environments by running the setup_pyenv.sh script: bash setup_pyenv.sh If you require one of these environments for a process within the pipeline, add the following as the first line of your script: source init_pyenv.sh && pyenv activate <environment name> Profiles and Running the Pipeline \u00b6 The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. Running the pipeline locally \u00b6 When running locally, the pipeline will run using the andersenlab/wi-nf docker image. You must have docker installed. nextflow run main.nf -profile local -resume Debugging the pipeline on Quest \u00b6 When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume Running the pipeline on Quest \u00b6 The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest -resume Configuration \u00b6 Most configuration is handled using the -profile flag and nextflow.config ; If you want to fine tune things you can use the options below. --cores \u00b6 The number of cores to use during alignments and variant calling. --out \u00b6 A directory in which to output results. By default it will be WI-YYYY-MM-DD where YYYY-MM-DD is todays date. --fqs (FASTQs) \u00b6 When running the wi-nf pipeline you must provide a sample sheet that tells it where fastqs are and which samples group into isotypes. By default, this is the sample sheet in the base of the wi-nf repo ( SM_sample_sheet.tsv ), but can be specified using --fqs if an alternative is required. The sample sheet provides information on the isotype, fastq_pairs, library, location of fastqs, and sequencing folder. More information on the sample sheet and adding new sequence data in the Sample sheet section. --fqs_file_prefix \u00b6 A prefix path for FASTQs defined in a sample sheet. The sample sheet designed for usage on Quest ( SM_sample_sheet.tsv ) uses absolute paths so no FASTQ prefix is necessary. Additionally, there is no need to set this option as it is set for you when using the -profile flag. This option is only necessary (maybe) with a custom dataset where you are not using absolute paths to reference FASTQs. --reference \u00b6 A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz --tmpdir \u00b6 A directory for storing temporary data. Sample Sheet \u00b6 The sample sheet defines which FASTQs belong to which isotypes, set FASTQ IDs, library, and more. The sample sheet is constructed using the scripts/construct_SM_sheet.sh script. When new sequence data is added, this needs to be modified to add information about the new FASTQs. Updates to this script should be committed to the repo. Important You have to assign isotypes using the concordance script before they can be processed using wi-nf. When you run the scripts/construct_SM_sheet.sh , it will output the SM_Sample_sheet.tsv file in the base of the repo. You should carefully examine the diff of this file using Git to ensure it is modifying the sample sheet correctly. Errors can be disasterous . Note that the output uses absolute paths to FASTQ files. Sample sheet structure AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Notice that the file does not include a header. The table with corresponding columns looks like this. isotype fastq_pair_id library fastq-1-path fastq-2-path sequencing_folder AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set The columns are detailed below: isotype - The name of the isotype. It is used to group FASTQ-pairs into BAMs which are treated as individuals. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. There is (unfortunately) no standard for this. library - A name identifying the DNA library. If the FASTQ-pairs for a strain were sequenced using different library preps they should be assigned different library names. Likewise, if they were the same DNA library they should have the same library name. Keep in mind that within an isotype the library names for each strain must be independent. fastq-1-path - The absolute path of the first fastq. fastq-2-path - The absolute path of the second fastq. Output \u00b6 The output from the pipeline is structured to enable it to easily be integrated with CeNDR. The final output directory looks like this: \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 alignment \u2502 \u251c\u2500\u2500 isotype_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 isotype_bam_stats.tsv \u2502 \u251c\u2500\u2500 isotype_coverage.full.tsv \u2502 \u2514\u2500\u2500 isotype_coverage.tsv \u251c\u2500\u2500 cegwas \u2502 \u251c\u2500\u2500 kinship.Rda \u2502 \u2514\u2500\u2500 snps.Rda \u251c\u2500\u2500 isotype \u2502 \u251c\u2500\u2500 tsv \u2502 \u2502 \u251c\u2500\u2500 <isotype>.(date).tsv.gz \u2502 \u2502 \u2514\u2500\u2500 <isotype>.(date).tsv.gz \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 <isotype>.(date).vcf.gz \u2502 \u2514\u2500\u2500 <isotype>.(date).vcf.gz.tbi \u251c\u2500\u2500 phenotype \u2502 \u251c\u2500\u2500 MT_content.tsv \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 telseq.tsv \u251c\u2500\u2500 popgen \u2502 \u251c\u2500\u2500 WI.(date).tajima.bed.gz \u2502 \u251c\u2500\u2500 WI.(date).tajima.bed.gz.tbi \u2502 \u2514\u2500\u2500 trees \u2502 \u251c\u2500\u2500 I.pdf \u2502 \u251c\u2500\u2500 I.png \u2502 \u251c\u2500\u2500 I.tree \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 genome.pdf \u2502 \u251c\u2500\u2500 genome.png \u2502 \u2514\u2500\u2500 genome.tree \u251c\u2500\u2500 report \u2502 \u251c\u2500\u2500 multiqc.html \u2502 \u2514\u2500\u2500 multiqc_data \u2502 \u251c\u2500\u2500 multiqc_bcftools_stats.json \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_fastqc.json \u2502 \u251c\u2500\u2500 multiqc_general_stats.json \u2502 \u251c\u2500\u2500 multiqc_picard_AlignmentSummaryMetrics.json \u2502 \u251c\u2500\u2500 multiqc_picard_dups.json \u2502 \u251c\u2500\u2500 multiqc_picard_insertSize.json \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.json \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.json \u2502 \u251c\u2500\u2500 multiqc_snpeff.json \u2502 \u2514\u2500\u2500 multiqc_sources.json \u251c\u2500\u2500 tracks \u2502 \u251c\u2500\u2500 (date).HIGH.bed.gz \u2502 \u251c\u2500\u2500 (date).HIGH.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).LOW.bed.gz \u2502 \u251c\u2500\u2500 (date).LOW.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).MODERATE.bed.gz \u2502 \u251c\u2500\u2500 (date).MODERATE.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).MODIFIER.bed.gz \u2502 \u251c\u2500\u2500 (date).MODIFIER.bed.gz.tbi \u2502 \u251c\u2500\u2500 phastcons.bed.gz \u2502 \u251c\u2500\u2500 phastcons.bed.gz.tbi \u2502 \u251c\u2500\u2500 phylop.bed.gz \u2502 \u2514\u2500\u2500 phylop.bed.gz.tbi \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.csi \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).soft-filter.stats.txt \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.csi \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).hard-filter.stats.txt \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.tsv \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.frequency.tsv \u251c\u2500\u2500 WI.(date).impute.vcf.gz \u251c\u2500\u2500 WI.(date).impute.vcf.gz.csi \u251c\u2500\u2500 WI.(date).impute.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).impute.stats.txt \u251c\u2500\u2500 sitelist.tsv.gz \u2514\u2500\u2500 sitelist.tsv.gz.tbi log.txt \u00b6 A summary of the nextflow run. alignment/ \u00b6 Alignment statistics by isotype isotype_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. isotype_bam_stats.tsv - BAM summary at the sample level isotype_coverage.full.tsv - Coverage at the sample level isotype_coverage.tsv - Simple coverage at the sample level. cegwas/ \u00b6 kinship.Rda - A kinship matrix constructed from WI.(date).impute.vcf.gz snps.Rda - A mapping snp set generated from WI.(date).hard-filter.vcf.gz isotype \u00b6 This directory contains files that integrate with the genome browser on CeDNR. isotype/tsv/ \u00b6 This directory contains tsv's that are used to show where variants are in CeNDR. isotype/vcf/ \u00b6 This direcoty contains the VCFs of isotypes. phenotype/ \u00b6 MT_content.tsv - Mitochondrial content (MtDNA cov / Nuclear cov). kmers.tsv - 6-mers for each isotype. telseq.tsv - Telomere length for each isotype ~ split out by read group. popgen/ \u00b6 WI.(date).tajima.bed.gz - Tajima's D bedfile for use on the report viewer of CeNDR. WI.(date).tajima.bed.gz.tbi - Tajima's D index. trees/ - Phylogenetic trees for each chromosome and the entire genome. I.(pdf|png|tree) ... genome.(pdf|png|tree) report/ \u00b6 multiqc.html - A comprehensive report of the sequencing run. multiqc_data/ multiqc_bcftools_stats.json multiqc_data.json multiqc_fastqc.json multiqc_general_stats.json multiqc_picard_AlignmentSummaryMetrics.json multiqc_picard_dups.json multiqc_picard_insertSize.json multiqc_samtools_idxstats.json multiqc_samtools_stats.json multiqc_snpeff.json multiqc_sources.json track/ \u00b6 (date).LOW.bed.gz(+.tbi) - LOW effect mutations bed track and index. (date).MODERATE.bed.gz(+.tbi) - MODERATE effect mutations bed track and index. (date).HIGH.bed.gz(+.tbi) - HIGH effect mutations bed track and index. (date).MODIFIER.bed.gz(+.tbi) - MODERATE effect mutations bed track and index. phastcons.bed.gz(+.tbi) - Phastcons bed track and index. phylop.bed.gz(+.tbi) - PhyloP bed track and index Variation/ \u00b6 WI.(date).soft-filter.vcf.gz(+.csi|+.tbi) - WI.(date).soft-filter.stats.txt - WI.(date).hard-filter.vcf.gz(+.csi|+.tbi) - WI.(date).hard-filter.stats.txt - WI.(date).hard-filter.genotypes.tsv - WI.(date).hard-filter.genotypes.frequency.tsv - WI.(date).impute.vcf.gz(+.csi|+.tbi) - WI.(date).impute.stats.txt - sitelist.tsv.gz(+.tbi) - Union of all sites identified in original SNV variant calling round.","title":"WI"},{"location":"pipeline-wi/#wi-nf","text":"The wi-nf pipeline aligns, calls variants, and performs analysis from wild isolate sequence data. The output of the wi-nf pipeline can be uploaded to google storage as a new release for the CeNDR website. wi-nf Usage Pipeline Overview Usage Docker File pyenv environments Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Configuration --cores --out --fqs (FASTQs) --fqs_file_prefix --reference --tmpdir Sample Sheet Output log.txt alignment/ cegwas/ isotype isotype/tsv/ isotype/vcf/ phenotype/ popgen/ report/ track/ Variation/","title":"wi-nf"},{"location":"pipeline-wi/#usage","text":"\u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2584\u2584 \u2584 \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2588\u2591\u2588\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2588\u2591\u2588\u2584\u2584\u2584\u2584 \u2590\u2591\u258c \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u258c \u2590\u2591\u2591\u258c\u2590\u2591\u258c \u2580\u2580 \u2580\u2580 \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \u2580 \u2580\u2580 \u2580 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test ${params.debug} --cores Regular job cores ${params.cores} --out Directory to output results ${params.out} --fqs fastq file (see help) ${params.fqs} --fq_file_prefix fastq prefix ${params.fq_file_prefix} --reference Reference Genome ${params.reference} --annotation_reference SnpEff annotation ${params.annotation_reference} --bamdir Location for bams ${params.bamdir} --tmpdir A temporary directory ${params.tmpdir} --email Email to be sent results ${params.email} HELP: http://andersenlab.org/dry-guide/pipeline-wi/","title":"Usage"},{"location":"pipeline-wi/#pipeline_overview","text":"","title":"Pipeline Overview"},{"location":"pipeline-wi/#usage_1","text":"","title":"Usage"},{"location":"pipeline-wi/#docker_file","text":"andersenlab/wi-nf is the docker image used within the wi-nf pipeline. If Quest ever supports singularity, it can be converted to a singularity image and used with Nextflow.","title":"Docker File"},{"location":"pipeline-wi/#pyenv_environments","text":"The pipeline uses two python environments due to clashing dependencies. The two environments are: vcf-kit - A python 2.7.14 environment with vcf-kit and cyvcf installed. multiqc - A python 3.6.0 environment with multiqc installed. If you are not using the docker container you must install these virtual environments by running the setup_pyenv.sh script: bash setup_pyenv.sh If you require one of these environments for a process within the pipeline, add the following as the first line of your script: source init_pyenv.sh && pyenv activate <environment name>","title":"pyenv environments"},{"location":"pipeline-wi/#profiles_and_running_the_pipeline","text":"The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset.","title":"Profiles and Running the Pipeline"},{"location":"pipeline-wi/#running_the_pipeline_locally","text":"When running locally, the pipeline will run using the andersenlab/wi-nf docker image. You must have docker installed. nextflow run main.nf -profile local -resume","title":"Running the pipeline locally"},{"location":"pipeline-wi/#debugging_the_pipeline_on_quest","text":"When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume","title":"Debugging the pipeline on Quest"},{"location":"pipeline-wi/#running_the_pipeline_on_quest","text":"The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest -resume","title":"Running the pipeline on Quest"},{"location":"pipeline-wi/#configuration","text":"Most configuration is handled using the -profile flag and nextflow.config ; If you want to fine tune things you can use the options below.","title":"Configuration"},{"location":"pipeline-wi/#--cores","text":"The number of cores to use during alignments and variant calling.","title":"--cores"},{"location":"pipeline-wi/#--out","text":"A directory in which to output results. By default it will be WI-YYYY-MM-DD where YYYY-MM-DD is todays date.","title":"--out"},{"location":"pipeline-wi/#--fqs_fastqs","text":"When running the wi-nf pipeline you must provide a sample sheet that tells it where fastqs are and which samples group into isotypes. By default, this is the sample sheet in the base of the wi-nf repo ( SM_sample_sheet.tsv ), but can be specified using --fqs if an alternative is required. The sample sheet provides information on the isotype, fastq_pairs, library, location of fastqs, and sequencing folder. More information on the sample sheet and adding new sequence data in the Sample sheet section.","title":"--fqs (FASTQs)"},{"location":"pipeline-wi/#--fqs_file_prefix","text":"A prefix path for FASTQs defined in a sample sheet. The sample sheet designed for usage on Quest ( SM_sample_sheet.tsv ) uses absolute paths so no FASTQ prefix is necessary. Additionally, there is no need to set this option as it is set for you when using the -profile flag. This option is only necessary (maybe) with a custom dataset where you are not using absolute paths to reference FASTQs.","title":"--fqs_file_prefix"},{"location":"pipeline-wi/#--reference","text":"A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz","title":"--reference"},{"location":"pipeline-wi/#--tmpdir","text":"A directory for storing temporary data.","title":"--tmpdir"},{"location":"pipeline-wi/#sample_sheet","text":"The sample sheet defines which FASTQs belong to which isotypes, set FASTQ IDs, library, and more. The sample sheet is constructed using the scripts/construct_SM_sheet.sh script. When new sequence data is added, this needs to be modified to add information about the new FASTQs. Updates to this script should be committed to the repo. Important You have to assign isotypes using the concordance script before they can be processed using wi-nf. When you run the scripts/construct_SM_sheet.sh , it will output the SM_Sample_sheet.tsv file in the base of the repo. You should carefully examine the diff of this file using Git to ensure it is modifying the sample sheet correctly. Errors can be disasterous . Note that the output uses absolute paths to FASTQ files. Sample sheet structure AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Notice that the file does not include a header. The table with corresponding columns looks like this. isotype fastq_pair_id library fastq-1-path fastq-2-path sequencing_folder AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set The columns are detailed below: isotype - The name of the isotype. It is used to group FASTQ-pairs into BAMs which are treated as individuals. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. There is (unfortunately) no standard for this. library - A name identifying the DNA library. If the FASTQ-pairs for a strain were sequenced using different library preps they should be assigned different library names. Likewise, if they were the same DNA library they should have the same library name. Keep in mind that within an isotype the library names for each strain must be independent. fastq-1-path - The absolute path of the first fastq. fastq-2-path - The absolute path of the second fastq.","title":"Sample Sheet"},{"location":"pipeline-wi/#output","text":"The output from the pipeline is structured to enable it to easily be integrated with CeNDR. The final output directory looks like this: \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 alignment \u2502 \u251c\u2500\u2500 isotype_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 isotype_bam_stats.tsv \u2502 \u251c\u2500\u2500 isotype_coverage.full.tsv \u2502 \u2514\u2500\u2500 isotype_coverage.tsv \u251c\u2500\u2500 cegwas \u2502 \u251c\u2500\u2500 kinship.Rda \u2502 \u2514\u2500\u2500 snps.Rda \u251c\u2500\u2500 isotype \u2502 \u251c\u2500\u2500 tsv \u2502 \u2502 \u251c\u2500\u2500 <isotype>.(date).tsv.gz \u2502 \u2502 \u2514\u2500\u2500 <isotype>.(date).tsv.gz \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 <isotype>.(date).vcf.gz \u2502 \u2514\u2500\u2500 <isotype>.(date).vcf.gz.tbi \u251c\u2500\u2500 phenotype \u2502 \u251c\u2500\u2500 MT_content.tsv \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 telseq.tsv \u251c\u2500\u2500 popgen \u2502 \u251c\u2500\u2500 WI.(date).tajima.bed.gz \u2502 \u251c\u2500\u2500 WI.(date).tajima.bed.gz.tbi \u2502 \u2514\u2500\u2500 trees \u2502 \u251c\u2500\u2500 I.pdf \u2502 \u251c\u2500\u2500 I.png \u2502 \u251c\u2500\u2500 I.tree \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 genome.pdf \u2502 \u251c\u2500\u2500 genome.png \u2502 \u2514\u2500\u2500 genome.tree \u251c\u2500\u2500 report \u2502 \u251c\u2500\u2500 multiqc.html \u2502 \u2514\u2500\u2500 multiqc_data \u2502 \u251c\u2500\u2500 multiqc_bcftools_stats.json \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_fastqc.json \u2502 \u251c\u2500\u2500 multiqc_general_stats.json \u2502 \u251c\u2500\u2500 multiqc_picard_AlignmentSummaryMetrics.json \u2502 \u251c\u2500\u2500 multiqc_picard_dups.json \u2502 \u251c\u2500\u2500 multiqc_picard_insertSize.json \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.json \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.json \u2502 \u251c\u2500\u2500 multiqc_snpeff.json \u2502 \u2514\u2500\u2500 multiqc_sources.json \u251c\u2500\u2500 tracks \u2502 \u251c\u2500\u2500 (date).HIGH.bed.gz \u2502 \u251c\u2500\u2500 (date).HIGH.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).LOW.bed.gz \u2502 \u251c\u2500\u2500 (date).LOW.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).MODERATE.bed.gz \u2502 \u251c\u2500\u2500 (date).MODERATE.bed.gz.tbi \u2502 \u251c\u2500\u2500 (date).MODIFIER.bed.gz \u2502 \u251c\u2500\u2500 (date).MODIFIER.bed.gz.tbi \u2502 \u251c\u2500\u2500 phastcons.bed.gz \u2502 \u251c\u2500\u2500 phastcons.bed.gz.tbi \u2502 \u251c\u2500\u2500 phylop.bed.gz \u2502 \u2514\u2500\u2500 phylop.bed.gz.tbi \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.csi \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).soft-filter.stats.txt \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.csi \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).hard-filter.stats.txt \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.tsv \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.frequency.tsv \u251c\u2500\u2500 WI.(date).impute.vcf.gz \u251c\u2500\u2500 WI.(date).impute.vcf.gz.csi \u251c\u2500\u2500 WI.(date).impute.vcf.gz.tbi \u251c\u2500\u2500 WI.(date).impute.stats.txt \u251c\u2500\u2500 sitelist.tsv.gz \u2514\u2500\u2500 sitelist.tsv.gz.tbi","title":"Output"},{"location":"pipeline-wi/#logtxt","text":"A summary of the nextflow run.","title":"log.txt"},{"location":"pipeline-wi/#alignment","text":"Alignment statistics by isotype isotype_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. isotype_bam_stats.tsv - BAM summary at the sample level isotype_coverage.full.tsv - Coverage at the sample level isotype_coverage.tsv - Simple coverage at the sample level.","title":"alignment/"},{"location":"pipeline-wi/#cegwas","text":"kinship.Rda - A kinship matrix constructed from WI.(date).impute.vcf.gz snps.Rda - A mapping snp set generated from WI.(date).hard-filter.vcf.gz","title":"cegwas/"},{"location":"pipeline-wi/#isotype","text":"This directory contains files that integrate with the genome browser on CeDNR.","title":"isotype"},{"location":"pipeline-wi/#isotypetsv","text":"This directory contains tsv's that are used to show where variants are in CeNDR.","title":"isotype/tsv/"},{"location":"pipeline-wi/#isotypevcf","text":"This direcoty contains the VCFs of isotypes.","title":"isotype/vcf/"},{"location":"pipeline-wi/#phenotype","text":"MT_content.tsv - Mitochondrial content (MtDNA cov / Nuclear cov). kmers.tsv - 6-mers for each isotype. telseq.tsv - Telomere length for each isotype ~ split out by read group.","title":"phenotype/"},{"location":"pipeline-wi/#popgen","text":"WI.(date).tajima.bed.gz - Tajima's D bedfile for use on the report viewer of CeNDR. WI.(date).tajima.bed.gz.tbi - Tajima's D index. trees/ - Phylogenetic trees for each chromosome and the entire genome. I.(pdf|png|tree) ... genome.(pdf|png|tree)","title":"popgen/"},{"location":"pipeline-wi/#report","text":"multiqc.html - A comprehensive report of the sequencing run. multiqc_data/ multiqc_bcftools_stats.json multiqc_data.json multiqc_fastqc.json multiqc_general_stats.json multiqc_picard_AlignmentSummaryMetrics.json multiqc_picard_dups.json multiqc_picard_insertSize.json multiqc_samtools_idxstats.json multiqc_samtools_stats.json multiqc_snpeff.json multiqc_sources.json","title":"report/"},{"location":"pipeline-wi/#track","text":"(date).LOW.bed.gz(+.tbi) - LOW effect mutations bed track and index. (date).MODERATE.bed.gz(+.tbi) - MODERATE effect mutations bed track and index. (date).HIGH.bed.gz(+.tbi) - HIGH effect mutations bed track and index. (date).MODIFIER.bed.gz(+.tbi) - MODERATE effect mutations bed track and index. phastcons.bed.gz(+.tbi) - Phastcons bed track and index. phylop.bed.gz(+.tbi) - PhyloP bed track and index","title":"track/"},{"location":"pipeline-wi/#variation","text":"WI.(date).soft-filter.vcf.gz(+.csi|+.tbi) - WI.(date).soft-filter.stats.txt - WI.(date).hard-filter.vcf.gz(+.csi|+.tbi) - WI.(date).hard-filter.stats.txt - WI.(date).hard-filter.genotypes.tsv - WI.(date).hard-filter.genotypes.frequency.tsv - WI.(date).impute.vcf.gz(+.csi|+.tbi) - WI.(date).impute.stats.txt - sitelist.tsv.gz(+.tbi) - Union of all sites identified in original SNV variant calling round.","title":"Variation/"},{"location":"quest-andersen-lab-env/","text":"The Andersen Lab Software Environment \u00b6 The Andersen Lab Software Environment andersen-lab-env pyenv Setting the global version Setting the local version pyenv-virtualenv conda conda integrates with pyenv and pyenv-virtualenv pyenv environments are inherited andersen-lab-env structure Installing the andersen-lab-env andersen-lab-env git structure adding new software andersen-lab-env \u00b6 Computational Reproducibility is the ability to reproduce an analysis exactly. In order for comutational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub. Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software we've developed the andersen-lab-env . The andersen-lab-env is a set of software environments that can be used in conjunction with the bioinformatic pipelines we have developed for Quest. These environments can be installed locally on a Mac or on Quest. The andersen-lab-env is designed to change over time, but we explicitly define the software versions, and we track changes to the environments over time. The system is in some ways complex. This page is designed to try to explain how it works. We rely on three different tools to manage software environments. In concert they provide a lot of flexibility when it comes to setting up the software environment. Note The software environments on Mac and Linux are not exactly identical...but they are very close. There is an installation script you can use to install the andersen-lab-env , but it is recommended that you read this page before doing so. pyenv \u00b6 pyenv documentation pyenv is used to install and manage different versions of python. For example, you might have a python 3 script for one project and a python 2 script for another. You want to be able to run both scripts on your system. One option is to modify the python 2 script to work with python 3, but this is not always an option. The solution is to be able to install multiple versions of python simultaneously. pyenv allows you to do this. More than this, pyenv allows you to set the python version that you want to use at the local or global level. local - Sets the python version to a specific directory. global - Sets the python version to use everywhere unless a local version is set. Lets look at an example of this. First, lets install two different versions of python. pyenv install 2.7.11 pyenv install 3.6.0 Now you can see the installed versions by typing pyenv versions : $ pyenv versions * system 2.7.11 3.6.0 The * indicates that that is the current version of python you are using. In the case above it is set to use the system python which is preinstalled and is often python 2. Setting the global version \u00b6 Example setting the global version of python to 3.6.0 pyenv global 3.6.0 Now when we run python it will use python version 3.6.0. Setting the local version \u00b6 Example setting the local version of python to 2.7.11 mkdir my_python2_project cd my_python2_project pyenv local 2.7.11 Now if we go into a particular directoy and type pyenv local 2.7.11 , a .python-version file is created that says 2.7.11 and makes it so that directory always uses python 2.7.11. Now lets see what this looks like: As is illustrated above, versions are inherited from parent directories. When you cd to a directory, pyenv searches up through each directory looking for a .python-version file to identify which version of python to use. If it reaches the top before finding one it uses the global version. tl;dr; - pyenv allows us to install separate versions of python and set them at the directory level. pyenv-virtualenv \u00b6 Documentation pyenv lets us install multiple versions of python, and lets us use those versions locally within certain directories or globally. But what if we have two projects that use Python 2.7.11 and one requires a python module with a specific version: networkx==1.0 . Another project the same module greater than version 2.0 networkx>2.0 . How can we simultaneously work on both projects on the same system? virtualenv is a python tool for creating isolated python environments (also known as virtualenvs; The usage tends to be specific for python virtual environments and is short for 'virtual environment'). You can create a virtualenv for every project that you do - and these can be used to ensure that when you update or install modules for a given project that they do not interfere with each other. We won't be using virtualenv directly, but instead will the pyenv flavor of virtualenvs. pyenv-virtualenv is a tool that can create virtual environments that operate similar to the way pyenv python environments do. You can create virtualenvs that act globally or you can create virtualenvs that are local to a specific directory. To create a pyenv-virtualenv you must provide a base python environment that you have installed and a name for the environment. For example, below the python environment is 2.7.11 and the name of the environment is c_elegans_project : pyenv virtualenv 2.7.11 c_elegans_project Then you can set that virtualenv to a local directory using: mkdir c_elegans_project cd c_elegans_project pyenv local c_elegans_project Notice that the folder name is the same as the virtualenv. This can be a good idea for clarity. You can see a list of python versions and virtual environments by typing: pyenv versions Output: 2.7.11 * 2.7.11/envs/c_elegans_project 3.6.0 Virtual environments are designated as <version>/envs/<name> . Now we can also install the module we need for that specific project. pyenv installs a python-specific package manager called pip : pip install networkx==1.0 Notice that at this point we have isolated independent environments that do not interfere with one another. If we leave them alone for a year we should be able to come back and the software environment should be the same... and if they work with data they should reproduce the identical result. tl;dr - pyenv-virtualenv can define custom isolated python environments and set them the same way pyenv sets python installations. conda \u00b6 Conda Documentation Thus far we've managed to install multiple versions of python and figured out how to use them in independent, isolated environments. But we obviously use a lot more than just Python. We need to be able to install things like bcftools to work with variant data. We need to be able to install Java packages, and R packages, and all kinds of software. Conda can help us with this. Conda is a language-agnostic package manager. That means it can be used to install packages from python, R, Java, C/C++, etc. For example, the command below will install R and the R Tidyverse . conda install r-tidyverse conda integrates with pyenv and pyenv-virtualenv \u00b6 Important for our purposes, conda can be installed by pyenv . When I stated earlier that pyenv is used to install and manage versions of python I ommitted the fact that it can also install conda to avoid confusion. conda is not a version of python , but it is written in python, and it can be used to install python modules in addition to lots of other stuff. Similar to python virtualenvs, isolated conda environments can be created as was demonstrated above. You would run something like: pyenv install miniconda3-4.3.27 pyenv virtualenv miniconda3-4.3.27 my_new_conda_env pyenv local my_new_conda_env conda install bcftools pip install requests # This version of pip is specific gto What is great about these environments is that we can create custom software environments to suit any project. We can install R packages, python modules, C/C++ executables, and more. pyenv environments are inherited \u00b6 We can now install custom environments for each project. Even better, pyenv allows you to specify multiple environments together. Consider the example in this diagram: There are two environments defined: env_1 bcftools v1.6 bedtools v1.2 R-tidyverse 1.0 env_2 bcftools v1.6 vcf-kit v1.6 Those environments on their own appear in blue above. If we were to use the following command to specify these environments: pyenv local env_2 env_1 base_version We would produce the green environment in the diagram. What you are seeing are two environments being combined. However, the order you specify them in matters. Notice that bcftools v1.7 is used and not bcftools v1.6 . This is because env_2 is searched first when commands libraries are retrieved. After pulling all the libraries in env_2 , the combined library will inherit anything remaining in env_1 . This allows to easily combine environments for analysis. Remember that each of these virtual environments is based on a version of python or conda. But you can also put a plain version of python or conda as your last environment. This is useful when using conda because the conda command does not inherit from conda-based virtualenvs. andersen-lab-env structure \u00b6 The anderse-lab-env uses two conda environments: primary - The primary environment contains the majority of the tools required for performing sequence analysis. py2 - For programs that require python 2. You can create and use your own conda environments for projects, but these are designed to be comprehensive. Installing the andersen-lab-env \u00b6 If you are on Quest Edit your .bashrc file to contain the following: # .bashrc export PKG_CONFIG_PATH=/usr/share/pkgconfig:$PKG_CONFIG_PATH # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi Installation The andersen-lab-env can be installed by running the following command: cd && if cd ~/andersen-lab-env; then git pull; else git clone http://www.github.com/andersenlab/andersen-lab-env; fi bash setup.sh This command will clone the repo, cd into it, and run the setup.sh script. When you run the setup.sh script it will install the latest version of the primary and py2 environments, and it will assign these environments globally as: pyenv primary-(date) py2-(date) minicondax-x.x.x You should not need to change your global environment Note If you have existing versions of the primary and py2 environments installed they will remain. You can set them locally at the project level if necessary. andersen-lab-env git structure \u00b6 The andersen-lab-env is used to manage and version the software environments. The repo has the following structure: \u251c\u2500\u2500 Brewfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 primary.environment.yaml \u251c\u2500\u2500 py2.environment.yaml \u251c\u2500\u2500 rebuild_envs.sh \u251c\u2500\u2500 setup.sh \u251c\u2500\u2500 user_bash_profile.sh \u2514\u2500\u2500 versions \u251c\u2500\u2500 Linux.2018-03-08.primary.yaml \u251c\u2500\u2500 Linux.2018-03-08.py2.yaml \u251c\u2500\u2500 Mac.2018-03-08.primary.yaml \u2514\u2500\u2500 Mac.2018-03-08.py2.yaml primary.environment.yaml - base primary environment. This lists the software to be installed, but not specific versions of it. py2.environment.yaml - The base py2 environment. This lists the software to be installed, but not specific versions of it. Brewfile - Defines the software software-dependencies to be installed when running setup.sh rebuild_envs - Used to construct new versions of the environments. Note that you need to do this on a Linux and Mac computer. user_bash_profile.sh - The optional bash profile that is created with setup.sh . versions/ - Software required for each environment with all dependencies. Versioned in git and by platform and date. adding new software \u00b6 When you want to add new software a new version of the primary and py2 environments should be created. You must modify the primary.environment.yaml or py2.environment.yaml files and build the files you see in the versions folder which define the required software by specific version and includes all the dependencies. bash rebuild_envs.sh This will output two new versions specific to your platform in the versions/ folder. You must run this script and generate the appropriate version files on both Mac and Linux. Commit the updated versions to git. Other users can then install them by running the command in installing the andersen-lab-env","title":"Andersen-Lab-Env"},{"location":"quest-andersen-lab-env/#the_andersen_lab_software_environment","text":"The Andersen Lab Software Environment andersen-lab-env pyenv Setting the global version Setting the local version pyenv-virtualenv conda conda integrates with pyenv and pyenv-virtualenv pyenv environments are inherited andersen-lab-env structure Installing the andersen-lab-env andersen-lab-env git structure adding new software","title":"The Andersen Lab Software Environment"},{"location":"quest-andersen-lab-env/#andersen-lab-env","text":"Computational Reproducibility is the ability to reproduce an analysis exactly. In order for comutational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub. Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software we've developed the andersen-lab-env . The andersen-lab-env is a set of software environments that can be used in conjunction with the bioinformatic pipelines we have developed for Quest. These environments can be installed locally on a Mac or on Quest. The andersen-lab-env is designed to change over time, but we explicitly define the software versions, and we track changes to the environments over time. The system is in some ways complex. This page is designed to try to explain how it works. We rely on three different tools to manage software environments. In concert they provide a lot of flexibility when it comes to setting up the software environment. Note The software environments on Mac and Linux are not exactly identical...but they are very close. There is an installation script you can use to install the andersen-lab-env , but it is recommended that you read this page before doing so.","title":"andersen-lab-env"},{"location":"quest-andersen-lab-env/#pyenv","text":"pyenv documentation pyenv is used to install and manage different versions of python. For example, you might have a python 3 script for one project and a python 2 script for another. You want to be able to run both scripts on your system. One option is to modify the python 2 script to work with python 3, but this is not always an option. The solution is to be able to install multiple versions of python simultaneously. pyenv allows you to do this. More than this, pyenv allows you to set the python version that you want to use at the local or global level. local - Sets the python version to a specific directory. global - Sets the python version to use everywhere unless a local version is set. Lets look at an example of this. First, lets install two different versions of python. pyenv install 2.7.11 pyenv install 3.6.0 Now you can see the installed versions by typing pyenv versions : $ pyenv versions * system 2.7.11 3.6.0 The * indicates that that is the current version of python you are using. In the case above it is set to use the system python which is preinstalled and is often python 2.","title":"pyenv"},{"location":"quest-andersen-lab-env/#setting_the_global_version","text":"Example setting the global version of python to 3.6.0 pyenv global 3.6.0 Now when we run python it will use python version 3.6.0.","title":"Setting the global version"},{"location":"quest-andersen-lab-env/#setting_the_local_version","text":"Example setting the local version of python to 2.7.11 mkdir my_python2_project cd my_python2_project pyenv local 2.7.11 Now if we go into a particular directoy and type pyenv local 2.7.11 , a .python-version file is created that says 2.7.11 and makes it so that directory always uses python 2.7.11. Now lets see what this looks like: As is illustrated above, versions are inherited from parent directories. When you cd to a directory, pyenv searches up through each directory looking for a .python-version file to identify which version of python to use. If it reaches the top before finding one it uses the global version. tl;dr; - pyenv allows us to install separate versions of python and set them at the directory level.","title":"Setting the local version"},{"location":"quest-andersen-lab-env/#pyenv-virtualenv","text":"Documentation pyenv lets us install multiple versions of python, and lets us use those versions locally within certain directories or globally. But what if we have two projects that use Python 2.7.11 and one requires a python module with a specific version: networkx==1.0 . Another project the same module greater than version 2.0 networkx>2.0 . How can we simultaneously work on both projects on the same system? virtualenv is a python tool for creating isolated python environments (also known as virtualenvs; The usage tends to be specific for python virtual environments and is short for 'virtual environment'). You can create a virtualenv for every project that you do - and these can be used to ensure that when you update or install modules for a given project that they do not interfere with each other. We won't be using virtualenv directly, but instead will the pyenv flavor of virtualenvs. pyenv-virtualenv is a tool that can create virtual environments that operate similar to the way pyenv python environments do. You can create virtualenvs that act globally or you can create virtualenvs that are local to a specific directory. To create a pyenv-virtualenv you must provide a base python environment that you have installed and a name for the environment. For example, below the python environment is 2.7.11 and the name of the environment is c_elegans_project : pyenv virtualenv 2.7.11 c_elegans_project Then you can set that virtualenv to a local directory using: mkdir c_elegans_project cd c_elegans_project pyenv local c_elegans_project Notice that the folder name is the same as the virtualenv. This can be a good idea for clarity. You can see a list of python versions and virtual environments by typing: pyenv versions Output: 2.7.11 * 2.7.11/envs/c_elegans_project 3.6.0 Virtual environments are designated as <version>/envs/<name> . Now we can also install the module we need for that specific project. pyenv installs a python-specific package manager called pip : pip install networkx==1.0 Notice that at this point we have isolated independent environments that do not interfere with one another. If we leave them alone for a year we should be able to come back and the software environment should be the same... and if they work with data they should reproduce the identical result. tl;dr - pyenv-virtualenv can define custom isolated python environments and set them the same way pyenv sets python installations.","title":"pyenv-virtualenv"},{"location":"quest-andersen-lab-env/#conda","text":"Conda Documentation Thus far we've managed to install multiple versions of python and figured out how to use them in independent, isolated environments. But we obviously use a lot more than just Python. We need to be able to install things like bcftools to work with variant data. We need to be able to install Java packages, and R packages, and all kinds of software. Conda can help us with this. Conda is a language-agnostic package manager. That means it can be used to install packages from python, R, Java, C/C++, etc. For example, the command below will install R and the R Tidyverse . conda install r-tidyverse","title":"conda"},{"location":"quest-andersen-lab-env/#conda_integrates_with_pyenv_and_pyenv-virtualenv","text":"Important for our purposes, conda can be installed by pyenv . When I stated earlier that pyenv is used to install and manage versions of python I ommitted the fact that it can also install conda to avoid confusion. conda is not a version of python , but it is written in python, and it can be used to install python modules in addition to lots of other stuff. Similar to python virtualenvs, isolated conda environments can be created as was demonstrated above. You would run something like: pyenv install miniconda3-4.3.27 pyenv virtualenv miniconda3-4.3.27 my_new_conda_env pyenv local my_new_conda_env conda install bcftools pip install requests # This version of pip is specific gto What is great about these environments is that we can create custom software environments to suit any project. We can install R packages, python modules, C/C++ executables, and more.","title":"conda integrates with pyenv and pyenv-virtualenv"},{"location":"quest-andersen-lab-env/#pyenv_environments_are_inherited","text":"We can now install custom environments for each project. Even better, pyenv allows you to specify multiple environments together. Consider the example in this diagram: There are two environments defined: env_1 bcftools v1.6 bedtools v1.2 R-tidyverse 1.0 env_2 bcftools v1.6 vcf-kit v1.6 Those environments on their own appear in blue above. If we were to use the following command to specify these environments: pyenv local env_2 env_1 base_version We would produce the green environment in the diagram. What you are seeing are two environments being combined. However, the order you specify them in matters. Notice that bcftools v1.7 is used and not bcftools v1.6 . This is because env_2 is searched first when commands libraries are retrieved. After pulling all the libraries in env_2 , the combined library will inherit anything remaining in env_1 . This allows to easily combine environments for analysis. Remember that each of these virtual environments is based on a version of python or conda. But you can also put a plain version of python or conda as your last environment. This is useful when using conda because the conda command does not inherit from conda-based virtualenvs.","title":"pyenv environments are inherited"},{"location":"quest-andersen-lab-env/#andersen-lab-env_structure","text":"The anderse-lab-env uses two conda environments: primary - The primary environment contains the majority of the tools required for performing sequence analysis. py2 - For programs that require python 2. You can create and use your own conda environments for projects, but these are designed to be comprehensive.","title":"andersen-lab-env structure"},{"location":"quest-andersen-lab-env/#installing_the_andersen-lab-env","text":"If you are on Quest Edit your .bashrc file to contain the following: # .bashrc export PKG_CONFIG_PATH=/usr/share/pkgconfig:$PKG_CONFIG_PATH # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi Installation The andersen-lab-env can be installed by running the following command: cd && if cd ~/andersen-lab-env; then git pull; else git clone http://www.github.com/andersenlab/andersen-lab-env; fi bash setup.sh This command will clone the repo, cd into it, and run the setup.sh script. When you run the setup.sh script it will install the latest version of the primary and py2 environments, and it will assign these environments globally as: pyenv primary-(date) py2-(date) minicondax-x.x.x You should not need to change your global environment Note If you have existing versions of the primary and py2 environments installed they will remain. You can set them locally at the project level if necessary.","title":"Installing the andersen-lab-env"},{"location":"quest-andersen-lab-env/#andersen-lab-env_git_structure","text":"The andersen-lab-env is used to manage and version the software environments. The repo has the following structure: \u251c\u2500\u2500 Brewfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 primary.environment.yaml \u251c\u2500\u2500 py2.environment.yaml \u251c\u2500\u2500 rebuild_envs.sh \u251c\u2500\u2500 setup.sh \u251c\u2500\u2500 user_bash_profile.sh \u2514\u2500\u2500 versions \u251c\u2500\u2500 Linux.2018-03-08.primary.yaml \u251c\u2500\u2500 Linux.2018-03-08.py2.yaml \u251c\u2500\u2500 Mac.2018-03-08.primary.yaml \u2514\u2500\u2500 Mac.2018-03-08.py2.yaml primary.environment.yaml - base primary environment. This lists the software to be installed, but not specific versions of it. py2.environment.yaml - The base py2 environment. This lists the software to be installed, but not specific versions of it. Brewfile - Defines the software software-dependencies to be installed when running setup.sh rebuild_envs - Used to construct new versions of the environments. Note that you need to do this on a Linux and Mac computer. user_bash_profile.sh - The optional bash profile that is created with setup.sh . versions/ - Software required for each environment with all dependencies. Versioned in git and by platform and date.","title":"andersen-lab-env git structure"},{"location":"quest-andersen-lab-env/#adding_new_software","text":"When you want to add new software a new version of the primary and py2 environments should be created. You must modify the primary.environment.yaml or py2.environment.yaml files and build the files you see in the versions folder which define the required software by specific version and includes all the dependencies. bash rebuild_envs.sh This will output two new versions specific to your platform in the versions/ folder. You must run this script and generate the appropriate version files on both Mac and Linux. Commit the updated versions to git. Other users can then install them by running the command in installing the andersen-lab-env","title":"adding new software"},{"location":"quest-intro/","text":"Introduction \u00b6 Introduction Signing into Quest Login Nodes Home Directory Projects Running interactive jobs on Quest The Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation Signing into Quest \u00b6 After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu I recommend setting an alias in your .bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below). Login Nodes \u00b6 There are four login nodes we use: quser21-24. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh quser21 ). Warning When using screen to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions. Home Directory \u00b6 Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc . More information is provided below to help install and use software. Projects \u00b6 Quest is broadly organized into projects. Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 155 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. b1059 - The Andersen Lab Project. b1059 does not have any nodes associated with it, but it does have 40 Tb of storage. b1059 storage is located at: /projects/b1059/ . Note Anyone who use quest should build your own project folder under /projects/b1059/projects with your name. You should only write and revise files under your project folder. You can read/copy data from b1059 but don't write any data out of your project folder. Running interactive jobs on Quest \u00b6 If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands srun -A b1042 --partition=genomicsguest -N 1 -n 24 --mem=64G --time=12:00:00 --pty bash -i Important Do not run commands for big data on quser21-24 . These are login nodes and are not meant for running heavy-load workflows.","title":"Introduction"},{"location":"quest-intro/#introduction","text":"Introduction Signing into Quest Login Nodes Home Directory Projects Running interactive jobs on Quest The Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation","title":"Introduction"},{"location":"quest-intro/#signing_into_quest","text":"After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu I recommend setting an alias in your .bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below).","title":"Signing into Quest"},{"location":"quest-intro/#login_nodes","text":"There are four login nodes we use: quser21-24. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh quser21 ). Warning When using screen to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions.","title":"Login Nodes"},{"location":"quest-intro/#home_directory","text":"Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc . More information is provided below to help install and use software.","title":"Home Directory"},{"location":"quest-intro/#projects","text":"Quest is broadly organized into projects. Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 155 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. b1059 - The Andersen Lab Project. b1059 does not have any nodes associated with it, but it does have 40 Tb of storage. b1059 storage is located at: /projects/b1059/ . Note Anyone who use quest should build your own project folder under /projects/b1059/projects with your name. You should only write and revise files under your project folder. You can read/copy data from b1059 but don't write any data out of your project folder.","title":"Projects"},{"location":"quest-intro/#running_interactive_jobs_on_quest","text":"If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands srun -A b1042 --partition=genomicsguest -N 1 -n 24 --mem=64G --time=12:00:00 --pty bash -i Important Do not run commands for big data on quser21-24 . These are login nodes and are not meant for running heavy-load workflows.","title":"Running interactive jobs on Quest"},{"location":"quest-mount/","text":"1. Download and Install Fuse for Mac OS https://osxfuse.github.io/ 2. Install sshfs You can use the link on https://osxfuse.github.io/ or use: brew install sshfs 3. Create a folder in your documents called b1059 mkdir ~/b1059 4. Mount our labs quest project folder ( b1059 ) to the b1059 folder you created locally sshfs <NETID>@quest.it.northwestern.edu:/projects/b1059/ ~/Documents/b1059 -ovolname=b1059 To mount alignments of isotypes at this location: sshfs <NETID>@quest.it.northwestern.edu:/projects/b1059/data/alignments/WI/isotype ~/Documents/b1059 -ovolname=b1059","title":"Mounting Quest"},{"location":"quest-nextflow/","text":"Installation Quest cluster configuration Global Configuration: ~/.nextflow/config Screen Resources Installation \u00b6 Important If you haven't already, take a look at Andersen-Lab-Env for more information on how to setup your environment on Quest. Nextflow can be installed with linuxbrew or homebrew. Use: brew tap homebrew/science brew install nextflow Quest cluster configuration \u00b6 Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config . Global Configuration: ~/.nextflow/config \u00b6 In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'genomicsguestA' clusterOptions = '-A b1042 -t 24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work/<your folder>\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: Sets the executor to slurm (which is what Quest uses) Sets the queue to genomicsguestA which submits jobs to genomics nodes. The genomicsguestA will submit jobs to our dedicated nodes first, which we have high priority. If our dedicated nodes are full, it will submit to other nodes we don't have priority. So far, our lab have 2 dedicated nodes, with 28 cores and related memory (close to 1:5) for each dedicated node. We will have more in the future. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. To better organization, Please build your own folder under /projects/b1042/AndersenLab/work/ , and define it here. tmpDir - Creates a temporary working directory. This can be used within workflows when necessary. Screen \u00b6 When jobs run for a very long time you should run them in screen. Screen lets you continue to run jobs in the background even if you get kicked off the cluster or log off. Screen Tutorial Keep in mind that quest has several login nodes. We use quser21-24 . Screen sessions only persist on ONE of these login nodes. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh quser 22 ). Resources \u00b6 Nextflow documentation Awesome Nextflow pipeline examples - Repository of great nextflow pipelines.","title":"Nextflow"},{"location":"quest-nextflow/#installation","text":"Important If you haven't already, take a look at Andersen-Lab-Env for more information on how to setup your environment on Quest. Nextflow can be installed with linuxbrew or homebrew. Use: brew tap homebrew/science brew install nextflow","title":"Installation"},{"location":"quest-nextflow/#quest_cluster_configuration","text":"Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config .","title":"Quest cluster configuration"},{"location":"quest-nextflow/#global_configuration_nextflowconfig","text":"In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'genomicsguestA' clusterOptions = '-A b1042 -t 24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work/<your folder>\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: Sets the executor to slurm (which is what Quest uses) Sets the queue to genomicsguestA which submits jobs to genomics nodes. The genomicsguestA will submit jobs to our dedicated nodes first, which we have high priority. If our dedicated nodes are full, it will submit to other nodes we don't have priority. So far, our lab have 2 dedicated nodes, with 28 cores and related memory (close to 1:5) for each dedicated node. We will have more in the future. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. To better organization, Please build your own folder under /projects/b1042/AndersenLab/work/ , and define it here. tmpDir - Creates a temporary working directory. This can be used within workflows when necessary.","title":"Global Configuration: ~/.nextflow/config"},{"location":"quest-nextflow/#screen","text":"When jobs run for a very long time you should run them in screen. Screen lets you continue to run jobs in the background even if you get kicked off the cluster or log off. Screen Tutorial Keep in mind that quest has several login nodes. We use quser21-24 . Screen sessions only persist on ONE of these login nodes. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh quser 22 ).","title":"Screen"},{"location":"quest-nextflow/#resources","text":"Nextflow documentation Awesome Nextflow pipeline examples - Repository of great nextflow pipelines.","title":"Resources"},{"location":"r/","text":"R \u00b6 R Packages \u00b6 The Andersen lab maintains several R packages. cegwas \u00b6 andersenlab/cegwas linkagemapping \u00b6 andersenlab/linkagemapping easysorter \u00b6 andersenlab/easysorter","title":"R"},{"location":"r/#r","text":"","title":"R"},{"location":"r/#r_packages","text":"The Andersen lab maintains several R packages.","title":"R Packages"},{"location":"r/#cegwas","text":"andersenlab/cegwas","title":"cegwas"},{"location":"r/#linkagemapping","text":"andersenlab/linkagemapping","title":"linkagemapping"},{"location":"r/#easysorter","text":"andersenlab/easysorter","title":"easysorter"},{"location":"sample-sheets/","text":"Sample Sheets \u00b6 Sample Sheets Creating sample sheets wi-nf and concordance-nf pipelines nil-ril-nf Sample-Sheet Format Absolute vs. relative paths The wi-nf , concordance-nf , and nil-ril-nf pipelines all make use of sample sheets. Sample sheets specify which fastqs belong to a given strain or isotype. Creating sample sheets \u00b6 wi-nf and concordance-nf pipelines \u00b6 For the wi-nf and concordance-nf pipelines, sample-sheets are generated using the file located (in each of these repos) in the scripts/construct_sample_sheet.sh . Importantly, these scripts are almost identical except that the concordance-nf pipeline constructs a sample sheet for strains whereas the wi-nf sample sheet is for isotypes . When adding new sequence data you need to update these scripts. Note The nomenclature regarding sample sheets and scripts was changed in March of 2018 to make it clearer. You may encounter older files with the following names that correspond to the newer names SM_sample_sheet --> sample_sheet.tsv construct_SM_sheet.sh --> construct_sample_sheet.tsv nil-ril-nf \u00b6 For the nil-ril-nf pipelines you must manually create the sample sheets according to the format below. Sample-Sheet Format \u00b6 The sample sheet defines which FASTQs belong to which strain/isotype and specifies additional information regarding a sample. Additional information specfieid are the FASTQ ID (a unique identifier for a FASTQ-pair), Sequencing POOL (which defines the group of samples that were sequenced together), the locations of the FASTQs, and the sequencing folder. Note Internally, the 'sequencing pool' information as treated as the DNA-library identifier by BWA ( LB ). Our lab processes sequence data such that the pool name uniquely identifies DNA-libraries for each sample. Sample sheet structure All columns are required. Sample Identifier - How FASTQs should be grouped in the pipeline. Usually this is by strain or isotype. FASTQ ID - A unique ID for the FASTQ pair. It must be unique for all sequencing runs defined in the sample sheet. Sequencing pool - The sequencing pool is often defined arbitrarily. It refers to the set of strains that were sequenced together. It acts as an identifer of the DNA library within the pipelines. FASTQ1 - A relative or absolute path to the first FASTQ. FASTQ2 - A relative or absolute path to the second FASTQ. Sequencing Folder - This column is provided for informational purposes. It generally refers to the name of the folder containing the FASTQs. Example AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Notice that the file does not include a header . The table with corresponding header included below look like this: Sample Identifeir FASTQ ID Sequencing Pool fastq-1-path fastq-2-path sequencing_folder AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Absolute vs. relative paths \u00b6 When constructing the sample sheet for the wi-nf and concordance-nf pipelines you are required to use the absolute paths to each FASTQ. The nil-ril-nf pipeline can use relative paths to FASTQs by specifying the --fq_file_prefix option to the parent directory containing FASTQs.","title":"Sample Sheets"},{"location":"sample-sheets/#sample_sheets","text":"Sample Sheets Creating sample sheets wi-nf and concordance-nf pipelines nil-ril-nf Sample-Sheet Format Absolute vs. relative paths The wi-nf , concordance-nf , and nil-ril-nf pipelines all make use of sample sheets. Sample sheets specify which fastqs belong to a given strain or isotype.","title":"Sample Sheets"},{"location":"sample-sheets/#creating_sample_sheets","text":"","title":"Creating sample sheets"},{"location":"sample-sheets/#wi-nf_and_concordance-nf_pipelines","text":"For the wi-nf and concordance-nf pipelines, sample-sheets are generated using the file located (in each of these repos) in the scripts/construct_sample_sheet.sh . Importantly, these scripts are almost identical except that the concordance-nf pipeline constructs a sample sheet for strains whereas the wi-nf sample sheet is for isotypes . When adding new sequence data you need to update these scripts. Note The nomenclature regarding sample sheets and scripts was changed in March of 2018 to make it clearer. You may encounter older files with the following names that correspond to the newer names SM_sample_sheet --> sample_sheet.tsv construct_SM_sheet.sh --> construct_sample_sheet.tsv","title":"wi-nf and concordance-nf pipelines"},{"location":"sample-sheets/#nil-ril-nf","text":"For the nil-ril-nf pipelines you must manually create the sample sheets according to the format below.","title":"nil-ril-nf"},{"location":"sample-sheets/#sample-sheet_format","text":"The sample sheet defines which FASTQs belong to which strain/isotype and specifies additional information regarding a sample. Additional information specfieid are the FASTQ ID (a unique identifier for a FASTQ-pair), Sequencing POOL (which defines the group of samples that were sequenced together), the locations of the FASTQs, and the sequencing folder. Note Internally, the 'sequencing pool' information as treated as the DNA-library identifier by BWA ( LB ). Our lab processes sequence data such that the pool name uniquely identifies DNA-libraries for each sample. Sample sheet structure All columns are required. Sample Identifier - How FASTQs should be grouped in the pipeline. Usually this is by strain or isotype. FASTQ ID - A unique ID for the FASTQ pair. It must be unique for all sequencing runs defined in the sample sheet. Sequencing pool - The sequencing pool is often defined arbitrarily. It refers to the set of strains that were sequenced together. It acts as an identifer of the DNA library within the pipelines. FASTQ1 - A relative or absolute path to the first FASTQ. FASTQ2 - A relative or absolute path to the second FASTQ. Sequencing Folder - This column is provided for informational purposes. It generally refers to the name of the folder containing the FASTQs. Example AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Notice that the file does not include a header . The table with corresponding header included below look like this: Sample Identifeir FASTQ ID Sequencing Pool fastq-1-path fastq-2-path sequencing_folder AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set","title":"Sample-Sheet Format"},{"location":"sample-sheets/#absolute_vs_relative_paths","text":"When constructing the sample sheet for the wi-nf and concordance-nf pipelines you are required to use the absolute paths to each FASTQ. The nil-ril-nf pipeline can use relative paths to FASTQs by specifying the --fq_file_prefix option to the parent directory containing FASTQs.","title":"Absolute vs. relative paths"},{"location":"travis-ci/","text":"Setting up Quest \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Travis ci"},{"location":"travis-ci/#setting_up_quest","text":"For full documentation visit mkdocs.org .","title":"Setting up Quest"},{"location":"travis-ci/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"travis-ci/#project_layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"}]}